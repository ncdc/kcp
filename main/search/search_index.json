{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"content/en/","text":"kcp Documentation Overview kcp is a Kubernetes-like control plane focusing on: A control plane for many independent, isolated \u201cclusters\u201d known as workspaces Enabling API service providers to offer APIs centrally using multi-tenant operators Easy API consumption for users in their workspaces Flexible scheduling of workloads to physical clusters Transparent movement of workloads among compatible physical clusters Advanced deployment strategies for scenarios such as affinity/anti-affinity, geographic replication, cross-cloud replication, etc. kcp can be a building block for SaaS service providers who need a massively multi-tenant platform to offer services to a large number of fully isolated tenants using Kubernetes-native APIs. The goal is to be useful to cloud providers as well as enterprise IT departments offering APIs within their company. Quickstart Prerequisites kubectl A Kubernetes cluster (for local testing, consider kind ) Download kcp Visit our latest release page and download kcp and kubectl-kcp-plugin that match your operating system and architecture. Extract kcp and kubectl-kcp-plugin and place all the files in the bin directories somewhere in your $PATH . Start kcp You can start kcp using this command: kcp start This launches kcp in the foreground. You can press ctrl-c to stop it. To see a complete list of server options, run kcp start options . Set your KUBECONFIG During its startup, kcp generates a kubeconfig in .kcp/admin.kubeconfig . Use this to connect to kcp and display the version to confirm it's working: $ export KUBECONFIG=.kcp/admin.kubeconfig $ kubectl version WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.4\", GitCommit:\"95ee5ab382d64cfe6c28967f36b53970b8374491\", GitTreeState:\"clean\", BuildDate:\"2022-08-17T18:46:11Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Kustomize Version: v4.5.4 Server Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.3+kcp-v0.8.0\", GitCommit:\"41863897\", GitTreeState:\"clean\", BuildDate:\"2022-09-02T18:10:37Z\", GoVersion:\"go1.18.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Configure kcp to sync to your cluster kcp can't run pods by itself - it needs at least one physical cluster for that. For this example, we'll be using a local kind cluster. It does not have to exist yet. In this recipe we use the root workspace to hold the description of the workload and where it goes. These usually would go elsewhere, but we use the root workspace here for simplicity. Run the following command to tell kcp about the kind cluster (replace the syncer image tag as needed; CI now puts built images in https://github.com/orgs/kcp-dev/packages): $ kubectl kcp workload sync kind --syncer-image ghcr.io/kcp-dev/kcp/syncer:v0.10.0 -o syncer-kind-main.yaml Creating synctarget \"kind\" Creating service account \"kcp-syncer-kind-25coemaz\" Creating cluster role \"kcp-syncer-kind-25coemaz\" to give service account \"kcp-syncer-kind-25coemaz\" 1. write and sync access to the synctarget \"kcp-syncer-kind-25coemaz\" 2. write access to apiresourceimports. Creating or updating cluster role binding \"kcp-syncer-kind-25coemaz\" to bind service account \"kcp-syncer-kind-25coemaz\" to cluster role \"kcp-syncer-kind-25coemaz\". Wrote physical cluster manifest to syncer-kind-main.yaml for namespace \"kcp-syncer-kind-25coemaz\". Use KUBECONFIG=<pcluster-config> kubectl apply -f \"syncer-kind-main.yaml\" to apply it. Use KUBECONFIG=<pcluster-config> kubectl get deployment -n \"kcp-syncer-kind-25coemaz\" kcp-syncer-kind-25coemaz to verify the syncer pod is running. Next, we need to install the syncer pod on our kind cluster - this is what actually syncs content from kcp to the physical cluster. The kind cluster needs to be running by now. Run the following command: $ KUBECONFIG=</path/to/kind/kubeconfig> kubectl apply -f \"syncer-kind-main.yaml\" namespace/kcp-syncer-kind-25coemaz created serviceaccount/kcp-syncer-kind-25coemaz created secret/kcp-syncer-kind-25coemaz-token created clusterrole.rbac.authorization.k8s.io/kcp-syncer-kind-25coemaz created clusterrolebinding.rbac.authorization.k8s.io/kcp-syncer-kind-25coemaz created secret/kcp-syncer-kind-25coemaz created deployment.apps/kcp-syncer-kind-25coemaz created Bind to workload APIs and create default placement If you are running kcp version v0.10.0 or higher, you will need to run the following commmand (continuing in the root workspace) to create a binding to the workload APIs export and a default placement for your physical cluster: $ kubectl kcp bind compute root Binding APIExport \"root:compute:kubernetes\". placement placement-1pfxsevk created. Placement \"placement-1pfxsevk\" is ready. Create a deployment in kcp Let's create a deployment in our kcp workspace and see it get synced to our cluster: $ kubectl create deployment --image=gcr.io/kuar-demo/kuard-amd64:blue --port=8080 kuard deployment.apps/kuard created Once your cluster has pulled the image and started the pod, you should be able to verify the deployment is running in kcp: $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE kuard 1/1 1 1 3s We are still working on adding support for kubectl logs , kubectl exec , and kubectl port-forward to kcp. For the time being, you can check directly in your cluster. kcp translates the names of namespaces in workspaces to unique names in a physical cluster. We first must get this translated name; if you're following along, your translated name might be different. $ KUBECONFIG=</path/to/kind/kubeconfig> kubectl get pods --all-namespaces --selector app=kuard NAMESPACE NAME READY STATUS RESTARTS AGE kcp-26zq2mc2yajx kuard-7d49c786c5-wfpcc 1/1 Running 0 4m28s Now we can e.g. check the pod logs: $ KUBECONFIG=</path/to/kind/kubeconfig> kubectl --namespace kcp-26zq2mc2yajx logs deployment/kuard | head 2022/09/07 14:04:35 Starting kuard version: v0.10.0-blue 2022/09/07 14:04:35 ********************************************************************** 2022/09/07 14:04:35 * WARNING: This server may expose sensitive 2022/09/07 14:04:35 * and secret information. Be careful. 2022/09/07 14:04:35 ********************************************************************** 2022/09/07 14:04:35 Config: { \"address\": \":8080\", \"debug\": false, \"debug-sitedata-dir\": \"./sitedata\", Next steps Thanks for checking out our quickstart! If you're interested in learning more about all the features kcp has to offer, please check out our additional documentation: Concepts - a high level overview of kcp concepts Workspaces - a more thorough introduction on kcp's workspaces Locations & scheduling - details on kcp's primitives that abstract over clusters Syncer - information on running the kcp agent that syncs content between kcp and a physical cluster kubectl plugin Authorization - how kcp manages access control to workspaces and content Virtual workspaces - details on kcp's mechanism for virtual views of workspace content Contributing We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing and Developer guides. Getting in touch There are several ways to communicate with us: The #kcp-dev channel in the Kubernetes Slack workspace Our mailing lists: kcp-dev for development discussions kcp-users for discussions among users and potential users Subscribe to the community calendar for community meetings and events The kcp-dev mailing list is subscribed to this calendar See recordings of past community meetings on YouTube See upcoming and past community meeting agendas and notes Browse the shared Google Drive to share design docs, notes, etc. Members of the kcp-dev mailing list can view this drive Additional references KubeCon EU 2021: Kubernetes as the Hybrid Cloud Control Plane Keynote - Clayton Coleman (video) OpenShift Commons: Kubernetes as the Control Plane for the Hybrid Cloud - Clayton Coleman (video) TGI Kubernetes 157: Exploring kcp: apiserver without Kubernetes K8s SIG Architecture meeting discussing kcp - June 29, 2021 Let's Learn kcp - A minimal Kubernetes API server with Saiyam Pathak - July 7, 2021","title":"<img src=\"logo.png\" style=\"vertical-align: middle;\" /> kcp Documentation"},{"location":"content/en/#kcp-documentation","text":"","title":" kcp Documentation"},{"location":"content/en/#overview","text":"kcp is a Kubernetes-like control plane focusing on: A control plane for many independent, isolated \u201cclusters\u201d known as workspaces Enabling API service providers to offer APIs centrally using multi-tenant operators Easy API consumption for users in their workspaces Flexible scheduling of workloads to physical clusters Transparent movement of workloads among compatible physical clusters Advanced deployment strategies for scenarios such as affinity/anti-affinity, geographic replication, cross-cloud replication, etc. kcp can be a building block for SaaS service providers who need a massively multi-tenant platform to offer services to a large number of fully isolated tenants using Kubernetes-native APIs. The goal is to be useful to cloud providers as well as enterprise IT departments offering APIs within their company.","title":"Overview"},{"location":"content/en/#quickstart","text":"","title":"Quickstart"},{"location":"content/en/#prerequisites","text":"kubectl A Kubernetes cluster (for local testing, consider kind )","title":"Prerequisites"},{"location":"content/en/#download-kcp","text":"Visit our latest release page and download kcp and kubectl-kcp-plugin that match your operating system and architecture. Extract kcp and kubectl-kcp-plugin and place all the files in the bin directories somewhere in your $PATH .","title":"Download kcp"},{"location":"content/en/#start-kcp","text":"You can start kcp using this command: kcp start This launches kcp in the foreground. You can press ctrl-c to stop it. To see a complete list of server options, run kcp start options .","title":"Start kcp"},{"location":"content/en/#set-your-kubeconfig","text":"During its startup, kcp generates a kubeconfig in .kcp/admin.kubeconfig . Use this to connect to kcp and display the version to confirm it's working: $ export KUBECONFIG=.kcp/admin.kubeconfig $ kubectl version WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.4\", GitCommit:\"95ee5ab382d64cfe6c28967f36b53970b8374491\", GitTreeState:\"clean\", BuildDate:\"2022-08-17T18:46:11Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Kustomize Version: v4.5.4 Server Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.3+kcp-v0.8.0\", GitCommit:\"41863897\", GitTreeState:\"clean\", BuildDate:\"2022-09-02T18:10:37Z\", GoVersion:\"go1.18.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}","title":"Set your KUBECONFIG"},{"location":"content/en/#configure-kcp-to-sync-to-your-cluster","text":"kcp can't run pods by itself - it needs at least one physical cluster for that. For this example, we'll be using a local kind cluster. It does not have to exist yet. In this recipe we use the root workspace to hold the description of the workload and where it goes. These usually would go elsewhere, but we use the root workspace here for simplicity. Run the following command to tell kcp about the kind cluster (replace the syncer image tag as needed; CI now puts built images in https://github.com/orgs/kcp-dev/packages): $ kubectl kcp workload sync kind --syncer-image ghcr.io/kcp-dev/kcp/syncer:v0.10.0 -o syncer-kind-main.yaml Creating synctarget \"kind\" Creating service account \"kcp-syncer-kind-25coemaz\" Creating cluster role \"kcp-syncer-kind-25coemaz\" to give service account \"kcp-syncer-kind-25coemaz\" 1. write and sync access to the synctarget \"kcp-syncer-kind-25coemaz\" 2. write access to apiresourceimports. Creating or updating cluster role binding \"kcp-syncer-kind-25coemaz\" to bind service account \"kcp-syncer-kind-25coemaz\" to cluster role \"kcp-syncer-kind-25coemaz\". Wrote physical cluster manifest to syncer-kind-main.yaml for namespace \"kcp-syncer-kind-25coemaz\". Use KUBECONFIG=<pcluster-config> kubectl apply -f \"syncer-kind-main.yaml\" to apply it. Use KUBECONFIG=<pcluster-config> kubectl get deployment -n \"kcp-syncer-kind-25coemaz\" kcp-syncer-kind-25coemaz to verify the syncer pod is running. Next, we need to install the syncer pod on our kind cluster - this is what actually syncs content from kcp to the physical cluster. The kind cluster needs to be running by now. Run the following command: $ KUBECONFIG=</path/to/kind/kubeconfig> kubectl apply -f \"syncer-kind-main.yaml\" namespace/kcp-syncer-kind-25coemaz created serviceaccount/kcp-syncer-kind-25coemaz created secret/kcp-syncer-kind-25coemaz-token created clusterrole.rbac.authorization.k8s.io/kcp-syncer-kind-25coemaz created clusterrolebinding.rbac.authorization.k8s.io/kcp-syncer-kind-25coemaz created secret/kcp-syncer-kind-25coemaz created deployment.apps/kcp-syncer-kind-25coemaz created","title":"Configure kcp to sync to your cluster"},{"location":"content/en/#bind-to-workload-apis-and-create-default-placement","text":"If you are running kcp version v0.10.0 or higher, you will need to run the following commmand (continuing in the root workspace) to create a binding to the workload APIs export and a default placement for your physical cluster: $ kubectl kcp bind compute root Binding APIExport \"root:compute:kubernetes\". placement placement-1pfxsevk created. Placement \"placement-1pfxsevk\" is ready.","title":"Bind to workload APIs and create default placement"},{"location":"content/en/#create-a-deployment-in-kcp","text":"Let's create a deployment in our kcp workspace and see it get synced to our cluster: $ kubectl create deployment --image=gcr.io/kuar-demo/kuard-amd64:blue --port=8080 kuard deployment.apps/kuard created Once your cluster has pulled the image and started the pod, you should be able to verify the deployment is running in kcp: $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE kuard 1/1 1 1 3s We are still working on adding support for kubectl logs , kubectl exec , and kubectl port-forward to kcp. For the time being, you can check directly in your cluster. kcp translates the names of namespaces in workspaces to unique names in a physical cluster. We first must get this translated name; if you're following along, your translated name might be different. $ KUBECONFIG=</path/to/kind/kubeconfig> kubectl get pods --all-namespaces --selector app=kuard NAMESPACE NAME READY STATUS RESTARTS AGE kcp-26zq2mc2yajx kuard-7d49c786c5-wfpcc 1/1 Running 0 4m28s Now we can e.g. check the pod logs: $ KUBECONFIG=</path/to/kind/kubeconfig> kubectl --namespace kcp-26zq2mc2yajx logs deployment/kuard | head 2022/09/07 14:04:35 Starting kuard version: v0.10.0-blue 2022/09/07 14:04:35 ********************************************************************** 2022/09/07 14:04:35 * WARNING: This server may expose sensitive 2022/09/07 14:04:35 * and secret information. Be careful. 2022/09/07 14:04:35 ********************************************************************** 2022/09/07 14:04:35 Config: { \"address\": \":8080\", \"debug\": false, \"debug-sitedata-dir\": \"./sitedata\",","title":"Create a deployment in kcp"},{"location":"content/en/#next-steps","text":"Thanks for checking out our quickstart! If you're interested in learning more about all the features kcp has to offer, please check out our additional documentation: Concepts - a high level overview of kcp concepts Workspaces - a more thorough introduction on kcp's workspaces Locations & scheduling - details on kcp's primitives that abstract over clusters Syncer - information on running the kcp agent that syncs content between kcp and a physical cluster kubectl plugin Authorization - how kcp manages access control to workspaces and content Virtual workspaces - details on kcp's mechanism for virtual views of workspace content","title":"Next steps"},{"location":"content/en/#contributing","text":"We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing and Developer guides.","title":"Contributing"},{"location":"content/en/#getting-in-touch","text":"There are several ways to communicate with us: The #kcp-dev channel in the Kubernetes Slack workspace Our mailing lists: kcp-dev for development discussions kcp-users for discussions among users and potential users Subscribe to the community calendar for community meetings and events The kcp-dev mailing list is subscribed to this calendar See recordings of past community meetings on YouTube See upcoming and past community meeting agendas and notes Browse the shared Google Drive to share design docs, notes, etc. Members of the kcp-dev mailing list can view this drive","title":"Getting in touch"},{"location":"content/en/#additional-references","text":"KubeCon EU 2021: Kubernetes as the Hybrid Cloud Control Plane Keynote - Clayton Coleman (video) OpenShift Commons: Kubernetes as the Control Plane for the Hybrid Cloud - Clayton Coleman (video) TGI Kubernetes 157: Exploring kcp: apiserver without Kubernetes K8s SIG Architecture meeting discussing kcp - June 29, 2021 Let's Learn kcp - A minimal Kubernetes API server with Saiyam Pathak - July 7, 2021","title":"Additional references"},{"location":"content/en/CONTRIBUTING/","text":"Contributing to kcp kcp is Apache 2.0 licensed and we accept contributions via GitHub pull requests. Please read the following guide if you're interested in contributing to kcp. Certificate of Origin By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details. Getting started Prerequisites Clone this repository. Install Go (1.19+). Install kubectl . Build & verify In one terminal, build and start kcp : go run ./cmd/kcp start In another terminal, tell kubectl where to find the kubeconfig: export KUBECONFIG=.kcp/admin.kubeconfig Confirm you can connect to kcp : kubectl api-resources Finding areas to contribute Starting to participate in a new project can sometimes be overwhelming, and you may not know where to begin. Fortunately, we are here to help! We track all of our tasks here in GitHub, and we label our issues to categorize them. Here are a couple of handy links to check out: Good first issue issues Help wanted issues You're certainly not limited to only these kinds of issues, though! If you're comfortable, please feel free to try working on anything that is open. We do use the assignee feature in GitHub for issues. If you find an unassigned issue, comment asking if you can be assigned, and ideally wait for a maintainer to respond. If you find an assigned issue and you want to work on it or help out, please reach out to the assignee first. Sometimes you might get an amazing idea and start working on a huge amount of code. We love and encourage excitement like this, but we do ask that before you embarking on a giant pull request, please reach out to the community first for an initial discussion. You could file an issue , send a discussion to our mailing list , and/or join one of our community meetings . Finally, we welcome and value all types of contributions, beyond \"just code\"! Other types include triaging bugs, tracking down and fixing flaky tests, improving our documentation, helping answer community questions, proposing and reviewing designs, etc. Priorities & milestones We prioritize issues and features both synchronously (during community meetings) and asynchronously (Slack/GitHub conversations). We group issues together into milestones. Each milestone represents a set of new features and bug fixes that we want users to try out. We aim for each milestone to take about a month from start to finish. You can see the current list of milestones in GitHub. For a given issue or pull request, its milestone may be: unset/unassigned : we haven't looked at this yet, or if we have, we aren't sure if we want to do it and it needs more community discussion assigned to a named milestone assigned to TBD - we have looked at this, decided that it is important and we eventually would like to do it, but we aren't sure exactly when If you are confident about the target milestone for your issue or PR, please set it. If you don\u2019t have permissions, please ask & we\u2019ll set it for you. Epics We use the epic label to track large features that typically involve multiple stories. When creating a new epic, please use the epic issue template . Please make sure that you fill in all the sections of the template (it's ok if some of this is done later, after creating the issue). If you need help with anything, please let us know. Story tasks Story tasks in an epic should generally represent an independent chunk of work that can be implemented. These don't necessarily need to be copied to standalone GitHub issues; it's ok if we just track the story in the epic as a task. On a case by case basis, if a story seems large enough that it warrants its own issue, we can discuss creating one. Please tag yourself using your GitHub handle next to a story task you plan to work on. If you don't have permission to do this, please let us know by either commenting on the issue, or reaching out in Slack, and we'll assist you. When you open a PR for a story task, please edit the epic description and add a link to the PR next to your task. When the PR has been merged, please make sure the task is checked off in the epic. Tracking work Issue status and project board We use the Github projects beta for project management, compare our project board . Please add issues and PRs into the kcp project and update the status (new, in-progress, ...) for those you are actively working on. Unplanned/untracked work If you find yourself working on something that is unplanned and/or untracked (i.e., not an open GitHub issue or story task in an epic), that's 100% ok, but we'd like to track this type of work too! Please file a new issue for it, and when you have a PR ready, mark the PR as fixing the issue. Coding guidelines & conventions Always be clear about what clients or client configs target. Never use an unqualified client . Instead, always qualify. For example: rootClient orgClient pclusterClient rootKcpClient orgKubeClient Configs intended for NewForConfig (i.e. today often called \"admin workspace config\") should uniformly be called clusterConfig Note: with org workspaces, kcp will no longer default clients to the \"root\" (\"admin\") logical cluster Note 2: sometimes we use clients for same purpose, but this can be harder to read Cluster-aware clients should follow similar naming conventions: crdClusterClient kcpClusterClient kubeClusterClient clusterName is a kcp term. It is NOT a name of a physical cluster. If we mean the latter, use pclusterName or similar. In the syncer: upstream = kcp, downstream = pcluster. Depending on direction, \"from\" and \"to\" can have different meanings. source and sink are synonyms for upstream and downstream. Qualify \"namespace\"s in code that handle up- and downstream, e.g. upstreamNamespace , downstreamNamespace , and also upstreamObj , downstreamObj . Logging: Use the fmt.Sprintf(\"%s|%s/%s\", clusterName, namespace, name syntax. Default log-level is 2. Controllers should generally log (a) one line (not more) non-error progress per item with klog.V(2) (b) actions like create/update/delete via klog.V(3) and (c) skipped actions, i.e. what was not done for reasons via klog.V(4) . When orgs land: clusterName or fooClusterName is always the fully qualified value that you can stick into obj.ObjectMeta.ClusterName. It's not necessarily the (Cluster)Workspace.Name from the object. For the latter, use workspaceName or orgName . Generally do klog.Errorf or return err , but not both together. If you need to make it clear where an error came from, you can wrap it. New features start under a feature-gate ( --feature-gate GateName=true ). (At some point in the future), new feature-gates are off by default at least until the APIs are promoted to beta (we are not there before we have reached MVP). Feature-gated code can be incomplete. Also their e2e coverage can be incomplete. We do not compromise on unit tests . Every feature-gated code needs full unit tests as every other code-path. Go Proverbs are good guidelines for style: https://go-proverbs.github.io/ \u2013 watch https://www.youtube.com/watch?v=PAAkCSZUG1c. We use https://github.com/stretchr/testify/tree/master/require a lot in tests, and avoid https://github.com/stretchr/testify/tree/master/assert . Note this subtle distinction of nested require statements: Golang require.Eventually(t, func() bool { foos, err := client.List(...) require.NoError(err) // fail fast, including failing require.Eventually immediately return someCondition(foos) }, ...) and Golang require.Eventually(t, func() bool { foos, err := client.List(...) if err != nil { return false // keep trying } return someCondition(foos) }, ...) The first fails fast on every client error. The second ignores client errors and keeps trying. Either has its place, depending on whether the client error is to be expected (e.g. because of asynchronicity making the resource available), or signals a real test problem. Using Kubebuilder CRD Validation Annotations All of the built-in types for kcp are CustomResourceDefinitions , and we generate YAML spec for them from our Go types using kubebuilder . When adding a field that requires validation, custom annotations are used to translate this logic into the generated OpenAPI spec. This doc gives an overview of possible validations. These annotations map directly to concepts in the OpenAPI Spec so, for instance, the format of strings is defined there, not in kubebuilder. Furthermore, Kubernetes has forked the OpenAPI project here and extends more formats in the extensions-apiserver here . Getting your PR Merged The kcp project uses OWNERS files to denote the collaborators who can assist you in getting your PR merged. There are two roles: reviewer and approver. Merging a PR requires sign off from both a reviewer and an approver. Continuous Integration kcp uses a combination of GitHub Actions and and prow to automate the build process. Here are the most important links: .github/workflows/ci.yaml defines the Github Actions based jobs. openshift/release/ci-operator/config/kcp-dev/kcp defines the prow based jobs. OpenShift CI docs describes the OpenShift CI system that kcp is currently piggy backing on. Testgrid shows CI statistics. Debugging flakes Tests that sometimes pass and sometimes fail are known as flakes. Sometimes, there is only an issue with the test, while other times, there is an actual bug in the main code. Regardless of the root cause, it's important to try to eliminate as many flakes as possible. Unit test flakes If you're trying to debug a unit test flake, you can try to do something like this: go test -race ./pkg/reconciler/apis/apibinding -run TestReconcileBinding -count 100 -failfast This tests one specific package, running only a single test case by name, 100 times in a row. It fails as soon as it encounters any failure. If this passes, it may still be possible there is a flake somewhere, so you may need to run it a few times to be certain. If it fails, that's a great sign - you've been able to reproduce it locally. Now you need to dig into the test condition that is failing. Work backwards from the condition and try to determine if the condition is correct, and if it should be that way all the time. Look at the code under test and see if there are any reasons things might not be deterministic. End to end test flakes Debugging an end-to-end (e2e) test that is flaky can be a bit trickier than a unit test. Our e2e tests run in one of two modes: Tests share a single kcp server Tests in a package share a single kcp server The e2e-shared-server CI job uses mode 1, and the e2e CI job uses mode 2. There are also a handful of tests that require a fully isolated kcp server, because they manipulate some configuration aspects that are system-wide and would break all the other tests. These tests run in both e2e and e2e-shared-server , separate from the other kcp instance(s). You can use the same run , -count , and -failfast settings from the unit test section above for trying to reproduce e2e flakes locally. Additionally, if you would like to operate in mode 1 (all tests share a single kcp server), you can start a kcp instance locally in a separate terminal or tab: bin/test-server Then, to have your test use that shared kcp server, you add -args --use-default-kcp-server to your go test run: go test ./test/e2e/apibinding -count 20 -failfast -args --use-default-kcp-server Community Roles Reviewers Reviewers are responsible for reviewing code for correctness and adherence to standards. Oftentimes reviewers will be able to advise on code efficiency and style as it relates to golang or project conventions as well as other considerations that might not be obvious to the contributor. Approvers Approvers are responsible for sign-off on the acceptance of the contribution. In essence, approval indicates that the change is desired and good for the project, aligns with code, api, and system conventions, and appears to follow all required process including adequate testing, documentation, follow ups, or notifications to other areas who might be interested or affected by the change. Approvers are also reviewers. Management of OWNERS files If a reviewer or approver no longer wishes to be in their current role it is requested that a PR be opened to update the OWNERS file. OWNERS files may be periodically reviewed and updated based on project activity or feedback to ensure an acceptable contributor experience is maintained.","title":"Contributing to kcp"},{"location":"content/en/CONTRIBUTING/#contributing-to-kcp","text":"kcp is Apache 2.0 licensed and we accept contributions via GitHub pull requests. Please read the following guide if you're interested in contributing to kcp.","title":"Contributing to kcp"},{"location":"content/en/CONTRIBUTING/#certificate-of-origin","text":"By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.","title":"Certificate of Origin"},{"location":"content/en/CONTRIBUTING/#getting-started","text":"","title":"Getting started"},{"location":"content/en/CONTRIBUTING/#prerequisites","text":"Clone this repository. Install Go (1.19+). Install kubectl .","title":"Prerequisites"},{"location":"content/en/CONTRIBUTING/#build-verify","text":"In one terminal, build and start kcp : go run ./cmd/kcp start In another terminal, tell kubectl where to find the kubeconfig: export KUBECONFIG=.kcp/admin.kubeconfig Confirm you can connect to kcp : kubectl api-resources","title":"Build &amp; verify"},{"location":"content/en/CONTRIBUTING/#finding-areas-to-contribute","text":"Starting to participate in a new project can sometimes be overwhelming, and you may not know where to begin. Fortunately, we are here to help! We track all of our tasks here in GitHub, and we label our issues to categorize them. Here are a couple of handy links to check out: Good first issue issues Help wanted issues You're certainly not limited to only these kinds of issues, though! If you're comfortable, please feel free to try working on anything that is open. We do use the assignee feature in GitHub for issues. If you find an unassigned issue, comment asking if you can be assigned, and ideally wait for a maintainer to respond. If you find an assigned issue and you want to work on it or help out, please reach out to the assignee first. Sometimes you might get an amazing idea and start working on a huge amount of code. We love and encourage excitement like this, but we do ask that before you embarking on a giant pull request, please reach out to the community first for an initial discussion. You could file an issue , send a discussion to our mailing list , and/or join one of our community meetings . Finally, we welcome and value all types of contributions, beyond \"just code\"! Other types include triaging bugs, tracking down and fixing flaky tests, improving our documentation, helping answer community questions, proposing and reviewing designs, etc.","title":"Finding areas to contribute"},{"location":"content/en/CONTRIBUTING/#priorities-milestones","text":"We prioritize issues and features both synchronously (during community meetings) and asynchronously (Slack/GitHub conversations). We group issues together into milestones. Each milestone represents a set of new features and bug fixes that we want users to try out. We aim for each milestone to take about a month from start to finish. You can see the current list of milestones in GitHub. For a given issue or pull request, its milestone may be: unset/unassigned : we haven't looked at this yet, or if we have, we aren't sure if we want to do it and it needs more community discussion assigned to a named milestone assigned to TBD - we have looked at this, decided that it is important and we eventually would like to do it, but we aren't sure exactly when If you are confident about the target milestone for your issue or PR, please set it. If you don\u2019t have permissions, please ask & we\u2019ll set it for you.","title":"Priorities &amp; milestones"},{"location":"content/en/CONTRIBUTING/#epics","text":"We use the epic label to track large features that typically involve multiple stories. When creating a new epic, please use the epic issue template . Please make sure that you fill in all the sections of the template (it's ok if some of this is done later, after creating the issue). If you need help with anything, please let us know.","title":"Epics"},{"location":"content/en/CONTRIBUTING/#story-tasks","text":"Story tasks in an epic should generally represent an independent chunk of work that can be implemented. These don't necessarily need to be copied to standalone GitHub issues; it's ok if we just track the story in the epic as a task. On a case by case basis, if a story seems large enough that it warrants its own issue, we can discuss creating one. Please tag yourself using your GitHub handle next to a story task you plan to work on. If you don't have permission to do this, please let us know by either commenting on the issue, or reaching out in Slack, and we'll assist you. When you open a PR for a story task, please edit the epic description and add a link to the PR next to your task. When the PR has been merged, please make sure the task is checked off in the epic.","title":"Story tasks"},{"location":"content/en/CONTRIBUTING/#tracking-work","text":"","title":"Tracking work"},{"location":"content/en/CONTRIBUTING/#issue-status-and-project-board","text":"We use the Github projects beta for project management, compare our project board . Please add issues and PRs into the kcp project and update the status (new, in-progress, ...) for those you are actively working on.","title":"Issue status and project board"},{"location":"content/en/CONTRIBUTING/#unplanneduntracked-work","text":"If you find yourself working on something that is unplanned and/or untracked (i.e., not an open GitHub issue or story task in an epic), that's 100% ok, but we'd like to track this type of work too! Please file a new issue for it, and when you have a PR ready, mark the PR as fixing the issue.","title":"Unplanned/untracked work"},{"location":"content/en/CONTRIBUTING/#coding-guidelines-conventions","text":"Always be clear about what clients or client configs target. Never use an unqualified client . Instead, always qualify. For example: rootClient orgClient pclusterClient rootKcpClient orgKubeClient Configs intended for NewForConfig (i.e. today often called \"admin workspace config\") should uniformly be called clusterConfig Note: with org workspaces, kcp will no longer default clients to the \"root\" (\"admin\") logical cluster Note 2: sometimes we use clients for same purpose, but this can be harder to read Cluster-aware clients should follow similar naming conventions: crdClusterClient kcpClusterClient kubeClusterClient clusterName is a kcp term. It is NOT a name of a physical cluster. If we mean the latter, use pclusterName or similar. In the syncer: upstream = kcp, downstream = pcluster. Depending on direction, \"from\" and \"to\" can have different meanings. source and sink are synonyms for upstream and downstream. Qualify \"namespace\"s in code that handle up- and downstream, e.g. upstreamNamespace , downstreamNamespace , and also upstreamObj , downstreamObj . Logging: Use the fmt.Sprintf(\"%s|%s/%s\", clusterName, namespace, name syntax. Default log-level is 2. Controllers should generally log (a) one line (not more) non-error progress per item with klog.V(2) (b) actions like create/update/delete via klog.V(3) and (c) skipped actions, i.e. what was not done for reasons via klog.V(4) . When orgs land: clusterName or fooClusterName is always the fully qualified value that you can stick into obj.ObjectMeta.ClusterName. It's not necessarily the (Cluster)Workspace.Name from the object. For the latter, use workspaceName or orgName . Generally do klog.Errorf or return err , but not both together. If you need to make it clear where an error came from, you can wrap it. New features start under a feature-gate ( --feature-gate GateName=true ). (At some point in the future), new feature-gates are off by default at least until the APIs are promoted to beta (we are not there before we have reached MVP). Feature-gated code can be incomplete. Also their e2e coverage can be incomplete. We do not compromise on unit tests . Every feature-gated code needs full unit tests as every other code-path. Go Proverbs are good guidelines for style: https://go-proverbs.github.io/ \u2013 watch https://www.youtube.com/watch?v=PAAkCSZUG1c. We use https://github.com/stretchr/testify/tree/master/require a lot in tests, and avoid https://github.com/stretchr/testify/tree/master/assert . Note this subtle distinction of nested require statements: Golang require.Eventually(t, func() bool { foos, err := client.List(...) require.NoError(err) // fail fast, including failing require.Eventually immediately return someCondition(foos) }, ...) and Golang require.Eventually(t, func() bool { foos, err := client.List(...) if err != nil { return false // keep trying } return someCondition(foos) }, ...) The first fails fast on every client error. The second ignores client errors and keeps trying. Either has its place, depending on whether the client error is to be expected (e.g. because of asynchronicity making the resource available), or signals a real test problem.","title":"Coding guidelines &amp; conventions"},{"location":"content/en/CONTRIBUTING/#using-kubebuilder-crd-validation-annotations","text":"All of the built-in types for kcp are CustomResourceDefinitions , and we generate YAML spec for them from our Go types using kubebuilder . When adding a field that requires validation, custom annotations are used to translate this logic into the generated OpenAPI spec. This doc gives an overview of possible validations. These annotations map directly to concepts in the OpenAPI Spec so, for instance, the format of strings is defined there, not in kubebuilder. Furthermore, Kubernetes has forked the OpenAPI project here and extends more formats in the extensions-apiserver here .","title":"Using Kubebuilder CRD Validation Annotations"},{"location":"content/en/CONTRIBUTING/#getting-your-pr-merged","text":"The kcp project uses OWNERS files to denote the collaborators who can assist you in getting your PR merged. There are two roles: reviewer and approver. Merging a PR requires sign off from both a reviewer and an approver.","title":"Getting your PR Merged"},{"location":"content/en/CONTRIBUTING/#continuous-integration","text":"kcp uses a combination of GitHub Actions and and prow to automate the build process. Here are the most important links: .github/workflows/ci.yaml defines the Github Actions based jobs. openshift/release/ci-operator/config/kcp-dev/kcp defines the prow based jobs. OpenShift CI docs describes the OpenShift CI system that kcp is currently piggy backing on. Testgrid shows CI statistics.","title":"Continuous Integration"},{"location":"content/en/CONTRIBUTING/#debugging-flakes","text":"Tests that sometimes pass and sometimes fail are known as flakes. Sometimes, there is only an issue with the test, while other times, there is an actual bug in the main code. Regardless of the root cause, it's important to try to eliminate as many flakes as possible.","title":"Debugging flakes"},{"location":"content/en/CONTRIBUTING/#unit-test-flakes","text":"If you're trying to debug a unit test flake, you can try to do something like this: go test -race ./pkg/reconciler/apis/apibinding -run TestReconcileBinding -count 100 -failfast This tests one specific package, running only a single test case by name, 100 times in a row. It fails as soon as it encounters any failure. If this passes, it may still be possible there is a flake somewhere, so you may need to run it a few times to be certain. If it fails, that's a great sign - you've been able to reproduce it locally. Now you need to dig into the test condition that is failing. Work backwards from the condition and try to determine if the condition is correct, and if it should be that way all the time. Look at the code under test and see if there are any reasons things might not be deterministic.","title":"Unit test flakes"},{"location":"content/en/CONTRIBUTING/#end-to-end-test-flakes","text":"Debugging an end-to-end (e2e) test that is flaky can be a bit trickier than a unit test. Our e2e tests run in one of two modes: Tests share a single kcp server Tests in a package share a single kcp server The e2e-shared-server CI job uses mode 1, and the e2e CI job uses mode 2. There are also a handful of tests that require a fully isolated kcp server, because they manipulate some configuration aspects that are system-wide and would break all the other tests. These tests run in both e2e and e2e-shared-server , separate from the other kcp instance(s). You can use the same run , -count , and -failfast settings from the unit test section above for trying to reproduce e2e flakes locally. Additionally, if you would like to operate in mode 1 (all tests share a single kcp server), you can start a kcp instance locally in a separate terminal or tab: bin/test-server Then, to have your test use that shared kcp server, you add -args --use-default-kcp-server to your go test run: go test ./test/e2e/apibinding -count 20 -failfast -args --use-default-kcp-server","title":"End to end test flakes"},{"location":"content/en/CONTRIBUTING/#community-roles","text":"","title":"Community Roles"},{"location":"content/en/CONTRIBUTING/#reviewers","text":"Reviewers are responsible for reviewing code for correctness and adherence to standards. Oftentimes reviewers will be able to advise on code efficiency and style as it relates to golang or project conventions as well as other considerations that might not be obvious to the contributor.","title":"Reviewers"},{"location":"content/en/CONTRIBUTING/#approvers","text":"Approvers are responsible for sign-off on the acceptance of the contribution. In essence, approval indicates that the change is desired and good for the project, aligns with code, api, and system conventions, and appears to follow all required process including adequate testing, documentation, follow ups, or notifications to other areas who might be interested or affected by the change. Approvers are also reviewers.","title":"Approvers"},{"location":"content/en/CONTRIBUTING/#management-of-owners-files","text":"If a reviewer or approver no longer wishes to be in their current role it is requested that a PR be opened to update the OWNERS file. OWNERS files may be periodically reviewed and updated based on project activity or feedback to ensure an acceptable contributor experience is maintained.","title":"Management of OWNERS files"},{"location":"content/en/GOALS/","text":"Project Goals !!! warning This is a prototype! It is not production software, or a fully realized project with a definite road map. In the short term, it is to serve as a test bed for some opinionated multi-cluster concepts. This document describes the aspirations and inspirations and is written in a \"this is what we could do\" style, not \"what we do today\". kcp can be used to manage Kubernetes-like applications across one or more clusters and integrate with cloud services. To an end user, kcp should appear to be a normal cluster (supports the same APIs, client tools, and extensibility) but allows you to move your workloads between clusters or span multiple clusters without effort. kcp lets you keep your existing workflow and abstract Kube clusters like a Kube cluster abstracts individual machines. kcp also helps broaden the definition of \"Kubernetes applications\" by being extensible, only loosely dependent on nodes, pods, or clusters, and thinking more broadly about what an application is than \"just some containers\". What should it do for me? 1-3 years As an ecosystem participant, kcp is a reusable component that allows you to: Build your own secure control planes As an application team, kcp allows you to: Deploy services, serverless applications, and containers side by side using familiar declarative config tooling from the Kubernetes ecosystem Go from the very small (laptop) to the very large (deployed around the world) without changing your development workflow As an application infrastructure team, kcp allows you to: Define how your application teams work and integrate across machines, clusters, clouds, and environments without having to switch context Provide the tools for keeping your services resilient, observable, up-to-date, and profitable across any computing environment you choose to leverage These first two areas are deliberately broad - they reflect where we think we as an ecosystem should be going even if we may not get there in one step, and to frame what we think is important for the ecosystem. 3-12 months More pragmatically, we think the Kubernetes ecosystem is a great place to start from and so these are the kinds of incremental improvements from where we are today towards that aspirational future: As a Kubernetes application author, kcp allows you to: Take existing Kubernetes applications and set them up to run across one or more clusters even if a cluster fails Set up a development workflow that uses existing Kubernetes tools but brings your diverse environments (local, dev, staging, production) together Run multiple applications side by side in logical clusters As a Kubernetes administrator, kcp allows you to: Support a large number of application teams building applications without giving them access to clusters Have strong tenant separation between different application teams and control who can run where Allow tenant teams to run their own custom resources (CRDs) and controllers without impacting others Subdivide access to the underlying clusters, keep those clusters simpler and with fewer extensions, and reduce the impact of cluster failure As an author of Kubernetes extensions, kcp allows you to: Build multi-cluster integrations more easily by providing standard ways to abstract multi-cluster actions like placement/scheduling, admission, and recovery Test and run Kubernetes CRDs and controllers in isolation without needing a full cluster As a Kubernetes community member, kcp is intended to: Solve problems that benefit both regular Kubernetes clusters and standalone kcp Improve low level tooling for client authors writing controllers across multiple namespaces and clusters Be a reusable building block for ambitious control-plane-for-my-apps platforms The Manifesto Our mission is to improve building and running cloud-native applications. We see a convergence in tooling and technology between clusters, clouds, and services as being both possible and desirable and this prototype explores how the existing Kubernetes ecosystem might evolve to serve that need. Not every idea below may bear fruit, but it's never the wrong time to look for new ways to change. Key Concepts Use Kubernetes APIs to decouple desired intent and actual state for replicating applications to multiple clusters Kubernetes' strength is separating user intent from actual state so that machines can ensure recovery as infrastructure changes. Since clusters are intended to be a single failure domain, by separating the desired state from any one \"real\" cluster we can potentially unlock better resiliency, simpler workload isolation, and allow workloads to move through the dev/stage/prod pipeline more cleanly. If we can keep familiar tools and APIs working, but separate the app just a bit from the cluster, that can help us move and react to failure more effectively. Virtualize some key user focused Kube APIs so that the control plane can delegate complexity to a target cluster The Kubernetes APIs layer on top of each other and compose loosely. Some concepts like Deployments are well suited for delegation because they are self-contained - the spec describes the goal and status summarizes whether the goal is reached. The same goes for a PersistentVolumeClaim - you ask for storage and it follows your pod around a cluster - you don't really care about the details. On the other hand, you definitely need to get Pod logs to debug problems, and Services have a lot of cluster specific meaning (like DNS and the cluster IP). To scale, we need to let the real clusters focus on keeping the workload running, and keep the control plane at a higher level, and that may require us to pretend to have pods on the control plane while actually delegating to the underlying cluster. Identify and invest in workload APIs and integrations that enable applications to spread across clusters transparently Multi-cluster workload scheduling and placement has a rich history within Kubernetes from the very beginning of the project, starting with Kubernetes federation v1 . Even today, projects like karmada are exploring how to take Kube APIs and make them work across multiple clusters. We want to amplify their ideas by improving the control plane itself - make it easy to plug in a workload orchestration system above Kube that still feels like Kube, without having a pesky cluster sitting around. See the investigations doc for transparent multi-cluster for more. Use logical tenant clusters as the basis for application and security isolation Allow a single kube-apiserver to support multiple (up to 1000) logical clusters that can map/sync/schedule to zero or many physical clusters. Each logical cluster could be much more focused - only the resources needed to support a single application or team, but with the ability to scale to lots of applications. Because the logical clusters are served by the same server, we could amortize the cost of each individual cluster (things like RBAC, CRDs, and authentication can be shared / hierarchal). We took inspiration from the virtual cluster project within sig-multicluster as well as vcluster and other similar approaches that leverage cluster tenancy which led us to ask if we could make those clusters an order of magnitude cheaper by building within the kube-apiserver rather than running full copies. Most applications are small, which means amortizing costs can become a huge win. Single process sharing would let us embed significantly more powerful tenancy concepts like hierarchy across clusters, virtualizing key interfaces, and a much more resilient admission chain than what can be done in webhooks. See the investigations doc for logical clusters for more. Most importantly, if clusters are cheap, we can: Support stronger tenancy and isolation of CRDs and applications Lots of little clusters gives us the opportunity to improve how CRDs can be isolated (for development or individual teams), shared (one source for many consumers), and evolved (identify and flag incompatibilities between APIs provided by different clusters). A control plane above Kubernetes lets us separate the \"data plane\" of controllers/integrations from the infrastructure that runs them and allows for centralization of integrations. If you have higher level workloads, talking to higher level abstractions like cloud services, and the individual clusters are just a component, suddenly integrating new patterns and controls becomes more valuable. Conversely, if we have a control plane and a data plane, the types of integrations at each level can begin to differ. More powerful integrations to physical clusters might be run only by infrastructure operations teams, while application integrations could be safely namespaced within the control plane. Likewise, as we split up applications into smaller chunks, we can more carefully define their dependencies. The account service from the identity team doesn't need to know the details of the frontend website or even where or how it runs. Instead, teams could have the freedom of their own personal clusters, with the extensions they need, without being able to access the details of their peer's except by explicit contract. If we can make extending Kubernetes more interesting by providing this higher level control plane, we likewise need to deal with the scalability of that extensibility: Make Kubernetes controllers more scalable and flexible on both the client and the server Subdividing one cluster into hundreds makes integrations harder - a controller would need to be able to access resources across all of those clusters (whether logical, virtual, or physical). For this model to work, we need to explore improvements to the Kubernetes API that would make multi-cluster controllers secure and easy. That involves ideas like watching multiple resources at the same time, listing or watching in bulk across lots of logical clusters, filtering server side, and better client tooling. Many of these improvements could also benefit single-cluster use cases and scalability. To go further, standardizing some of the multi-cluster concepts (whether scheduling, location, or resiliency) into widely used APIs could benefit everyone in the Kubernetes ecosystem, as we often end up building and rebuilding custom platform tooling. The best outcome would be small incremental improvements across the entire Kube ecosystem leading to increased reuse and a reduced need to invest in specific solutions, regardless of the level of the project. Finally, the bar is still high to writing controllers. Lowering the friction of automation and integration is in everyone's benefit - whether that's a bash script, a Terraform configuration, or custom SRE services. If we can reduce the cost of both infrastructure as code and new infrastructure APIs we can potentially make operational investments more composable. See the investigations doc for minimal API server for more on improving the composability of the Kube API server. Drive new workload APIs and explore standardization in the ecosystem There are hundreds of ways to build and run applications, and that will never change. The key success of Kubernetes was offering \"good enough\" standardized deployment, which created a center of gravity for the concepts around deployment. There are plenty of deployments that will never run in containers yet consume them daily. Aligning the deployment of multiple types of workloads from common CI/CD tooling at a higher level, as well as abstracting their dependencies, is something in widespread practice today. Beyond deployment, we could look at connections between these applications (networking, security, identity, access) and find ways to bridge the operational divide between cloud and cluster. That might include expanding existing APIs like PersistentVolumeClaims so your data can follow you across clusters or services. Or documenting a selection of choices for multi-cluster networking that simplify assumptions apps need to make. Or even ways of connecting cluster and cloud resources more directly via unified identity, service meshes, and proxies (all of which are hot topics in our ecosystem). Process Right now we are interested in assessing how these goals fit within the larger ecosystem. The investigations directory is where we will capture specific deep dive details. Terminology We've attempted to pick novel terms for concepts introduced here so as not to conflict or confuse existing projects, but if you do spot problems let us know. logical cluster - a cluster that looks and acts like a Kube cluster but is not served by kube-apiserver (as distinct from virtual clusters in the upstream which are instances of kube-apiserver). physical cluster - a cluster with nodes, a kube-apiserver or equivalent tied to the standard APIs. Logical clusters might be indistingushable from physical clusters in some cases, but not always. kcp the prototype - where we are today kcp the generic control plane - a hypothetical future control plane leveraging kubernetes API tooling but not tied to kube the container orchestrator that can support diverse and interesting workloads kcp the kube control plane - a hypothetical future control plane for existing Kube applications that makes multi-cluster easy, superset of the generic control plane kcp the extensible library - a hypothetical golang library that can be embedded to make developing custom control planes easier Principles Principles are the high level guiding rules we'd like to frame designs around. This is generally useful for resolving design debates by finding thematic connections that reinforce other choices. A few early principles have been discussed: Convention over configuration / optimize for the user's benefit Do as much as possible for the user the \"right way by default\" (conventions over configuration). For example, kcp embeds the data store for local iteration, but still allows (should allow) remote etcd. Support both a push model and a pull model that fit the control plane mindset Both push (control plane tells others what to do) and pull (agents derive truth from control plane) models have their place. Pull works well when pulling small amounts of desired state and when local resiliency is desired as well as to create a security boundary. Push works well in simple getting started scenarios and when the process is \"acting on behalf\" of a user. For example, kcp and the cluster-controller example in the demo can work in both the push model (talk to each cluster to grab CRDs and sync resources) and the pull model (run as a separate controller so that customized security rules could be in place). Users should have the ability to pick the right tradeoff for their scale and how their control planes are structured. Balance between speed of local development AND running as a high scale service The prototype should not overly bias towards \"just the demo\" (in the long run) or take explicit steps that would prevent it from becoming a real project that could make control-plane-as-a-service possible in the future (in the short run). The best outcome would be a simple tool that works at multiple scales and layers well. Be simple, composable, and orthogonal The core Kubernetes model is made of simple composable resources (pods vs services, deployments vs replica sets, persistent volumes vs inline volumes) with a focus on solving a core use case well. kcp should look for the key composable, orthogonal, and \"minimum-viable-simple\" concepts that help people build control planes, support API driven infra across a wide footprint, and provides a center of gravity for \"integrate all the things\". It however should not be afraid to make specific sets of users happy in their daily workflow. Be open to change There is a massive ecosystem of users, vendors, service providers, hackers, operators, developers, and machine AIs (maybe not the last one) building and developing on Kubernetes. This is a starting point, a stake in the ground, a rallying cry. It should challenge, excite, and inspire others, but never limit. As we evolve, we should stay open to new ideas and also opening the door for dramatic rethinks of the possibilities by ourselves or others. Whether this becomes a project, inspires many projects, or fails gloriously, it's about making our lives a bit easier and our tools a bit more reliable, and a meaningful dialogue with the real world is fundamental to success. Consolidate efforts in the ecosystem into a more focused effort Kubernetes is mature and changes to the core happen slowly. By concentrating use cases among a number of participants we can better articulate common needs, focus the design time spent in the core project into a smaller set of efforts, and bring new investment into common shared problems strategically. We should make fast progress and be able to suggest high-impact changes without derailing other important Kubernetes initiatives. Make individual clusters transient / make multi-cluster as easy as multi-node Just like Kubernetes made multi-node use cases trivial for applications, multi-cluster use cases should be trivial with kcp (or at least, the transparent multi-cluster approach). That doesn't eliminate the need to have deep control, just clarifies it.","title":"Project Goals"},{"location":"content/en/GOALS/#project-goals","text":"!!! warning This is a prototype! It is not production software, or a fully realized project with a definite road map. In the short term, it is to serve as a test bed for some opinionated multi-cluster concepts. This document describes the aspirations and inspirations and is written in a \"this is what we could do\" style, not \"what we do today\". kcp can be used to manage Kubernetes-like applications across one or more clusters and integrate with cloud services. To an end user, kcp should appear to be a normal cluster (supports the same APIs, client tools, and extensibility) but allows you to move your workloads between clusters or span multiple clusters without effort. kcp lets you keep your existing workflow and abstract Kube clusters like a Kube cluster abstracts individual machines. kcp also helps broaden the definition of \"Kubernetes applications\" by being extensible, only loosely dependent on nodes, pods, or clusters, and thinking more broadly about what an application is than \"just some containers\".","title":"Project Goals"},{"location":"content/en/GOALS/#what-should-it-do-for-me","text":"","title":"What should it do for me?"},{"location":"content/en/GOALS/#1-3-years","text":"As an ecosystem participant, kcp is a reusable component that allows you to: Build your own secure control planes As an application team, kcp allows you to: Deploy services, serverless applications, and containers side by side using familiar declarative config tooling from the Kubernetes ecosystem Go from the very small (laptop) to the very large (deployed around the world) without changing your development workflow As an application infrastructure team, kcp allows you to: Define how your application teams work and integrate across machines, clusters, clouds, and environments without having to switch context Provide the tools for keeping your services resilient, observable, up-to-date, and profitable across any computing environment you choose to leverage These first two areas are deliberately broad - they reflect where we think we as an ecosystem should be going even if we may not get there in one step, and to frame what we think is important for the ecosystem.","title":"1-3 years"},{"location":"content/en/GOALS/#3-12-months","text":"More pragmatically, we think the Kubernetes ecosystem is a great place to start from and so these are the kinds of incremental improvements from where we are today towards that aspirational future: As a Kubernetes application author, kcp allows you to: Take existing Kubernetes applications and set them up to run across one or more clusters even if a cluster fails Set up a development workflow that uses existing Kubernetes tools but brings your diverse environments (local, dev, staging, production) together Run multiple applications side by side in logical clusters As a Kubernetes administrator, kcp allows you to: Support a large number of application teams building applications without giving them access to clusters Have strong tenant separation between different application teams and control who can run where Allow tenant teams to run their own custom resources (CRDs) and controllers without impacting others Subdivide access to the underlying clusters, keep those clusters simpler and with fewer extensions, and reduce the impact of cluster failure As an author of Kubernetes extensions, kcp allows you to: Build multi-cluster integrations more easily by providing standard ways to abstract multi-cluster actions like placement/scheduling, admission, and recovery Test and run Kubernetes CRDs and controllers in isolation without needing a full cluster As a Kubernetes community member, kcp is intended to: Solve problems that benefit both regular Kubernetes clusters and standalone kcp Improve low level tooling for client authors writing controllers across multiple namespaces and clusters Be a reusable building block for ambitious control-plane-for-my-apps platforms","title":"3-12 months"},{"location":"content/en/GOALS/#the-manifesto","text":"Our mission is to improve building and running cloud-native applications. We see a convergence in tooling and technology between clusters, clouds, and services as being both possible and desirable and this prototype explores how the existing Kubernetes ecosystem might evolve to serve that need. Not every idea below may bear fruit, but it's never the wrong time to look for new ways to change.","title":"The Manifesto"},{"location":"content/en/GOALS/#key-concepts","text":"Use Kubernetes APIs to decouple desired intent and actual state for replicating applications to multiple clusters Kubernetes' strength is separating user intent from actual state so that machines can ensure recovery as infrastructure changes. Since clusters are intended to be a single failure domain, by separating the desired state from any one \"real\" cluster we can potentially unlock better resiliency, simpler workload isolation, and allow workloads to move through the dev/stage/prod pipeline more cleanly. If we can keep familiar tools and APIs working, but separate the app just a bit from the cluster, that can help us move and react to failure more effectively. Virtualize some key user focused Kube APIs so that the control plane can delegate complexity to a target cluster The Kubernetes APIs layer on top of each other and compose loosely. Some concepts like Deployments are well suited for delegation because they are self-contained - the spec describes the goal and status summarizes whether the goal is reached. The same goes for a PersistentVolumeClaim - you ask for storage and it follows your pod around a cluster - you don't really care about the details. On the other hand, you definitely need to get Pod logs to debug problems, and Services have a lot of cluster specific meaning (like DNS and the cluster IP). To scale, we need to let the real clusters focus on keeping the workload running, and keep the control plane at a higher level, and that may require us to pretend to have pods on the control plane while actually delegating to the underlying cluster. Identify and invest in workload APIs and integrations that enable applications to spread across clusters transparently Multi-cluster workload scheduling and placement has a rich history within Kubernetes from the very beginning of the project, starting with Kubernetes federation v1 . Even today, projects like karmada are exploring how to take Kube APIs and make them work across multiple clusters. We want to amplify their ideas by improving the control plane itself - make it easy to plug in a workload orchestration system above Kube that still feels like Kube, without having a pesky cluster sitting around. See the investigations doc for transparent multi-cluster for more. Use logical tenant clusters as the basis for application and security isolation Allow a single kube-apiserver to support multiple (up to 1000) logical clusters that can map/sync/schedule to zero or many physical clusters. Each logical cluster could be much more focused - only the resources needed to support a single application or team, but with the ability to scale to lots of applications. Because the logical clusters are served by the same server, we could amortize the cost of each individual cluster (things like RBAC, CRDs, and authentication can be shared / hierarchal). We took inspiration from the virtual cluster project within sig-multicluster as well as vcluster and other similar approaches that leverage cluster tenancy which led us to ask if we could make those clusters an order of magnitude cheaper by building within the kube-apiserver rather than running full copies. Most applications are small, which means amortizing costs can become a huge win. Single process sharing would let us embed significantly more powerful tenancy concepts like hierarchy across clusters, virtualizing key interfaces, and a much more resilient admission chain than what can be done in webhooks. See the investigations doc for logical clusters for more. Most importantly, if clusters are cheap, we can: Support stronger tenancy and isolation of CRDs and applications Lots of little clusters gives us the opportunity to improve how CRDs can be isolated (for development or individual teams), shared (one source for many consumers), and evolved (identify and flag incompatibilities between APIs provided by different clusters). A control plane above Kubernetes lets us separate the \"data plane\" of controllers/integrations from the infrastructure that runs them and allows for centralization of integrations. If you have higher level workloads, talking to higher level abstractions like cloud services, and the individual clusters are just a component, suddenly integrating new patterns and controls becomes more valuable. Conversely, if we have a control plane and a data plane, the types of integrations at each level can begin to differ. More powerful integrations to physical clusters might be run only by infrastructure operations teams, while application integrations could be safely namespaced within the control plane. Likewise, as we split up applications into smaller chunks, we can more carefully define their dependencies. The account service from the identity team doesn't need to know the details of the frontend website or even where or how it runs. Instead, teams could have the freedom of their own personal clusters, with the extensions they need, without being able to access the details of their peer's except by explicit contract. If we can make extending Kubernetes more interesting by providing this higher level control plane, we likewise need to deal with the scalability of that extensibility: Make Kubernetes controllers more scalable and flexible on both the client and the server Subdividing one cluster into hundreds makes integrations harder - a controller would need to be able to access resources across all of those clusters (whether logical, virtual, or physical). For this model to work, we need to explore improvements to the Kubernetes API that would make multi-cluster controllers secure and easy. That involves ideas like watching multiple resources at the same time, listing or watching in bulk across lots of logical clusters, filtering server side, and better client tooling. Many of these improvements could also benefit single-cluster use cases and scalability. To go further, standardizing some of the multi-cluster concepts (whether scheduling, location, or resiliency) into widely used APIs could benefit everyone in the Kubernetes ecosystem, as we often end up building and rebuilding custom platform tooling. The best outcome would be small incremental improvements across the entire Kube ecosystem leading to increased reuse and a reduced need to invest in specific solutions, regardless of the level of the project. Finally, the bar is still high to writing controllers. Lowering the friction of automation and integration is in everyone's benefit - whether that's a bash script, a Terraform configuration, or custom SRE services. If we can reduce the cost of both infrastructure as code and new infrastructure APIs we can potentially make operational investments more composable. See the investigations doc for minimal API server for more on improving the composability of the Kube API server. Drive new workload APIs and explore standardization in the ecosystem There are hundreds of ways to build and run applications, and that will never change. The key success of Kubernetes was offering \"good enough\" standardized deployment, which created a center of gravity for the concepts around deployment. There are plenty of deployments that will never run in containers yet consume them daily. Aligning the deployment of multiple types of workloads from common CI/CD tooling at a higher level, as well as abstracting their dependencies, is something in widespread practice today. Beyond deployment, we could look at connections between these applications (networking, security, identity, access) and find ways to bridge the operational divide between cloud and cluster. That might include expanding existing APIs like PersistentVolumeClaims so your data can follow you across clusters or services. Or documenting a selection of choices for multi-cluster networking that simplify assumptions apps need to make. Or even ways of connecting cluster and cloud resources more directly via unified identity, service meshes, and proxies (all of which are hot topics in our ecosystem).","title":"Key Concepts"},{"location":"content/en/GOALS/#process","text":"Right now we are interested in assessing how these goals fit within the larger ecosystem. The investigations directory is where we will capture specific deep dive details.","title":"Process"},{"location":"content/en/GOALS/#terminology","text":"We've attempted to pick novel terms for concepts introduced here so as not to conflict or confuse existing projects, but if you do spot problems let us know. logical cluster - a cluster that looks and acts like a Kube cluster but is not served by kube-apiserver (as distinct from virtual clusters in the upstream which are instances of kube-apiserver). physical cluster - a cluster with nodes, a kube-apiserver or equivalent tied to the standard APIs. Logical clusters might be indistingushable from physical clusters in some cases, but not always. kcp the prototype - where we are today kcp the generic control plane - a hypothetical future control plane leveraging kubernetes API tooling but not tied to kube the container orchestrator that can support diverse and interesting workloads kcp the kube control plane - a hypothetical future control plane for existing Kube applications that makes multi-cluster easy, superset of the generic control plane kcp the extensible library - a hypothetical golang library that can be embedded to make developing custom control planes easier","title":"Terminology"},{"location":"content/en/GOALS/#principles","text":"Principles are the high level guiding rules we'd like to frame designs around. This is generally useful for resolving design debates by finding thematic connections that reinforce other choices. A few early principles have been discussed: Convention over configuration / optimize for the user's benefit Do as much as possible for the user the \"right way by default\" (conventions over configuration). For example, kcp embeds the data store for local iteration, but still allows (should allow) remote etcd. Support both a push model and a pull model that fit the control plane mindset Both push (control plane tells others what to do) and pull (agents derive truth from control plane) models have their place. Pull works well when pulling small amounts of desired state and when local resiliency is desired as well as to create a security boundary. Push works well in simple getting started scenarios and when the process is \"acting on behalf\" of a user. For example, kcp and the cluster-controller example in the demo can work in both the push model (talk to each cluster to grab CRDs and sync resources) and the pull model (run as a separate controller so that customized security rules could be in place). Users should have the ability to pick the right tradeoff for their scale and how their control planes are structured. Balance between speed of local development AND running as a high scale service The prototype should not overly bias towards \"just the demo\" (in the long run) or take explicit steps that would prevent it from becoming a real project that could make control-plane-as-a-service possible in the future (in the short run). The best outcome would be a simple tool that works at multiple scales and layers well. Be simple, composable, and orthogonal The core Kubernetes model is made of simple composable resources (pods vs services, deployments vs replica sets, persistent volumes vs inline volumes) with a focus on solving a core use case well. kcp should look for the key composable, orthogonal, and \"minimum-viable-simple\" concepts that help people build control planes, support API driven infra across a wide footprint, and provides a center of gravity for \"integrate all the things\". It however should not be afraid to make specific sets of users happy in their daily workflow. Be open to change There is a massive ecosystem of users, vendors, service providers, hackers, operators, developers, and machine AIs (maybe not the last one) building and developing on Kubernetes. This is a starting point, a stake in the ground, a rallying cry. It should challenge, excite, and inspire others, but never limit. As we evolve, we should stay open to new ideas and also opening the door for dramatic rethinks of the possibilities by ourselves or others. Whether this becomes a project, inspires many projects, or fails gloriously, it's about making our lives a bit easier and our tools a bit more reliable, and a meaningful dialogue with the real world is fundamental to success. Consolidate efforts in the ecosystem into a more focused effort Kubernetes is mature and changes to the core happen slowly. By concentrating use cases among a number of participants we can better articulate common needs, focus the design time spent in the core project into a smaller set of efforts, and bring new investment into common shared problems strategically. We should make fast progress and be able to suggest high-impact changes without derailing other important Kubernetes initiatives. Make individual clusters transient / make multi-cluster as easy as multi-node Just like Kubernetes made multi-node use cases trivial for applications, multi-cluster use cases should be trivial with kcp (or at least, the transparent multi-cluster approach). That doesn't eliminate the need to have deep control, just clarifies it.","title":"Principles"},{"location":"content/en/concepts/","text":"Logical cluster A logical cluster is a way to subdivide a single kube-apiserver + etcd storage into multiple clusters (different APIs, separate semantics for access, policy, and control) without requiring multiple instances. A logical cluster is a mechanism for achieving separation, but may be modelled differently in different use cases. A logical cluster is similar to a virtual cluster as defined by sig-multicluster, but is able to amortize the cost of a new cluster to be zero or near-zero memory and storage so that we can create tens of millions of empty clusters cheaply. A logical cluster is a storage level concept that adds an additional attribute to an object\u2019s identifier on a kube-apiserver. Regular servers identify objects by (group, version, resource, optional namespace, name). A logical cluster enriches an identifier: (group, version, resource, logical cluster name , optional namespace, name). Workload Cluster A physical cluster is a \u201creal Kubernetes cluster\u201d, i.e. one that can run Kubernetes workloads and accepts standard Kubernetes API objects. For the near term, it is assumed that a physical cluster is a distribution of Kubernetes and passes the conformance tests and exposes the behavior a regular Kubernetes admin or user expects. Workspace A workspace models a set of user-facing APIs for CRUD. Each workspace is backed by a logical cluster, but not all logical clusters may be exposed as workspaces. Creating a Workspace object results in a logical cluster being available via a URL for the client to connect and create resources supported by the APIs in that workspace. There could be multiple different models that result in logical clusters being created, with different policies or lifecycles, but Workspace is intended to be the most generic representation of the concept with the broadest possible utility to anyone building control planes. A workspace binds APIs and makes them accessible inside the logical cluster, allocates capacity for creating instances of those APIs (quota), and defines how multi-workspace operations can be performed by users, clients, and controller integrations. To a user, a workspace appears to be a Kubernetes cluster minus all the container orchestration specific resources. It has its own discovery, its own OpenAPI spec, and follows the kube-like constraints about uniqueness of Group-Version-Resource and its behaviour (no two GVRs with different schemas can exist per workspace, but workspaces can have different schemas). A user can define a workspace as a context in a kubeconfig file and kubectl get all -A would return all objects in all namespaces of that workspace. Workspace naming is chosen to be aligned with the Kubernetes Namespace object - a Namespace subdivides a workspace by name, a workspace subdivides the universe into chunks of meaningful work. Workspaces are the containers for all API objects, so users orient by viewing lists of workspaces from APIs. Workspace type Workspaces have types, which are mostly oriented around a set of default or optional APIs exposed. For instance, a workspace intended for use deploying Kube applications might expose the same API objects a user would encounter on a physical cluster. A workspace intended for building functions might expose only the knative serving APIs, config maps and secrets, and optionally enable knative eventing APIs. At the current time there is no decision on whether a workspace type represents an inheritance or composition model, although in general we prefer composition approaches. We also do not have a fully resolved design. Virtual Workspace An API object has one source of truth (is stored transactionally in one system), but may be exposed to different use cases with different fields or schemas. Since a workspace is the user facing interaction with an API object, if we want to deal with Workspaces in aggregate, we need to be able to list them. Since a user may have access to workspaces in multiple different contexts, or for different use cases (a workspace that belongs to the user personally, or one that belongs to a business organization), the list of \u201call workspaces\u201d itself needs to be exposed as an API object to an end user inside a workspace. That workspace is \u201cvirtual\u201d - it adapts or transforms the underlying source of truth for the object and potentially the schema the user sees. Index (e.g. Workspace Index) An index is the authoritative list of a particular API in their source of truth across the system. For instance, in order for a user to see all the workspaces they have available, they must consult the workspace index to return a list of their workspaces. It is expected that indices are suitable for consistent LIST/WATCHing (in the kubernetes sense) so that integrations can be built to view the list of those objects. Index in the control plane sense should not be confused with secondary indices (in the database sense), which may be used to enable a particular index. Shard A failure domain within the larger control plane service that cuts across the primary functionality. Most distributed systems must separate functionality across shards to mitigate failures, and typically users interact with shards through some transparent serving infrastructure. Since the primary problem of building distributed systems is reasoning about failure domains and dependencies across them, it is critical to allow operators to effectively match shards, understand dependencies, and bring them together. A control plane should be shardable in a way that maximizes application SLO - gives users a tool that allows them to better define their applications not to fail. API Binding The act of associating a set of APIs with a given logical cluster. The Workspace model defines one particular implementation of the lifecycle of a logical cluster and the APIs within it. Because APIs and the implementations that back an API evolve over time, it is important that the binding be introspectable and orchestrate-able - that a consumer can provide a rolling deployment of a new API or new implementation across hundreds or thousands of workspaces. There are likely a few objects involved in defining the APIs exposed within a workspace, but in general they probably define a spec (which APIs / implementations to associate with) and a status (the chosen APIs / implementations that are currently bound), allow a user to bulk associate APIs (i.e. multiple APIs at the same time, like \u201call knative serving APIs\u201d), and may be defaulted based on some attributes of a workspace type (all workspaces of this \u201ctype\u201d get the default Kube APIs, this other \u201ctype\u201d get the knative apis). The evolution of an API within a workspace and across workspaces is of key importance. Syncer A syncer is installed on a SyncTarget and is responsible for synchronizing data between kcp and that cluster. Location A collection of SyncTargets that describe runtime characteristics that allow placement of applications. Characteristics are not limited but could describe things like GPU, supported storage, compliance or regulatory fulfillment, or geographical placement.","title":"Terminology for kcp"},{"location":"content/en/concepts/#logical-cluster","text":"A logical cluster is a way to subdivide a single kube-apiserver + etcd storage into multiple clusters (different APIs, separate semantics for access, policy, and control) without requiring multiple instances. A logical cluster is a mechanism for achieving separation, but may be modelled differently in different use cases. A logical cluster is similar to a virtual cluster as defined by sig-multicluster, but is able to amortize the cost of a new cluster to be zero or near-zero memory and storage so that we can create tens of millions of empty clusters cheaply. A logical cluster is a storage level concept that adds an additional attribute to an object\u2019s identifier on a kube-apiserver. Regular servers identify objects by (group, version, resource, optional namespace, name). A logical cluster enriches an identifier: (group, version, resource, logical cluster name , optional namespace, name).","title":"Logical cluster"},{"location":"content/en/concepts/#workload-cluster","text":"A physical cluster is a \u201creal Kubernetes cluster\u201d, i.e. one that can run Kubernetes workloads and accepts standard Kubernetes API objects. For the near term, it is assumed that a physical cluster is a distribution of Kubernetes and passes the conformance tests and exposes the behavior a regular Kubernetes admin or user expects.","title":"Workload Cluster"},{"location":"content/en/concepts/#workspace","text":"A workspace models a set of user-facing APIs for CRUD. Each workspace is backed by a logical cluster, but not all logical clusters may be exposed as workspaces. Creating a Workspace object results in a logical cluster being available via a URL for the client to connect and create resources supported by the APIs in that workspace. There could be multiple different models that result in logical clusters being created, with different policies or lifecycles, but Workspace is intended to be the most generic representation of the concept with the broadest possible utility to anyone building control planes. A workspace binds APIs and makes them accessible inside the logical cluster, allocates capacity for creating instances of those APIs (quota), and defines how multi-workspace operations can be performed by users, clients, and controller integrations. To a user, a workspace appears to be a Kubernetes cluster minus all the container orchestration specific resources. It has its own discovery, its own OpenAPI spec, and follows the kube-like constraints about uniqueness of Group-Version-Resource and its behaviour (no two GVRs with different schemas can exist per workspace, but workspaces can have different schemas). A user can define a workspace as a context in a kubeconfig file and kubectl get all -A would return all objects in all namespaces of that workspace. Workspace naming is chosen to be aligned with the Kubernetes Namespace object - a Namespace subdivides a workspace by name, a workspace subdivides the universe into chunks of meaningful work. Workspaces are the containers for all API objects, so users orient by viewing lists of workspaces from APIs.","title":"Workspace"},{"location":"content/en/concepts/#workspace-type","text":"Workspaces have types, which are mostly oriented around a set of default or optional APIs exposed. For instance, a workspace intended for use deploying Kube applications might expose the same API objects a user would encounter on a physical cluster. A workspace intended for building functions might expose only the knative serving APIs, config maps and secrets, and optionally enable knative eventing APIs. At the current time there is no decision on whether a workspace type represents an inheritance or composition model, although in general we prefer composition approaches. We also do not have a fully resolved design.","title":"Workspace type"},{"location":"content/en/concepts/#virtual-workspace","text":"An API object has one source of truth (is stored transactionally in one system), but may be exposed to different use cases with different fields or schemas. Since a workspace is the user facing interaction with an API object, if we want to deal with Workspaces in aggregate, we need to be able to list them. Since a user may have access to workspaces in multiple different contexts, or for different use cases (a workspace that belongs to the user personally, or one that belongs to a business organization), the list of \u201call workspaces\u201d itself needs to be exposed as an API object to an end user inside a workspace. That workspace is \u201cvirtual\u201d - it adapts or transforms the underlying source of truth for the object and potentially the schema the user sees.","title":"Virtual Workspace"},{"location":"content/en/concepts/#index-eg-workspace-index","text":"An index is the authoritative list of a particular API in their source of truth across the system. For instance, in order for a user to see all the workspaces they have available, they must consult the workspace index to return a list of their workspaces. It is expected that indices are suitable for consistent LIST/WATCHing (in the kubernetes sense) so that integrations can be built to view the list of those objects. Index in the control plane sense should not be confused with secondary indices (in the database sense), which may be used to enable a particular index.","title":"Index (e.g. Workspace Index)"},{"location":"content/en/concepts/#shard","text":"A failure domain within the larger control plane service that cuts across the primary functionality. Most distributed systems must separate functionality across shards to mitigate failures, and typically users interact with shards through some transparent serving infrastructure. Since the primary problem of building distributed systems is reasoning about failure domains and dependencies across them, it is critical to allow operators to effectively match shards, understand dependencies, and bring them together. A control plane should be shardable in a way that maximizes application SLO - gives users a tool that allows them to better define their applications not to fail.","title":"Shard"},{"location":"content/en/concepts/#api-binding","text":"The act of associating a set of APIs with a given logical cluster. The Workspace model defines one particular implementation of the lifecycle of a logical cluster and the APIs within it. Because APIs and the implementations that back an API evolve over time, it is important that the binding be introspectable and orchestrate-able - that a consumer can provide a rolling deployment of a new API or new implementation across hundreds or thousands of workspaces. There are likely a few objects involved in defining the APIs exposed within a workspace, but in general they probably define a spec (which APIs / implementations to associate with) and a status (the chosen APIs / implementations that are currently bound), allow a user to bulk associate APIs (i.e. multiple APIs at the same time, like \u201call knative serving APIs\u201d), and may be defaulted based on some attributes of a workspace type (all workspaces of this \u201ctype\u201d get the default Kube APIs, this other \u201ctype\u201d get the knative apis). The evolution of an API within a workspace and across workspaces is of key importance.","title":"API Binding"},{"location":"content/en/concepts/#syncer","text":"A syncer is installed on a SyncTarget and is responsible for synchronizing data between kcp and that cluster.","title":"Syncer"},{"location":"content/en/concepts/#location","text":"A collection of SyncTargets that describe runtime characteristics that allow placement of applications. Characteristics are not limited but could describe things like GPU, supported storage, compliance or regulatory fulfillment, or geographical placement.","title":"Location"},{"location":"content/en/concepts/authorization/","text":"Within workspaces, KCP implements the same RBAC-based authorization mechanism as Kubernetes. Other authorization schemes (i.e. ABAC) are not supported. Generally, the same (cluster) role and (cluster) role binding principles apply exactly as in Kubernetes. In addition, additional RBAC semantics is implemented cross-workspaces, namely the following: Top-Level Organization access: the user must have this as pre-requisite to access any other workspace, or is even member and by that can create workspaces inside the organization workspace. Workspace Content access: the user needs access to a workspace or is even admin. for some resources, additional permission checks are performed, not represented by local or Kubernetes standard RBAC rules. E.g. workspace creation checks for organization membership (see above). workspace creation checks for use verb on the WorkspaceType . API binding via APIBinding objects requires verb bind access to the corresponding APIExport . System Workspaces access: system workspaces are prefixed with system: and are not accessible by users. The details are outlined below. Authorizers The following authorizers are configured in kcp: Authorizer Description Top-Level organization authorizer checks that the user is allowed to access the organization Workspace content authorizer determines additional groups a user gets inside of a workspace Maximal permission policy authorizer validates the maximal permission policy RBAC policy in the API exporter workspace Local Policy authorizer validates the RBAC policy in the workspace that is accessed Kubernetes Bootstrap Policy authorizer validates the RBAC Kubernetes standard policy They are related in the following way: top-level organization authorizer must allow workspace content authorizer must allow, and adds additional (virtual per-request) groups to the request user influencing the follow authorizers. maximal permission policy authorizer must allow one of the local authorizer or bootstrap policy authorizer must allow. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u25ba\u2502 Local Policy \u251c\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 authorizer \u2502 \u2502 request \u2502 Workspace \u2502 \u2502 Required \u2502 \u2502 Max. Permission \u2502 \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 Content \u251c\u2500\u2500\u2500\u2500\u25ba\u2502 Groups \u251c\u2500\u2500\u2500\u2500\u2524 Policy authorizer \u251c\u2500\u2500\u2500\u2524 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 Authorizer \u2502 \u2502 Authorizer \u2502 \u2502 \u2502 \u2502 \u25bc \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 OR\u2500\u2500\u2500\u25ba \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25b2 \u2502 \u2502 Bootstrap \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u25ba\u2502 Policy \u251c\u2500\u2500\u2518 \u2502 authorizer \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ASCIIFlow document Workspace Content authorizer The workspace content authorizer checks whether the user is granted access to the workspace. Access is granted access through verb=access non-resource permission to / inside of the workspace. The ClusterRole system:kcp:workspace:access is pre-defined which makes it easy to give a user access through a ClusterRoleBinding inside of the workspace. For example, to give a user user1 access, create the following ClusterRoleBinding: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: example-access subjects: - kind: User name: user1 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kcp:workspace:access To give a user user1 admin access, create the following ClusterRoleBinding: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: example-admin subjects: - kind: User name: user1 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin A service-account defined in a workspace implicitly is granted access to it. A service-account defined in a differant workspace is NOT given access to it. Required Groups Authorizer A authorization.kcp.io/required-groups annotation can be added to a LogicalCluster to specify additional groups that are required to access a workspace for a user to be member of. The syntax is a disjunction (separator , ) of conjunctions (separator ; ). For example, <group1>;<group2>,<group3> means that a user must be member of <group1> AND <group2> , OR of <group3> . The annotation is copied onto sub-workspaces during scheduling. Initializing Workspaces By default, workspaces are only accessible to a user if they are in Ready phase. Workspaces that are initializing can be access only by users that are granted admin verb on the workspaces/content resource in the parent workspace. Service accounts declared within a workspace don't have access to initializing workspaces. Maximal permission policy authorizer If the requested resource type is part of an API binding, then this authorizer verifies that the request is not exceeding the maximum permission policy of the related API export. Currently, the \"local policy\" maximum permission policy type is supported. Local policy The local maximum permission policy delegates the decision to the RBAC of the related API export. To distinguish between local RBAC role bindings in that workspace and those for this these maximum permission policy, every name and group is prefixed with apis.kcp.io:binding: . Example: Given an API binding for type foo declared in workspace consumer that refers to an API export declared in workspace provider and a user user-1 having the group group-1 requesting a create of foo in the default namespace in the consumer workspace, this authorizer verifies that user-1 is allowed to execute this request by delegating to provider 's RBAC using prefixed attributes. Here, this authorizer prepends the apis.kcp.io:binding: prefix to the username and all groups the user belongs to. Using prefixed attributes prevents RBAC collisions i.e. if user-1 is granted to execute requests within the provider workspace directly. For the given example RBAC request looks as follows: Username: apis.kcp.io:binding:user-1 Group: apis.kcp.io:binding:group-1 Resource: foo Namespace: default Workspace: provider Verb: create The following role and role binding declared within the provider workspace will grant access to the request: apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: foo-creator clusterName: provider rules: - apiGroups: - foo.api resources: - foos verbs: - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: user-1-foo-creator namespace: default clusterName: provider subjects: - kind: User name: apis.kcp.io:binding:user-1 roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: foo-creator !!! note The same authorization scheme is enforced when executing the request of a claimed resource via the virtual API Export API server, i.e. a claimed resource is bound to the same maximal permission policy. Only the actual owner of that resources can go beyond that policy. TBD: Example Kubernetes Bootstrap Policy authorizer The bootstrap policy authorizer works just like the local authorizer but references RBAC rules defined in the system:admin system workspace. Local Policy authorizer Once the top-level organization authorizer and the workspace content authorizer granted access to a workspace, RBAC rules contained in the workspace derived from the request context are evaluated. This authorizer ensures that RBAC rules contained within a workspace are being applied and work just like in a regular Kubernetes cluster. !!! note Groups added by the workspace content authorizer can be used for role bindings in that workspace. It is possible to bind to roles and cluster roles in the bootstrap policy from a local policy RoleBinding or ClusterRoleBinding . Service Accounts Kubernetes service accounts are granted access to the workspaces they are defined in and that are ready. E.g. a service account \"default\" in root:org:ws:ws is granted access to root:org:ws:ws , and through the workspace content authorizer it gains the system:kcp:clusterworkspace:access group membership.","title":"Authorization"},{"location":"content/en/concepts/authorization/#authorizers","text":"The following authorizers are configured in kcp: Authorizer Description Top-Level organization authorizer checks that the user is allowed to access the organization Workspace content authorizer determines additional groups a user gets inside of a workspace Maximal permission policy authorizer validates the maximal permission policy RBAC policy in the API exporter workspace Local Policy authorizer validates the RBAC policy in the workspace that is accessed Kubernetes Bootstrap Policy authorizer validates the RBAC Kubernetes standard policy They are related in the following way: top-level organization authorizer must allow workspace content authorizer must allow, and adds additional (virtual per-request) groups to the request user influencing the follow authorizers. maximal permission policy authorizer must allow one of the local authorizer or bootstrap policy authorizer must allow. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u25ba\u2502 Local Policy \u251c\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 authorizer \u2502 \u2502 request \u2502 Workspace \u2502 \u2502 Required \u2502 \u2502 Max. Permission \u2502 \u2502 \u2502 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 Content \u251c\u2500\u2500\u2500\u2500\u25ba\u2502 Groups \u251c\u2500\u2500\u2500\u2500\u2524 Policy authorizer \u251c\u2500\u2500\u2500\u2524 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 Authorizer \u2502 \u2502 Authorizer \u2502 \u2502 \u2502 \u2502 \u25bc \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 OR\u2500\u2500\u2500\u25ba \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25b2 \u2502 \u2502 Bootstrap \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u25ba\u2502 Policy \u251c\u2500\u2500\u2518 \u2502 authorizer \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ASCIIFlow document","title":"Authorizers"},{"location":"content/en/concepts/authorization/#workspace-content-authorizer","text":"The workspace content authorizer checks whether the user is granted access to the workspace. Access is granted access through verb=access non-resource permission to / inside of the workspace. The ClusterRole system:kcp:workspace:access is pre-defined which makes it easy to give a user access through a ClusterRoleBinding inside of the workspace. For example, to give a user user1 access, create the following ClusterRoleBinding: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: example-access subjects: - kind: User name: user1 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kcp:workspace:access To give a user user1 admin access, create the following ClusterRoleBinding: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: example-admin subjects: - kind: User name: user1 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin A service-account defined in a workspace implicitly is granted access to it. A service-account defined in a differant workspace is NOT given access to it.","title":"Workspace Content authorizer"},{"location":"content/en/concepts/authorization/#required-groups-authorizer","text":"A authorization.kcp.io/required-groups annotation can be added to a LogicalCluster to specify additional groups that are required to access a workspace for a user to be member of. The syntax is a disjunction (separator , ) of conjunctions (separator ; ). For example, <group1>;<group2>,<group3> means that a user must be member of <group1> AND <group2> , OR of <group3> . The annotation is copied onto sub-workspaces during scheduling.","title":"Required Groups Authorizer"},{"location":"content/en/concepts/authorization/#initializing-workspaces","text":"By default, workspaces are only accessible to a user if they are in Ready phase. Workspaces that are initializing can be access only by users that are granted admin verb on the workspaces/content resource in the parent workspace. Service accounts declared within a workspace don't have access to initializing workspaces.","title":"Initializing Workspaces"},{"location":"content/en/concepts/authorization/#maximal-permission-policy-authorizer","text":"If the requested resource type is part of an API binding, then this authorizer verifies that the request is not exceeding the maximum permission policy of the related API export. Currently, the \"local policy\" maximum permission policy type is supported.","title":"Maximal permission policy authorizer"},{"location":"content/en/concepts/authorization/#local-policy","text":"The local maximum permission policy delegates the decision to the RBAC of the related API export. To distinguish between local RBAC role bindings in that workspace and those for this these maximum permission policy, every name and group is prefixed with apis.kcp.io:binding: . Example: Given an API binding for type foo declared in workspace consumer that refers to an API export declared in workspace provider and a user user-1 having the group group-1 requesting a create of foo in the default namespace in the consumer workspace, this authorizer verifies that user-1 is allowed to execute this request by delegating to provider 's RBAC using prefixed attributes. Here, this authorizer prepends the apis.kcp.io:binding: prefix to the username and all groups the user belongs to. Using prefixed attributes prevents RBAC collisions i.e. if user-1 is granted to execute requests within the provider workspace directly. For the given example RBAC request looks as follows: Username: apis.kcp.io:binding:user-1 Group: apis.kcp.io:binding:group-1 Resource: foo Namespace: default Workspace: provider Verb: create The following role and role binding declared within the provider workspace will grant access to the request: apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: foo-creator clusterName: provider rules: - apiGroups: - foo.api resources: - foos verbs: - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: user-1-foo-creator namespace: default clusterName: provider subjects: - kind: User name: apis.kcp.io:binding:user-1 roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: foo-creator !!! note The same authorization scheme is enforced when executing the request of a claimed resource via the virtual API Export API server, i.e. a claimed resource is bound to the same maximal permission policy. Only the actual owner of that resources can go beyond that policy. TBD: Example","title":"Local policy"},{"location":"content/en/concepts/authorization/#kubernetes-bootstrap-policy-authorizer","text":"The bootstrap policy authorizer works just like the local authorizer but references RBAC rules defined in the system:admin system workspace.","title":"Kubernetes Bootstrap Policy authorizer"},{"location":"content/en/concepts/authorization/#local-policy-authorizer","text":"Once the top-level organization authorizer and the workspace content authorizer granted access to a workspace, RBAC rules contained in the workspace derived from the request context are evaluated. This authorizer ensures that RBAC rules contained within a workspace are being applied and work just like in a regular Kubernetes cluster. !!! note Groups added by the workspace content authorizer can be used for role bindings in that workspace. It is possible to bind to roles and cluster roles in the bootstrap policy from a local policy RoleBinding or ClusterRoleBinding .","title":"Local Policy authorizer"},{"location":"content/en/concepts/authorization/#service-accounts","text":"Kubernetes service accounts are granted access to the workspaces they are defined in and that are ready. E.g. a service account \"default\" in root:org:ws:ws is granted access to root:org:ws:ws , and through the workspace content authorizer it gains the system:kcp:clusterworkspace:access group membership.","title":"Service Accounts"},{"location":"content/en/concepts/cache-server/","text":"Purpose The primary purpose of the cache server is to support cross-shard communication. Shards need to communicate with each other in order to enable, among other things, migration and replication features. Direct shard-to-shard communication is not feasible in a larger setup as it scales with n*(n-1). The web of connections would be hard to reason about, maintain and troubleshoot. Thus, the cache server serves as a central place for shards to store and read common data. Note, that the final topology can have more than one cache server, i.e. one in some geographic region. High-level overview The cache server is a regular, Kubernetes-style CRUD API server with support of LIST/WATCH semantics. Conceptually it supports two modes of operations. The first mode is in which a shard gets its private space for storing its own data. The second mode is in which a shard can read other shards' data. The first mode of operation is implemented as a write controller that runs on a shard. It holds some state/data from that shard in memory and pushes it to the cache server. The controller uses standard informers for getting required resources both from a local kcp instance and remote cache server. Before pushing data it has to compare remote data to its local copy and make sure both copies are consistent. The second mode of operation is implemented as a read controller(s) that runs on some shard. It holds other shards' data in an in-memory cache using an informer, effectively pulling data from the central cache. Thanks to having a separate informer for interacting with the cache server a shard can implement a different resiliency strategy. For example, it can tolerate the unavailability of the secondary during startup and become ready. Running the server The cache server can be run as a standalone binary or as part of a kcp server. The standalone binary is in https://github.com/kcp-dev/kcp/tree/main/cmd/cache-server and can be run by issuing go run ./cmd/cache-server/main.go command. To run it as part of a kcp server, pass --cache-url flag to the kcp binary. Client-side functionality In order to interact with the cache server from a shard, the https://github.com/kcp-dev/kcp/tree/main/pkg/cache/client repository provides the following client-side functionality: ShardRoundTripper , a shard aware wrapper around http.RoundTripper . It changes the URL path to target a shard from the context. DefaultShardRoundTripper is a http.RoundTripper that sets a default shard name if not specified in the context. For example, in order to make a client shard aware, inject the http.RoundTrippers to a rest.Config import ( cacheclient \"github.com/kcp-dev/kcp/pkg/cache/client\" \"github.com/kcp-dev/kcp/pkg/cache/client/shard\" ) // adds shards awareness to a client with a default `shard.Wildcard` name. NewForConfig(cacheclient.WithShardRoundTripper(cacheclient.WithDefaultShardRoundTripper(serverConfig.LoopbackClientConfig, shard.Wildcard))) // and then change the context when you need to access a specific shard and pass is when making a HTTP request ctx = cacheclient.WithShardInContext(ctx, shard.New(\"cache\")) Authorization/Authentication Not implemented at the moment Built-in resources Out of the box, the server supports the following resources: apiresourceschemas apiexports shards All those resources are represented as CustomResourceDefinitions and stored in system:cache:server shard under system:system-crds cluster. Adding new resources Not implemented at the moment. Our near-term plan is to maintain a list of hard-coded resources that we want to keep in the cache server. In the future, we will use the ReplicationClam which will describe schemas that need to be exposed by the cache server. Deletion of data Not implemented at the moment. Only deleting resources explicitly is possible. In the future, some form of automatic removal will be implemented. Design details The cache server is implemented as the apiextensions-apiserver . It is based on the same fork used by the kcp server, extended with shard support. Since the server serves as a secondary replica it doesn't support versioning, validation, pruning, or admission. All resources persisted by the server are deprived of schema. That means the schema is implicit, maintained, and enforced by the shards pushing/pulling data into/from the server. On the HTTP level The server exposes the following path: /services/cache/shards/{shard-name}/clusters/{cluster-name}/apis/group/version/namespaces/{namespace-name}/resource/{resource-name} Parameters: {shard-name} : a required string holding a shard name, a wildcard is also supported indicating a cross-shard request {cluster-name} : a required string holding a cluster name, a wildcard is also supported indicating a cross-cluster request. {namespace-name} : an optional string holding a namespace the given request is for, not all resources are stored under a namespace. {resource-name} : an optional string holding the name of a resource For example: /services/cache/shards/*/clusters/*/apis/apis.kcp.io/v1alpha1/apiexports : for listing apiexports for all shards and clusters /services/cache/shards/amber/clusters/*/apis/apis.kcp.io/v1alpha1/apiexports : for listing apiexports for amber shard for all clusters /services/cache/shards/sapphire/clusters/system:sapphire/apis/apis.kcp.io/v1alpha1/apiexports : for listing apiexports for sapphire shard stored in system:sapphire cluster On the storage layer All resources stored by the cache server are prefixed with /cache . Thanks to that the server can share the same database with the kcp server. Ultimately a shard aware resources end up being stored under the following key: /cache/group/resource/{shard-name}/{cluster-name}/{namespace-name}/{resource-name} For more information about inner working of the storage layer, please visit: TODO: add a link that explains the mapping of a URL to a storage prefix.","title":"Cache Server"},{"location":"content/en/concepts/cache-server/#purpose","text":"The primary purpose of the cache server is to support cross-shard communication. Shards need to communicate with each other in order to enable, among other things, migration and replication features. Direct shard-to-shard communication is not feasible in a larger setup as it scales with n*(n-1). The web of connections would be hard to reason about, maintain and troubleshoot. Thus, the cache server serves as a central place for shards to store and read common data. Note, that the final topology can have more than one cache server, i.e. one in some geographic region.","title":"Purpose"},{"location":"content/en/concepts/cache-server/#high-level-overview","text":"The cache server is a regular, Kubernetes-style CRUD API server with support of LIST/WATCH semantics. Conceptually it supports two modes of operations. The first mode is in which a shard gets its private space for storing its own data. The second mode is in which a shard can read other shards' data. The first mode of operation is implemented as a write controller that runs on a shard. It holds some state/data from that shard in memory and pushes it to the cache server. The controller uses standard informers for getting required resources both from a local kcp instance and remote cache server. Before pushing data it has to compare remote data to its local copy and make sure both copies are consistent. The second mode of operation is implemented as a read controller(s) that runs on some shard. It holds other shards' data in an in-memory cache using an informer, effectively pulling data from the central cache. Thanks to having a separate informer for interacting with the cache server a shard can implement a different resiliency strategy. For example, it can tolerate the unavailability of the secondary during startup and become ready.","title":"High-level overview"},{"location":"content/en/concepts/cache-server/#running-the-server","text":"The cache server can be run as a standalone binary or as part of a kcp server. The standalone binary is in https://github.com/kcp-dev/kcp/tree/main/cmd/cache-server and can be run by issuing go run ./cmd/cache-server/main.go command. To run it as part of a kcp server, pass --cache-url flag to the kcp binary.","title":"Running the server"},{"location":"content/en/concepts/cache-server/#client-side-functionality","text":"In order to interact with the cache server from a shard, the https://github.com/kcp-dev/kcp/tree/main/pkg/cache/client repository provides the following client-side functionality: ShardRoundTripper , a shard aware wrapper around http.RoundTripper . It changes the URL path to target a shard from the context. DefaultShardRoundTripper is a http.RoundTripper that sets a default shard name if not specified in the context. For example, in order to make a client shard aware, inject the http.RoundTrippers to a rest.Config import ( cacheclient \"github.com/kcp-dev/kcp/pkg/cache/client\" \"github.com/kcp-dev/kcp/pkg/cache/client/shard\" ) // adds shards awareness to a client with a default `shard.Wildcard` name. NewForConfig(cacheclient.WithShardRoundTripper(cacheclient.WithDefaultShardRoundTripper(serverConfig.LoopbackClientConfig, shard.Wildcard))) // and then change the context when you need to access a specific shard and pass is when making a HTTP request ctx = cacheclient.WithShardInContext(ctx, shard.New(\"cache\"))","title":"Client-side functionality"},{"location":"content/en/concepts/cache-server/#authorizationauthentication","text":"Not implemented at the moment","title":"Authorization/Authentication"},{"location":"content/en/concepts/cache-server/#built-in-resources","text":"Out of the box, the server supports the following resources: apiresourceschemas apiexports shards All those resources are represented as CustomResourceDefinitions and stored in system:cache:server shard under system:system-crds cluster.","title":"Built-in resources"},{"location":"content/en/concepts/cache-server/#adding-new-resources","text":"Not implemented at the moment. Our near-term plan is to maintain a list of hard-coded resources that we want to keep in the cache server. In the future, we will use the ReplicationClam which will describe schemas that need to be exposed by the cache server.","title":"Adding new resources"},{"location":"content/en/concepts/cache-server/#deletion-of-data","text":"Not implemented at the moment. Only deleting resources explicitly is possible. In the future, some form of automatic removal will be implemented.","title":"Deletion of data"},{"location":"content/en/concepts/cache-server/#design-details","text":"The cache server is implemented as the apiextensions-apiserver . It is based on the same fork used by the kcp server, extended with shard support. Since the server serves as a secondary replica it doesn't support versioning, validation, pruning, or admission. All resources persisted by the server are deprived of schema. That means the schema is implicit, maintained, and enforced by the shards pushing/pulling data into/from the server.","title":"Design details"},{"location":"content/en/concepts/cache-server/#on-the-http-level","text":"The server exposes the following path: /services/cache/shards/{shard-name}/clusters/{cluster-name}/apis/group/version/namespaces/{namespace-name}/resource/{resource-name} Parameters: {shard-name} : a required string holding a shard name, a wildcard is also supported indicating a cross-shard request {cluster-name} : a required string holding a cluster name, a wildcard is also supported indicating a cross-cluster request. {namespace-name} : an optional string holding a namespace the given request is for, not all resources are stored under a namespace. {resource-name} : an optional string holding the name of a resource For example: /services/cache/shards/*/clusters/*/apis/apis.kcp.io/v1alpha1/apiexports : for listing apiexports for all shards and clusters /services/cache/shards/amber/clusters/*/apis/apis.kcp.io/v1alpha1/apiexports : for listing apiexports for amber shard for all clusters /services/cache/shards/sapphire/clusters/system:sapphire/apis/apis.kcp.io/v1alpha1/apiexports : for listing apiexports for sapphire shard stored in system:sapphire cluster","title":"On the HTTP level"},{"location":"content/en/concepts/cache-server/#on-the-storage-layer","text":"All resources stored by the cache server are prefixed with /cache . Thanks to that the server can share the same database with the kcp server. Ultimately a shard aware resources end up being stored under the following key: /cache/group/resource/{shard-name}/{cluster-name}/{namespace-name}/{resource-name} For more information about inner working of the storage layer, please visit: TODO: add a link that explains the mapping of a URL to a storage prefix.","title":"On the storage layer"},{"location":"content/en/concepts/cluster-mapper/","text":"Invariants Every object assigned by a location exists on the target location Every object in the target location has appropriate metadata set indicating source Every object in the target location that has status and the appropriate policy choice set will reflect that status back in the source object No object exists in the target location that is not assigned by a source object(s) 1..N mappers per location (sharding by virtual cluster?) mappers see only the objects assigned to them (special API) can load policy info during mapping? can load policy info from side channels like regular resources wants to watch many different resources at once and deal with them as unstructured assumption: all resources in the location are compatible with the target API (managed by control plane and CRD folding), and if that is broken the mapper is instructed to halt mapping how to deal with partial mapping when one object is broken how does CRD folding actually work (separate doc) assumption: higher level control (admission) manages location accessibility assumption: kcp has 1k virtual clusters with 50k resources, a given mapper may see 1k to 50k resources fully syncing will take 50k / default throttle (50-100 req/s) ~ 1000s in serial order may be important there may be 100-1k locations, so we may have up to 1/1000 cardinality (implies indexing) mappers that favor summarization objects (deployments) have scale advantages over those that don't (pods) Assumption: we prefer not to require order to correctly map, but some order is implicit due to resource version ordering Basic sync loop retrieve all objects assigned to a particular cluster (metadata.annotations[\"kcp.io/assigned-locations\"] = [\"a\",\"b\"]) transform them into one or more objects for the destination add labels? map namespace from source to target hide certain annotations (assigned-locations?) set and maintain other annotations (like the source namespace / virtual cluster) set a controller ref? perform a merge into the destination object (overwrite of spec) sync some fields (status?) back to source object delete all objects no longer assigned to the remote location read all mappable objects from all mappable resources? use kcp.io/location=X label to filter detect when an object policy on mapping is changed? only need the partial object (metadata) can we leverage the garbage collector to delete the object?","title":"Cluster Mapper"},{"location":"content/en/concepts/cluster-mapper/#invariants","text":"Every object assigned by a location exists on the target location Every object in the target location has appropriate metadata set indicating source Every object in the target location that has status and the appropriate policy choice set will reflect that status back in the source object No object exists in the target location that is not assigned by a source object(s) 1..N mappers per location (sharding by virtual cluster?) mappers see only the objects assigned to them (special API) can load policy info during mapping? can load policy info from side channels like regular resources wants to watch many different resources at once and deal with them as unstructured assumption: all resources in the location are compatible with the target API (managed by control plane and CRD folding), and if that is broken the mapper is instructed to halt mapping how to deal with partial mapping when one object is broken how does CRD folding actually work (separate doc) assumption: higher level control (admission) manages location accessibility assumption: kcp has 1k virtual clusters with 50k resources, a given mapper may see 1k to 50k resources fully syncing will take 50k / default throttle (50-100 req/s) ~ 1000s in serial order may be important there may be 100-1k locations, so we may have up to 1/1000 cardinality (implies indexing) mappers that favor summarization objects (deployments) have scale advantages over those that don't (pods) Assumption: we prefer not to require order to correctly map, but some order is implicit due to resource version ordering","title":"Invariants"},{"location":"content/en/concepts/cluster-mapper/#basic-sync-loop","text":"retrieve all objects assigned to a particular cluster (metadata.annotations[\"kcp.io/assigned-locations\"] = [\"a\",\"b\"]) transform them into one or more objects for the destination add labels? map namespace from source to target hide certain annotations (assigned-locations?) set and maintain other annotations (like the source namespace / virtual cluster) set a controller ref? perform a merge into the destination object (overwrite of spec) sync some fields (status?) back to source object delete all objects no longer assigned to the remote location read all mappable objects from all mappable resources? use kcp.io/location=X label to filter detect when an object policy on mapping is changed? only need the partial object (metadata) can we leverage the garbage collector to delete the object?","title":"Basic sync loop"},{"location":"content/en/concepts/kubectl-kcp-plugin/","text":"kcp provides a kubectl plugin that simplifies the operations with the kcp server. You can install the plugin from the current repo: $ make install go install ./cmd/... The plugin will be automatically discovered by your current kubectl binary : $ kubectl-kcp KCP is the easiest way to manage Kubernetes applications against one or more clusters, by giving you a personal control plane that schedules your workloads onto one or many clusters, and making it simple to pick up and move. Advanced use cases including spreading your apps across clusters for resiliency, scheduling batch workloads onto clusters with free capacity, and enabling collaboration for individual teams without having access to the underlying clusters. This command provides KCP specific sub-command for kubectl. Usage: kcp [command] Available Commands: completion generate the autocompletion script for the specified shell help Help about any command workspace Manages KCP workspaces Flags: --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files -h, --help help for kcp --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory --log_file string If non-empty, use this log file --log_file_max_size uint Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging Use \"kcp [command] --help\" for more information about a command.","title":"Kubectl kcp plugin"},{"location":"content/en/concepts/locations-and-scheduling/","text":"KCP implements Compute as a Service via a concept of Transparent Multi Cluster (TMC). TMC means that Kubernetes clusters are attached to a kcp installation to execute workload objects from the users' workspaces by syncing these workload objects down to those clusters and the objects' status back up. This gives the illusion of native compute in KCP. We call it Compute as a Service because the registered SyncTargets live in workspaces that are (normally) invisible to the users, and the teams operating compute can be different from the compute consumers. The APIs used for Compute as a Service are: scheduling.kcp.io/v1alpha1 \u2013 we call the outcome of this placement of namespaces. workload.kcp.io/v1alpha1 \u2013 responsible for the syncer component of TMC. Main Concepts SyncTarget in workload.kcp.io/v1alpha1 \u2013 representations of Kubernetes clusters that are attached to a kcp installation to execute workload objects from the users' workspaces. On a Kubernetes cluster, there is one syncer process for each SyncTarget object. Sync targets are invisible to users, and (medium term) at most identified via a UID. Location in scheduling.kcp.io/v1alpha1 \u2013 represents a collection of SyncTarget objects selected via instance labels, and exposes labels (potentially different from the instance labels) to the users to describe, identify and select locations to be used for placement of user namespaces onto sync targets. Locations are visible to users, but owned by the compute service team, i.e. read-only to the users and only projected into their workspaces for visibility. A placement decision references a location by name. SyncTarget s in a Location are transparent to the user. Workloads should be able to seamlessly move from one SyncTarget to another within a Location , based on operational concerns of the compute service provider, like decommissioning a cluster, rebalancing capacity, or due to an outage of a cluster. It is compute service's responsibility to ensure that for workloads in a location, to the user it looks like ONE cluster. Placement in scheduling.kcp.io/v1alpha1 \u2013 represents a selection rule to choose ONE Location via location labels, and bind the selected location to MULTIPLE namespaces in a user workspace. For Workspaces with multiple Namespaces, users can create multiple Placements to assign specific Namespace(s) to specific Locations. Placement are visible and writable to users. A default Placement is automatically created when a workload APIBinding is created on the user workspace, which randomly select a Location and bind to all namespaces in this workspace. The user can mutate or delete the default Placement . The corresponding APIBinding will be annotated with workload.kcp.io/skip-default-object-creation , so that the default Placement will not be recreated upon deletion. Compute Service Workspace (previously Negotiation Workspace ) \u2013 the workspace owned by the compute service team to hold the APIExport (named kubernetes today) with the synced resources, and SyncTarget and Location objects. The user binds to the APIExport called kubernetes using an APIBinding . From this moment on, the users' workspaces are subject to placement. !!! note Binding to a compute service is a permanent decision. Unbinding (i.e. deleting of the APIBinding object) means deletion of the workload objects. It is planned to allow multiple location workspaces for the same compute service, even with different owners. Placement and resource scheduling The placement state is one of Pending \u2013 the placement controller waits for a valid Location to select Bound \u2013 at least one namespace is bound to the placement. When the user updates the spec of the Placement , the selected location of the placement will be changed in Bound state. Unbound \u2013 a location is selected by the placement, but no namespace is bound to the placement. When the user updates the spec of the Placement , the selected location of the placement will be changed in Unbound state. !!! note Sync targets from different locations can be bound at the same time, while each location can only have one sync target bound to the namespace. The user interface to influence the placement decisions is the Placement object. For example, user can create a placement to bind namespace with label of \"app=foo\" to a location with label \"cloud=aws\" as below: apiVersion: scheduling.kcp.io/v1alpha1 kind: Placement metadata: name: aws spec: locationSelectors: - matchLabels: cloud: aws namespaceSelector: matchLabels: app: foo locationWorkspace: root:default:location-ws A matched location will be selected for this Placement at first, which makes the Placement turns from Pending to Unbound . Then if there is at least one matching Namespace, the Namespace will be annotated with scheduling.kcp.io/placement and the placement turns from Unbound to Bound . After this, a SyncTarget will be selected from the location picked by the placement. state.workload.kcp.io/<cluster-id> label with value of Sync will be set if a valid SyncTarget is selected. The user can create another placement targeted to a different location for this Namespace, e.g. apiVersion: scheduling.kcp.io/v1alpha1 kind: Placement metadata: name: gce spec: locationSelectors: - matchLabels: cloud: gce namespaceSelector: matchLabels: app: foo locationWorkspace: root:default:location-ws which will result in another state.workload.kcp.io/<cluster-id> label added to the Namespace, and the Namespace will have two different state.workload.kcp.io/<cluster-id> label. Placement is in the Ready status condition when selected location matches the Placement spec. selected location exists in the location workspace. Sync target removing A sync target will be removed when: corresponding Placement is deleted. corresponding Placement is not in Ready condition. corresponding SyncTarget is evicting/not Ready/deleted All above cases will make the SyncTarget represented in the label state.workload.kcp.io/<cluster-id> invalid, which will cause finalizers.workload.kcp.io/<cluster-id> annotation with removing time in the format of RFC-3339 added on the Namespace. Resource Syncing As soon as the state.workload.kcp.io/<cluster-id> label is set on the Namespace, the workload resource controller will copy the state.workload.kcp.io/<cluster-id> label to the resources in that namespace. !!! note In the future, the label on the resources is first set to empty string \"\" , and a coordination controller will be able to apply changes before syncing starts. This includes the ability to add per-location finalizers through the finalizers.workload.kcp.io/<cluster-id> annotation such that the coordination controller gets full control over the downstream life-cycle of the objects per location (imagine an ingress that blocks downstream removal until the new replicas have been launched on another sync target). Finally, the coordination controller will replace the empty string with Sync such that the state machine continues. With the state label set to Sync , the syncer will start seeing the resources in the namespace and starts syncing them downstream, first by creating the namespace. Before syncing, it will also set a finalizer workload.kcp.io/syncer-<cluster-id> on the upstream object in order to delay upstream deletion until the downstream object is also deleted. When the deletion.internal.workload.kcp.io/<cluster-id> is added to the Namespace. The virtual workspace apiserver will translate that annotation into a deletion timestamp on the object the syncer sees. The syncer notices that as a started deletion flow. As soon as there are no coordination controller finalizers registered via the finalizers.workload.kcp.io/<cluster-id> annotation anymore, the syncer will start a deletion of the downstream object. When the downstream deletion is complete, the syncer will remove the finalizer from the upstream object, and the state.workload.kcp.io/<cluster-id> labels gets deleted as well. The syncer stops seeing the object in the virtual workspace. !!! note There is a missing bit in the implementation (in v0.5) about removal of the state.workload.kcp.io/<cluster-id> label from namespaces: the syncer currently does not participate in the namespace deletion state-machine, but has to and signal finished downstream namespace deletion via state.workload.kcp.io/<cluster-id> label removal. For more information on the upsync use case for storage, refer to the [storage doc](storage.md). Resource Upsyncing In most cases kcp will be the source for syncing resources to the SyncTarget , however, in some cases, kcp would need to receive a resource that was provisioned by a controller on the SyncTarget . This is the case with storage PVs, which are created on the SyncTarget by a CSI driver. Unlike the Sync state, the Upsync state is exclusive, and only a single SyncTarget can be the source of truth for an upsynced resource. In addition, other SyncTargets cannot be syncing down while the resource is being upsynced. A resource coordination controller will be responsible for changing the state.workload.kcp.io/<cluster-id> label, to drive the different flows on the resource. A resource can be changed from Upsync to Sync in order to share it across SyncTargets . This change will be applied by the coordination controller when needed, and the original syncer will detect that change and stop upsyncing to that resource, and all the sync targets involved will be in Sync state.","title":"Placement, Locations and Scheduling"},{"location":"content/en/concepts/locations-and-scheduling/#main-concepts","text":"SyncTarget in workload.kcp.io/v1alpha1 \u2013 representations of Kubernetes clusters that are attached to a kcp installation to execute workload objects from the users' workspaces. On a Kubernetes cluster, there is one syncer process for each SyncTarget object. Sync targets are invisible to users, and (medium term) at most identified via a UID. Location in scheduling.kcp.io/v1alpha1 \u2013 represents a collection of SyncTarget objects selected via instance labels, and exposes labels (potentially different from the instance labels) to the users to describe, identify and select locations to be used for placement of user namespaces onto sync targets. Locations are visible to users, but owned by the compute service team, i.e. read-only to the users and only projected into their workspaces for visibility. A placement decision references a location by name. SyncTarget s in a Location are transparent to the user. Workloads should be able to seamlessly move from one SyncTarget to another within a Location , based on operational concerns of the compute service provider, like decommissioning a cluster, rebalancing capacity, or due to an outage of a cluster. It is compute service's responsibility to ensure that for workloads in a location, to the user it looks like ONE cluster. Placement in scheduling.kcp.io/v1alpha1 \u2013 represents a selection rule to choose ONE Location via location labels, and bind the selected location to MULTIPLE namespaces in a user workspace. For Workspaces with multiple Namespaces, users can create multiple Placements to assign specific Namespace(s) to specific Locations. Placement are visible and writable to users. A default Placement is automatically created when a workload APIBinding is created on the user workspace, which randomly select a Location and bind to all namespaces in this workspace. The user can mutate or delete the default Placement . The corresponding APIBinding will be annotated with workload.kcp.io/skip-default-object-creation , so that the default Placement will not be recreated upon deletion. Compute Service Workspace (previously Negotiation Workspace ) \u2013 the workspace owned by the compute service team to hold the APIExport (named kubernetes today) with the synced resources, and SyncTarget and Location objects. The user binds to the APIExport called kubernetes using an APIBinding . From this moment on, the users' workspaces are subject to placement. !!! note Binding to a compute service is a permanent decision. Unbinding (i.e. deleting of the APIBinding object) means deletion of the workload objects. It is planned to allow multiple location workspaces for the same compute service, even with different owners.","title":"Main Concepts"},{"location":"content/en/concepts/locations-and-scheduling/#placement-and-resource-scheduling","text":"The placement state is one of Pending \u2013 the placement controller waits for a valid Location to select Bound \u2013 at least one namespace is bound to the placement. When the user updates the spec of the Placement , the selected location of the placement will be changed in Bound state. Unbound \u2013 a location is selected by the placement, but no namespace is bound to the placement. When the user updates the spec of the Placement , the selected location of the placement will be changed in Unbound state. !!! note Sync targets from different locations can be bound at the same time, while each location can only have one sync target bound to the namespace. The user interface to influence the placement decisions is the Placement object. For example, user can create a placement to bind namespace with label of \"app=foo\" to a location with label \"cloud=aws\" as below: apiVersion: scheduling.kcp.io/v1alpha1 kind: Placement metadata: name: aws spec: locationSelectors: - matchLabels: cloud: aws namespaceSelector: matchLabels: app: foo locationWorkspace: root:default:location-ws A matched location will be selected for this Placement at first, which makes the Placement turns from Pending to Unbound . Then if there is at least one matching Namespace, the Namespace will be annotated with scheduling.kcp.io/placement and the placement turns from Unbound to Bound . After this, a SyncTarget will be selected from the location picked by the placement. state.workload.kcp.io/<cluster-id> label with value of Sync will be set if a valid SyncTarget is selected. The user can create another placement targeted to a different location for this Namespace, e.g. apiVersion: scheduling.kcp.io/v1alpha1 kind: Placement metadata: name: gce spec: locationSelectors: - matchLabels: cloud: gce namespaceSelector: matchLabels: app: foo locationWorkspace: root:default:location-ws which will result in another state.workload.kcp.io/<cluster-id> label added to the Namespace, and the Namespace will have two different state.workload.kcp.io/<cluster-id> label. Placement is in the Ready status condition when selected location matches the Placement spec. selected location exists in the location workspace.","title":"Placement and resource scheduling"},{"location":"content/en/concepts/locations-and-scheduling/#sync-target-removing","text":"A sync target will be removed when: corresponding Placement is deleted. corresponding Placement is not in Ready condition. corresponding SyncTarget is evicting/not Ready/deleted All above cases will make the SyncTarget represented in the label state.workload.kcp.io/<cluster-id> invalid, which will cause finalizers.workload.kcp.io/<cluster-id> annotation with removing time in the format of RFC-3339 added on the Namespace.","title":"Sync target removing"},{"location":"content/en/concepts/locations-and-scheduling/#resource-syncing","text":"As soon as the state.workload.kcp.io/<cluster-id> label is set on the Namespace, the workload resource controller will copy the state.workload.kcp.io/<cluster-id> label to the resources in that namespace. !!! note In the future, the label on the resources is first set to empty string \"\" , and a coordination controller will be able to apply changes before syncing starts. This includes the ability to add per-location finalizers through the finalizers.workload.kcp.io/<cluster-id> annotation such that the coordination controller gets full control over the downstream life-cycle of the objects per location (imagine an ingress that blocks downstream removal until the new replicas have been launched on another sync target). Finally, the coordination controller will replace the empty string with Sync such that the state machine continues. With the state label set to Sync , the syncer will start seeing the resources in the namespace and starts syncing them downstream, first by creating the namespace. Before syncing, it will also set a finalizer workload.kcp.io/syncer-<cluster-id> on the upstream object in order to delay upstream deletion until the downstream object is also deleted. When the deletion.internal.workload.kcp.io/<cluster-id> is added to the Namespace. The virtual workspace apiserver will translate that annotation into a deletion timestamp on the object the syncer sees. The syncer notices that as a started deletion flow. As soon as there are no coordination controller finalizers registered via the finalizers.workload.kcp.io/<cluster-id> annotation anymore, the syncer will start a deletion of the downstream object. When the downstream deletion is complete, the syncer will remove the finalizer from the upstream object, and the state.workload.kcp.io/<cluster-id> labels gets deleted as well. The syncer stops seeing the object in the virtual workspace. !!! note There is a missing bit in the implementation (in v0.5) about removal of the state.workload.kcp.io/<cluster-id> label from namespaces: the syncer currently does not participate in the namespace deletion state-machine, but has to and signal finished downstream namespace deletion via state.workload.kcp.io/<cluster-id> label removal. For more information on the upsync use case for storage, refer to the [storage doc](storage.md).","title":"Resource Syncing"},{"location":"content/en/concepts/locations-and-scheduling/#resource-upsyncing","text":"In most cases kcp will be the source for syncing resources to the SyncTarget , however, in some cases, kcp would need to receive a resource that was provisioned by a controller on the SyncTarget . This is the case with storage PVs, which are created on the SyncTarget by a CSI driver. Unlike the Sync state, the Upsync state is exclusive, and only a single SyncTarget can be the source of truth for an upsynced resource. In addition, other SyncTargets cannot be syncing down while the resource is being upsynced. A resource coordination controller will be responsible for changing the state.workload.kcp.io/<cluster-id> label, to drive the different flows on the resource. A resource can be changed from Upsync to Sync in order to share it across SyncTargets . This change will be applied by the coordination controller when needed, and the original syncer will detect that change and stop upsyncing to that resource, and all the sync targets involved will be in Sync state.","title":"Resource Upsyncing"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/","text":"Prerequisites A running kcp server. The quickstart is a good starting point. Set your KUBECONFIG To access a workspace, you need credentials and an Kubernetes API server URL for the workspace, both of which are stored in a kubeconfig file. The default context in the kcp-provided admin.kubeconfig gives access to the root workspace as the kcp-admin user. $ export KUBECONFIG=.kcp/admin.kubeconfig $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE base base kcp-admin * root root kcp-admin system:admin base shard-admin You can use API discovery to see what resources are available in this root workspace. We're here to explore the tenancy.kcp.io and apis.kcp.io resources. $ kubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND configmaps cm v1 true ConfigMap events ev v1 true Event ... apibindings apis.kcp.io/v1alpha1 false APIBinding apiexports apis.kcp.io/v1alpha1 false APIExport ... workspaces ws tenancy.kcp.io/v1alpha1 false Workspace Create and navigate some workspaces The ws plugin for kubectl makes it easy to switch your kubeconfig between workspaces, and to create new ones: $ kubectl ws . Current workspace is \"root\". $ kubectl ws create a --enter Workspace \"a\" (type root:organization) created. Waiting for it to be ready... Workspace \"a\" (type root:organization) is ready to use. Current workspace is \"root:a\". $ kubectl ws create b Workspace \"b\" (type root:universal) created. Waiting for it to be ready... Workspace \"b\" (type root:universal) is ready to use. $ kubectl get workspaces NAME TYPE PHASE URL b universal Ready https://myhost:6443/clusters/root:a:b $ kubectl ws b Current workspace is \"root:a:b\". $ kubectl ws .. Current workspace is \"root:a\". $ kubectl ws - Current workspace is \"root:a:b\". $ kubectl ws root Current workspace is \"root\". $ kubectl get workspaces NAME TYPE PHASE URL a organization Ready https://myhost:6443/clusters/root:a Our kubeconfig now contains two additional contexts, one which represents the current workspace, and the other to keep track of our most recently used workspace. This highlights that the kubectl ws plugin is primarily a convenience wrapper for managing a kubeconfig that can be used for working within a workspace. $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE base base kcp-admin root root kcp-admin system:admin base shard-admin * workspace.kcp.io/current workspace.kcp.io/current kcp-admin workspace.kcp.io/previous workspace.kcp.io/previous kcp-admin Understand workspace types As we can see above, workspaces can contain sub-workspaces, and workspaces have different types. A workspace type defines which sub-workspace types can be created under such workspaces. So, for example: A universal workspace is the base workspace type that most other workspace types inherit from - they may contain other universal workspaces, and they have a \"default\" namespace The root workspace primarily contains organization workspaces An organization workspace can contain universal workspaces, or can be further subdivided using team workspaces The final type of workspace is \"home workspaces\". These are workspaces that are intended to be used privately by individual users. They appear under the root:users workspace (type homeroot ) and they are further organized into a hierarchy of homebucket workspaces based on a hash of their name. $ kubectl ws Current workspace is \"root:users:zu:yc:kcp-admin\". $ kubectl ws root Current workspace is \"root\". $ kubectl get workspaces NAME TYPE PHASE URL a organization Ready https://myhost:6443/clusters/root:a users homeroot Ready https://myhost:6443/clusters/root:users Workspace types and their behaviors are defined using the WorkspaceType resource: $ kubectl get workspacetypes NAME AGE home 74m homebucket 74m homeroot 74m organization 74m root 74m team 74m universal 74m $ kubectl describe workspacetype/team Name: team ... API Version: tenancy.kcp.io/v1alpha1 Kind: WorkspaceType ... Spec: Default Child Workspace Type: Name: universal Path: root Extend: With: Name: universal Path: root Limit Allowed Parents: Types: Name: organization Path: root Name: team Path: root Status: Conditions: Status: True Type: Ready ... Workspaces FAQs Q: Why do we have both ClusterWorkspaces and Workspaces ? A: Workspaces are intended to be the user-facing resource, whereas ClusterWorkspaces is a low-level resource for kcp platform admins. Workspaces are actually a \"projected resource\", there is no such resource stored in etcd, instead it is served as a transformation of the ClusterWorkspace resource. ClusterWorkspaces contain details like shard assignment, which are low-level fields that users should not see. We are working on a change of this system behind the scenes. That will probably promote Workspaces to a normal, non-projected resource, and ClusterWorkspaces will change in its role. Publish some APIs as a service provider kcp offers APIExport and APIBinding resources which allow a service provider operating in one workspace to offer its capabilities to service consumers in other workspaces. First we'll create an organization workspace, and then within that create a service provider workspace. $ kubectl ws create wildwest --enter Workspace \"wildwest\" (type root:organization) created. Waiting for it to be ready... Workspace \"wildwest\" (type root:organization) is ready to use. Current workspace is \"root:wildwest\". $ kubectl ws create cowboys-service --enter Workspace \"cowboys-service\" (type root:universal) created. Waiting for it to be ready... Workspace \"cowboys-service\" (type root:universal) is ready to use. Current workspace is \"root:wildwest:cowboys-service\". Then we'll use a CRD to generate an APIResourceSchema and APIExport and apply these within the service provider workspace. The apigen tool used below can be found here . Builds for the tool are not currently published as part of the kcp release process. $ mkdir wildwest-schemas/ $ ./bin/apigen --input-dir test/e2e/customresourcedefinition/ --output-dir wildwest-schemas/ $ ls -1 wildwest-schemas/ apiexport-wildwest.dev.yaml apiresourceschema-cowboys.wildwest.dev.yaml $ kubectl apply -f wildwest-schemas/apiresourceschema-cowboys.wildwest.dev.yaml apiresourceschema.apis.kcp.io/v220920-6039d110.cowboys.wildwest.dev created $ kubectl apply -f wildwest-schemas/apiexport-wildwest.dev.yaml apiexport.apis.kcp.io/wildwest.dev created You can think of an APIResourceSchema as being equivalent to a CRD, and an APIExport makes a set of schemas available to consumers. Use those APIs as a service consumer Now we can adopt the service consumer persona and create a workspace from which we will use this new APIExport : $ kubectl ws Current workspace is \"root:users:zu:yc:kcp-admin\". $ kubectl ws create --enter test-consumer Workspace \"test-consumer\" (type root:universal) created. Waiting for it to be ready... Workspace \"test-consumer\" (type root:universal) is ready to use. Current workspace is \"root:users:zu:yc:kcp-admin:test-consumer\". $ kubectl apply -f - <<EOF apiVersion: apis.kcp.io/v1alpha1 kind: APIBinding metadata: name: cowboys spec: reference: workspace: path: root:wildwest:cowboys-service exportName: wildwest.dev EOF apibinding.apis.kcp.io/cowboys created Now this resource type is available for use within our workspace, so let's create an instance! $ kubectl api-resources | grep wildwest cowboys wildwest.dev/v1alpha1 true Cowboy $ kubectl apply -f - <<EOF apiVersion: wildwest.dev/v1alpha1 kind: Cowboy metadata: name: one spec: intent: one EOF cowboy.wildwest.dev/one created Dig deeper into APIExports Switching back to the service provider persona: $ kubectl ws root:wildwest:cowboys-service Current workspace is \"root:wildwest:cowboys-service\". $ kubectl get apiexport/wildwest.dev -o yaml apiVersion: apis.kcp.io/v1alpha1 kind: APIExport metadata: name: wildwest.dev ... status: ... identityHash: a6a0cc778bec8c4b844e6326965fbb740b6a9590963578afe07276e6a0d41e20 virtualWorkspaces: - url: https://myhost:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev We can see that our APIExport has two key attributes in its status - its identity (more on this below) and a \"virtual workspace\" URL. You can think of this virtual workspace as behaving just like a workspace or cluster, except it searches across all true workspaces for instances of the resource types provided by the APIExport . We can use API discovery to see what resource types are available via this virtual workspace: $ kubectl --server='https://myhost:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev/clusters/*/' api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND cowboys wildwest.dev/v1alpha1 true Cowboy The question is ... can we see the instance created by the consumer? $ kubectl --server='https://myhost:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev/clusters/*/' get -A cowboys \\ -o custom-columns='WORKSPACE:.metadata.annotations.kcp\\.dev/cluster,NAME:.metadata.name' WORKSPACE NAME root:users:zu:yc:kcp-admin:test-consumer one Yay! APIs FAQ Q: Why is there a new APIResourceSchema resource type that appears to be very similar to CustomResourceDefinition ? A: FIXME Q: Why do I have to append /clusters/*/ to the APIExport virtual workspace URL? A: The URL represents the base path of a virtual kcp API server. With a standard kcp API server, workspaces live under the /clusters/ path, so /clusters/*/ represents a wildcard search across all workspaces via this virtual API server. Q: How should we understand an APIExport identityHash ? A: Unlike with CRDs, a kcp instance might have many APIResourceSchemas of the same Group/Version/Kind, and users need some way of securely distinguishing them. Each APIExport is allocated a randomized private secret - this is currently just a large random number - and a public identity - just a SHA256 hash of the private secret - which securely identifies this APIExport from others. This is important because an APIExport makes a virtual workspace available to interact with all instances of a particular APIResourceShema , and we want to make sure that users are clear on which service provider APIExports they are trusting and only the owners of those APIExport have access to their resources via virtual workspaces. Q: Why do you have to use --all-namespaces with the apiexport virtual workspace? A: Think of this virtual workspace as representing a wildcard listing across all workspaces. It doesn't make sense to look at a specific namespace across all workspaces, so you have to list across all namespaces too. Q: If I attempt to use an APIExport virtual workspace before there are any APIBindings I get the \"Error from server (NotFound): Unable to list ...: the server could not find the requested resource\". Is this a bug? A: It is a bug. See https://github.com/kcp-dev/kcp/issues/1183 When fixed, we expect the APIExport behavior will change such that there will be no virtual workspace URLs until an APIBinding is created.","title":"Quickstart: Tenancy and APIs"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/#prerequisites","text":"A running kcp server. The quickstart is a good starting point.","title":"Prerequisites"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/#set-your-kubeconfig","text":"To access a workspace, you need credentials and an Kubernetes API server URL for the workspace, both of which are stored in a kubeconfig file. The default context in the kcp-provided admin.kubeconfig gives access to the root workspace as the kcp-admin user. $ export KUBECONFIG=.kcp/admin.kubeconfig $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE base base kcp-admin * root root kcp-admin system:admin base shard-admin You can use API discovery to see what resources are available in this root workspace. We're here to explore the tenancy.kcp.io and apis.kcp.io resources. $ kubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND configmaps cm v1 true ConfigMap events ev v1 true Event ... apibindings apis.kcp.io/v1alpha1 false APIBinding apiexports apis.kcp.io/v1alpha1 false APIExport ... workspaces ws tenancy.kcp.io/v1alpha1 false Workspace","title":"Set your KUBECONFIG"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/#create-and-navigate-some-workspaces","text":"The ws plugin for kubectl makes it easy to switch your kubeconfig between workspaces, and to create new ones: $ kubectl ws . Current workspace is \"root\". $ kubectl ws create a --enter Workspace \"a\" (type root:organization) created. Waiting for it to be ready... Workspace \"a\" (type root:organization) is ready to use. Current workspace is \"root:a\". $ kubectl ws create b Workspace \"b\" (type root:universal) created. Waiting for it to be ready... Workspace \"b\" (type root:universal) is ready to use. $ kubectl get workspaces NAME TYPE PHASE URL b universal Ready https://myhost:6443/clusters/root:a:b $ kubectl ws b Current workspace is \"root:a:b\". $ kubectl ws .. Current workspace is \"root:a\". $ kubectl ws - Current workspace is \"root:a:b\". $ kubectl ws root Current workspace is \"root\". $ kubectl get workspaces NAME TYPE PHASE URL a organization Ready https://myhost:6443/clusters/root:a Our kubeconfig now contains two additional contexts, one which represents the current workspace, and the other to keep track of our most recently used workspace. This highlights that the kubectl ws plugin is primarily a convenience wrapper for managing a kubeconfig that can be used for working within a workspace. $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE base base kcp-admin root root kcp-admin system:admin base shard-admin * workspace.kcp.io/current workspace.kcp.io/current kcp-admin workspace.kcp.io/previous workspace.kcp.io/previous kcp-admin","title":"Create and navigate some workspaces"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/#understand-workspace-types","text":"As we can see above, workspaces can contain sub-workspaces, and workspaces have different types. A workspace type defines which sub-workspace types can be created under such workspaces. So, for example: A universal workspace is the base workspace type that most other workspace types inherit from - they may contain other universal workspaces, and they have a \"default\" namespace The root workspace primarily contains organization workspaces An organization workspace can contain universal workspaces, or can be further subdivided using team workspaces The final type of workspace is \"home workspaces\". These are workspaces that are intended to be used privately by individual users. They appear under the root:users workspace (type homeroot ) and they are further organized into a hierarchy of homebucket workspaces based on a hash of their name. $ kubectl ws Current workspace is \"root:users:zu:yc:kcp-admin\". $ kubectl ws root Current workspace is \"root\". $ kubectl get workspaces NAME TYPE PHASE URL a organization Ready https://myhost:6443/clusters/root:a users homeroot Ready https://myhost:6443/clusters/root:users Workspace types and their behaviors are defined using the WorkspaceType resource: $ kubectl get workspacetypes NAME AGE home 74m homebucket 74m homeroot 74m organization 74m root 74m team 74m universal 74m $ kubectl describe workspacetype/team Name: team ... API Version: tenancy.kcp.io/v1alpha1 Kind: WorkspaceType ... Spec: Default Child Workspace Type: Name: universal Path: root Extend: With: Name: universal Path: root Limit Allowed Parents: Types: Name: organization Path: root Name: team Path: root Status: Conditions: Status: True Type: Ready ...","title":"Understand workspace types"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/#workspaces-faqs","text":"Q: Why do we have both ClusterWorkspaces and Workspaces ? A: Workspaces are intended to be the user-facing resource, whereas ClusterWorkspaces is a low-level resource for kcp platform admins. Workspaces are actually a \"projected resource\", there is no such resource stored in etcd, instead it is served as a transformation of the ClusterWorkspace resource. ClusterWorkspaces contain details like shard assignment, which are low-level fields that users should not see. We are working on a change of this system behind the scenes. That will probably promote Workspaces to a normal, non-projected resource, and ClusterWorkspaces will change in its role.","title":"Workspaces FAQs"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/#publish-some-apis-as-a-service-provider","text":"kcp offers APIExport and APIBinding resources which allow a service provider operating in one workspace to offer its capabilities to service consumers in other workspaces. First we'll create an organization workspace, and then within that create a service provider workspace. $ kubectl ws create wildwest --enter Workspace \"wildwest\" (type root:organization) created. Waiting for it to be ready... Workspace \"wildwest\" (type root:organization) is ready to use. Current workspace is \"root:wildwest\". $ kubectl ws create cowboys-service --enter Workspace \"cowboys-service\" (type root:universal) created. Waiting for it to be ready... Workspace \"cowboys-service\" (type root:universal) is ready to use. Current workspace is \"root:wildwest:cowboys-service\". Then we'll use a CRD to generate an APIResourceSchema and APIExport and apply these within the service provider workspace. The apigen tool used below can be found here . Builds for the tool are not currently published as part of the kcp release process. $ mkdir wildwest-schemas/ $ ./bin/apigen --input-dir test/e2e/customresourcedefinition/ --output-dir wildwest-schemas/ $ ls -1 wildwest-schemas/ apiexport-wildwest.dev.yaml apiresourceschema-cowboys.wildwest.dev.yaml $ kubectl apply -f wildwest-schemas/apiresourceschema-cowboys.wildwest.dev.yaml apiresourceschema.apis.kcp.io/v220920-6039d110.cowboys.wildwest.dev created $ kubectl apply -f wildwest-schemas/apiexport-wildwest.dev.yaml apiexport.apis.kcp.io/wildwest.dev created You can think of an APIResourceSchema as being equivalent to a CRD, and an APIExport makes a set of schemas available to consumers.","title":"Publish some APIs as a service provider"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/#use-those-apis-as-a-service-consumer","text":"Now we can adopt the service consumer persona and create a workspace from which we will use this new APIExport : $ kubectl ws Current workspace is \"root:users:zu:yc:kcp-admin\". $ kubectl ws create --enter test-consumer Workspace \"test-consumer\" (type root:universal) created. Waiting for it to be ready... Workspace \"test-consumer\" (type root:universal) is ready to use. Current workspace is \"root:users:zu:yc:kcp-admin:test-consumer\". $ kubectl apply -f - <<EOF apiVersion: apis.kcp.io/v1alpha1 kind: APIBinding metadata: name: cowboys spec: reference: workspace: path: root:wildwest:cowboys-service exportName: wildwest.dev EOF apibinding.apis.kcp.io/cowboys created Now this resource type is available for use within our workspace, so let's create an instance! $ kubectl api-resources | grep wildwest cowboys wildwest.dev/v1alpha1 true Cowboy $ kubectl apply -f - <<EOF apiVersion: wildwest.dev/v1alpha1 kind: Cowboy metadata: name: one spec: intent: one EOF cowboy.wildwest.dev/one created","title":"Use those APIs as a service consumer"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/#dig-deeper-into-apiexports","text":"Switching back to the service provider persona: $ kubectl ws root:wildwest:cowboys-service Current workspace is \"root:wildwest:cowboys-service\". $ kubectl get apiexport/wildwest.dev -o yaml apiVersion: apis.kcp.io/v1alpha1 kind: APIExport metadata: name: wildwest.dev ... status: ... identityHash: a6a0cc778bec8c4b844e6326965fbb740b6a9590963578afe07276e6a0d41e20 virtualWorkspaces: - url: https://myhost:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev We can see that our APIExport has two key attributes in its status - its identity (more on this below) and a \"virtual workspace\" URL. You can think of this virtual workspace as behaving just like a workspace or cluster, except it searches across all true workspaces for instances of the resource types provided by the APIExport . We can use API discovery to see what resource types are available via this virtual workspace: $ kubectl --server='https://myhost:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev/clusters/*/' api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND cowboys wildwest.dev/v1alpha1 true Cowboy The question is ... can we see the instance created by the consumer? $ kubectl --server='https://myhost:6443/services/apiexport/root:wildwest:cowboys-service/wildwest.dev/clusters/*/' get -A cowboys \\ -o custom-columns='WORKSPACE:.metadata.annotations.kcp\\.dev/cluster,NAME:.metadata.name' WORKSPACE NAME root:users:zu:yc:kcp-admin:test-consumer one Yay!","title":"Dig deeper into APIExports"},{"location":"content/en/concepts/quickstart-tenancy-and-apis/#apis-faq","text":"Q: Why is there a new APIResourceSchema resource type that appears to be very similar to CustomResourceDefinition ? A: FIXME Q: Why do I have to append /clusters/*/ to the APIExport virtual workspace URL? A: The URL represents the base path of a virtual kcp API server. With a standard kcp API server, workspaces live under the /clusters/ path, so /clusters/*/ represents a wildcard search across all workspaces via this virtual API server. Q: How should we understand an APIExport identityHash ? A: Unlike with CRDs, a kcp instance might have many APIResourceSchemas of the same Group/Version/Kind, and users need some way of securely distinguishing them. Each APIExport is allocated a randomized private secret - this is currently just a large random number - and a public identity - just a SHA256 hash of the private secret - which securely identifies this APIExport from others. This is important because an APIExport makes a virtual workspace available to interact with all instances of a particular APIResourceShema , and we want to make sure that users are clear on which service provider APIExports they are trusting and only the owners of those APIExport have access to their resources via virtual workspaces. Q: Why do you have to use --all-namespaces with the apiexport virtual workspace? A: Think of this virtual workspace as representing a wildcard listing across all workspaces. It doesn't make sense to look at a specific namespace across all workspaces, so you have to list across all namespaces too. Q: If I attempt to use an APIExport virtual workspace before there are any APIBindings I get the \"Error from server (NotFound): Unable to list ...: the server could not find the requested resource\". Is this a bug? A: It is a bug. See https://github.com/kcp-dev/kcp/issues/1183 When fixed, we expect the APIExport behavior will change such that there will be no virtual workspace URLs until an APIBinding is created.","title":"APIs FAQ"},{"location":"content/en/concepts/storage/","text":"Storage and stateful applications Overview KCP provides a control plane that implements the concept of Transparent Multi Cluster (TMC) for compute, network, and storage. In order to give the illusion of transparent storage in KCP, it exposes the same Kubernetes APIs for storage (PVC/PV), so users and workloads do not need to be aware of the coordinations taken by the control plane behind the scenes. Placement for storage in KCP uses the same concepts used for compute : \" SyncTargets in a Location are transparent to the user, and workloads should be able to seamlessly move from one SyncTarget to another within a Location , based on operational concerns of the compute service provider, like decommissioning a cluster, rebalancing capacity, or due to an outage of a cluster. It is the compute service's responsibility to ensure that for workloads in a location, to the user it looks like ONE cluster.\" KCP will provide the basic controllers and coordination logic for moving volumes, as efficiently as possible, using the underlying storage topology and capabilities. It will use the SyncTargets storage APIs to manage volumes, and not require direct access from the control plane to the storage itself. For more advanced or custom solutions, KCP will allow external coordinators to take over. Main concepts Transparent multi-cluster - describes the TMC concepts. Placement, Locations and Scheduling - describes the KCP APIs and mechanisms used to control compute placement, which will be used for storage as well. Refer to the concepts of SyncTarget , Location , and Placement . Kubernetes storage concepts - documentation of storage APIs in Kubernetes. Persistent Volumes - PVCs are the main storage APIs used to request storage resources for applications. PVs are invisible to users, and used by administrators or privileged controllers to provision storage to user claims, and will be coordinated by KCP to support transparent multi-cluster storage. Kubernetes CSI - The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage systems to containerized workloads. The list of drivers provides a \"menu\" of storage systems integrated with kubernetes and their properties. StatefulSets volumeClaimTemplates - workload definition used to manage \u201csharded\u201d stateful applications. Specifying volumeClaimTemplates in the statefulset spec will provide stable storage by creating a PVC per instance. Volume types Each physical-cluster (aka \"pcluster\") brings its own storage to multi-cluster environments, and in order to make efficient coordination decisions, KCP will identify the following types: Shared network-volumes These volumes are provisioned from an external storage system that is available to all/some of the pclusters over an infrastructure network. These volumes are typically provided by a shared-filesystem (aka NAS ), with access-mode of ReadWriteMany (RWX) or ReadOnlyMany (ROX). A shared volume can be used by any pod from any pcluster (that can reach it) at the same time. The application is responsible for the consistency of its data (for example with eventual consistency semantics, or stronger synchronization services like zookeeper). Examples of such storage are generic-NFS/SMB, AWS-EFS, Azure-File, GCP-Filestore, CephFS, GlusterFS, NetApp, GPFS, etc. Owned network-volumes These volumes are provisioned from an external storage system that is available to all/some of the pclusters over an infrastructure network. However unlike shared volumes, owned volumes require that only a single node/pod will mount the volume at a time. These volumes are typically provided by a block-level storage system, with access-mode of ReadWriteOnce (RWO) or ReadWriteOncePod (RWOP). It is possible to move the ownership between pclusters (that have access to that storage), by detaching from the current owner, and then attaching to the new owner. But it would have to guarantee a single owner to prevent data inconsistencies or corruptions, and even work if the owner pcluster is offline (see forcing detach with \u201cfencing\u201d below). Examples of such storage are AWS-EBS, Azure-Disk, Ceph-RBD, etc. Internal volumes These volumes are provisioned inside the pcluster itself, and rely on its internal resources (aka hyper-converged or software-defined storage). This means that the availability of the pcluster also determines the availability of the volume. In some systems these volumes are bound to a single node in the pcluster, because the storage is physically attached to a host. However, advanced clustered/distributed systems make efforts to overcome temporary and permanent node failures by adding data redundancy over multiple nodes. These volumes can have any type of access-mode (RWO/RWOP/RWX/ROX), but their strong dependency on the pcluster itself is the key difference from network volumes. Examples of such storage are host-path/local-drives, TopoLVM, Ceph-rook, Portworx, OpenEBS, etc. Topology and locations Regular topology A regular storage topology is one where every Location is defined so that all of its SyncTargets are connected to the same storage system. This makes it trivial to move network volumes transparently between SyncTargets inside the same location. Multi-zone cluster A more complex topology is where pclusters contain nodes from several availability-zones, for the sake of being resilient to a zone failure. Since volumes are bound to a single zone (where they were provisioned), then a volume will not be able to move between SyncTargets without nodes on that zone. This is ok if all the SyncTargets of the Location span over the same set of zones, but if the zones are different, or the capacity per zone is too limited, copying to another zone might be necessary. Internal volumes Internal volumes are always confined to one pcluster, which means it has to be copied outside of the pcluster continuously to keep the application available even in the case where the pcluster fails entirely (network split, region issue, etc). This is similar to how DR solutions work between locations. Disaster recover between locations A regular Disaster Recovery (DR) topology will create pairs of Locations so that one is \u201cprimary\u201d and the other is \u201csecondary\u201d (sometimes this relation is mutual). For volumes to be able to move between these locations, their storage systems would need to be configured to mirror/replicate/backup/snapshot (whichever approach is more appropriate depends on the case) every volume to its secondary. With such a setup, KCP would need to be able to map between the volumes on the primary and the secondary, so that it could failover and move workloads to the secondary and reconnect to the last copied volume state. See more on the DR section below. Provisioning volumes Volume provisioning in Kubernetes involves the CSI controllers and sidecar, as well as a custom storage driver. It reconciles PVCs by dynamically creating a PV for a PVC, and binding them together. This process depends on the CSI driver to be running on the SyncTarget compute resources, and would not be able to run on KCP workspaces. Instead, KCP will pick a designated SyncTarget for the workload placement, which will include the storage claims (PVCs), and the CSI driver on the SyncTarget will perform the storage provisioning. In order to support changing workload placement overtime, even if the provisioning SyncTarget is offline, KCP will have to retrieve the volume information from that SyncTarget , and keep it in the KCP workspace for future coordination. The volume information inside the PV is expected to be transferable between SyncTargets that connect to the same storage system and drivers, although some transformations would be required. To retrieve the volume information and maintain it in KCP, a special sync state is required that will sync UP the PV from a SyncTarget to KCP. This state is referred to as Upsync - see Resource Upsyncing . The provisioning flow includes: (A) PVC synced to SyncTarget , (B) CSI provisioning on the pcluster, (C) Syncer detects PVC binding and initiates PV Upsync . Transformations would be applied in KCP virtual workspace to make sure that the PVC and PV would appear bound in KCP, similar to how it is in a single cluster. Once provisioning itself is complete, coordination logic will switch to a normal Sync state, to allow multiple SyncTargets to share the same volume, and for owned volumes to move ownership to another SyncTarget . Moving shared volumes Shared volume can easily move to any SyncTarget in the same Location by syncing the PVC and PV together, so they bind only to each other on the pcluster. Syncing will transform their mutual references so that the PVC.volumeName = PV.name and PV.claimRef = { PVC.name, PVC.namespace } are set appropriately for the SyncTarget , since the downstream PVC.namespace and PV.name will not be the same as upstream. Moving volumes will set the volume's reclaimPolicy to always Retain , to avoid unintended deletion by any one of the SyncTargets while others use it. Once deletion of the upstream PVC is initiated, the coordination controller will transform the reclaimPolicy to Delete for one of the SyncTargets . See more in the section on deleting volumes. Moving owned volumes TBD - this section is a work in progress... Detach from owner Owned volumes require that at most one pcluster can use them at any given time. As placement changes, the coordination controller is responsible to serialize the state changes of the volume to move the ownership of the volume safely. First, it will detach the volume from the current owner, and wait for it to acknowledge that it successfully removed it, and only then will sync the volume to a new target. Forcing detach with fencing However, in case the owner is not able to acknowledge that it detached the volume, a forced-detach flow might be possible. The storage system has to support a CSI extension for network fencing, effectively blocking an entire pcluster from accessing the storage until fencing is removed. Once the failed pcluster recovers, and can acknowledge that it detached from the moved volumes, fencing will be removed from the storage and that pcluster can recover the rest of its workloads. kubernetes-csi-addons NetworkFence (currently implemented only by ceph-csi). Storage classes TBD - this section is a work in progress... Storage classes can be thought of as templates to PVs, which allow pclusters to support multiple storage providers, or configure different policies for the same provider. Just like PVs are invisible to users, so do storage classes. However, users may choose a storage class by name when specifying their PVCs. When the storage class field is left unspecified (which is common), the pcluster will use its default storage class. However, the default storage class is a bit limited for multi-tenancy because it is one class per the entire pcluster. Matching storage classes between SyncTargets in the same Location would be a simple way to ensure that storage can be moved transparently. However KCP should be able to verify the storage classes match across the Location and warn when this is not the case, to prevent future issues. Open questions How to match classes and make sure the same storage system is used in the location? How to support multiple classes per pcluster (eg. RWO + RWX)? Maybe a separate SyncTarget per class? Can we have a separate default class per workspace? Deleting volumes TBD - this section is a work in progress... Persistent-volumes reclaiming allows volumes to be configured how to behave when they are reclaimed. By default, storage classes will apply a reclaimPolicy: Delete to dynamically provisioned PVs unless explicitly specified to Retain . This means that volumes there were provisioned, will also get de-provisioned and their storage will be deleted. However, admins can modify the class to Retain volumes, and invoke cleanup on their own schedule. While moving volumes, either shared or owned, the volume's reclaimPolicy will be set to Retain to prevent any SyncTarget from releasing the volume storage on scheduling changes. Once the PVC is marked for deletion on KCP, the coordination controller will first pick one SyncTarget as owner (or use the current owner for owned volumes) and make sure to remove all sharers, and wait for their sync state to be cleared. Then it will set the owner's volume reclaimPolicy to Delete so that it will release the volume storage. Setting a PV to Retain on KCP itself should also be respected by the controllers and allow manual cleanup of the volume in KCP, instead of automatically with the PVC. Copying volumes TBD - this section is a work in progress... ramen volume-replication-operator volsync Disaster recovery TBD - this section is a work in progress... Pairing locations as continuously replicating storage between each other. KCP would have to be able to map primary volumes to secondary volumes to failover workloads between locations. Examples TBD - this section is a work in progress... Shared NFS storage NFS server running in every location, external to the SyncTarget , but available over the network. Note that high-availability and data-protection of the storage itself is out of scope and would be handled by storage admin or provided by enterprise products. Workloads allow volumes with RWX access-mode. KCP picks one SyncTarget to be the provisioner and syncs up the volume information. After provisioning completes, sync down to any SyncTarget in the Location that the workload decides to be placed to allow moving transparently as needed when clusters become offline or drained. Once the PVC is deleted, the deletion of the volume itself is performed by one of the SyncTargets . Roadmap Moving owned volumes Fencing Copy-on-demand Copy-continuous DR-location-pairing and primary->secondary volume mapping Statefulsets COSI Bucket + BucketAccess","title":"Storage and stateful applications"},{"location":"content/en/concepts/storage/#storage-and-stateful-applications","text":"","title":"Storage and stateful applications"},{"location":"content/en/concepts/storage/#overview","text":"KCP provides a control plane that implements the concept of Transparent Multi Cluster (TMC) for compute, network, and storage. In order to give the illusion of transparent storage in KCP, it exposes the same Kubernetes APIs for storage (PVC/PV), so users and workloads do not need to be aware of the coordinations taken by the control plane behind the scenes. Placement for storage in KCP uses the same concepts used for compute : \" SyncTargets in a Location are transparent to the user, and workloads should be able to seamlessly move from one SyncTarget to another within a Location , based on operational concerns of the compute service provider, like decommissioning a cluster, rebalancing capacity, or due to an outage of a cluster. It is the compute service's responsibility to ensure that for workloads in a location, to the user it looks like ONE cluster.\" KCP will provide the basic controllers and coordination logic for moving volumes, as efficiently as possible, using the underlying storage topology and capabilities. It will use the SyncTargets storage APIs to manage volumes, and not require direct access from the control plane to the storage itself. For more advanced or custom solutions, KCP will allow external coordinators to take over.","title":"Overview"},{"location":"content/en/concepts/storage/#main-concepts","text":"Transparent multi-cluster - describes the TMC concepts. Placement, Locations and Scheduling - describes the KCP APIs and mechanisms used to control compute placement, which will be used for storage as well. Refer to the concepts of SyncTarget , Location , and Placement . Kubernetes storage concepts - documentation of storage APIs in Kubernetes. Persistent Volumes - PVCs are the main storage APIs used to request storage resources for applications. PVs are invisible to users, and used by administrators or privileged controllers to provision storage to user claims, and will be coordinated by KCP to support transparent multi-cluster storage. Kubernetes CSI - The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage systems to containerized workloads. The list of drivers provides a \"menu\" of storage systems integrated with kubernetes and their properties. StatefulSets volumeClaimTemplates - workload definition used to manage \u201csharded\u201d stateful applications. Specifying volumeClaimTemplates in the statefulset spec will provide stable storage by creating a PVC per instance.","title":"Main concepts"},{"location":"content/en/concepts/storage/#volume-types","text":"Each physical-cluster (aka \"pcluster\") brings its own storage to multi-cluster environments, and in order to make efficient coordination decisions, KCP will identify the following types:","title":"Volume types"},{"location":"content/en/concepts/storage/#shared-network-volumes","text":"These volumes are provisioned from an external storage system that is available to all/some of the pclusters over an infrastructure network. These volumes are typically provided by a shared-filesystem (aka NAS ), with access-mode of ReadWriteMany (RWX) or ReadOnlyMany (ROX). A shared volume can be used by any pod from any pcluster (that can reach it) at the same time. The application is responsible for the consistency of its data (for example with eventual consistency semantics, or stronger synchronization services like zookeeper). Examples of such storage are generic-NFS/SMB, AWS-EFS, Azure-File, GCP-Filestore, CephFS, GlusterFS, NetApp, GPFS, etc.","title":"Shared network-volumes"},{"location":"content/en/concepts/storage/#owned-network-volumes","text":"These volumes are provisioned from an external storage system that is available to all/some of the pclusters over an infrastructure network. However unlike shared volumes, owned volumes require that only a single node/pod will mount the volume at a time. These volumes are typically provided by a block-level storage system, with access-mode of ReadWriteOnce (RWO) or ReadWriteOncePod (RWOP). It is possible to move the ownership between pclusters (that have access to that storage), by detaching from the current owner, and then attaching to the new owner. But it would have to guarantee a single owner to prevent data inconsistencies or corruptions, and even work if the owner pcluster is offline (see forcing detach with \u201cfencing\u201d below). Examples of such storage are AWS-EBS, Azure-Disk, Ceph-RBD, etc.","title":"Owned network-volumes"},{"location":"content/en/concepts/storage/#internal-volumes","text":"These volumes are provisioned inside the pcluster itself, and rely on its internal resources (aka hyper-converged or software-defined storage). This means that the availability of the pcluster also determines the availability of the volume. In some systems these volumes are bound to a single node in the pcluster, because the storage is physically attached to a host. However, advanced clustered/distributed systems make efforts to overcome temporary and permanent node failures by adding data redundancy over multiple nodes. These volumes can have any type of access-mode (RWO/RWOP/RWX/ROX), but their strong dependency on the pcluster itself is the key difference from network volumes. Examples of such storage are host-path/local-drives, TopoLVM, Ceph-rook, Portworx, OpenEBS, etc.","title":"Internal volumes"},{"location":"content/en/concepts/storage/#topology-and-locations","text":"","title":"Topology and locations"},{"location":"content/en/concepts/storage/#regular-topology","text":"A regular storage topology is one where every Location is defined so that all of its SyncTargets are connected to the same storage system. This makes it trivial to move network volumes transparently between SyncTargets inside the same location.","title":"Regular topology"},{"location":"content/en/concepts/storage/#multi-zone-cluster","text":"A more complex topology is where pclusters contain nodes from several availability-zones, for the sake of being resilient to a zone failure. Since volumes are bound to a single zone (where they were provisioned), then a volume will not be able to move between SyncTargets without nodes on that zone. This is ok if all the SyncTargets of the Location span over the same set of zones, but if the zones are different, or the capacity per zone is too limited, copying to another zone might be necessary.","title":"Multi-zone cluster"},{"location":"content/en/concepts/storage/#internal-volumes_1","text":"Internal volumes are always confined to one pcluster, which means it has to be copied outside of the pcluster continuously to keep the application available even in the case where the pcluster fails entirely (network split, region issue, etc). This is similar to how DR solutions work between locations.","title":"Internal volumes"},{"location":"content/en/concepts/storage/#disaster-recover-between-locations","text":"A regular Disaster Recovery (DR) topology will create pairs of Locations so that one is \u201cprimary\u201d and the other is \u201csecondary\u201d (sometimes this relation is mutual). For volumes to be able to move between these locations, their storage systems would need to be configured to mirror/replicate/backup/snapshot (whichever approach is more appropriate depends on the case) every volume to its secondary. With such a setup, KCP would need to be able to map between the volumes on the primary and the secondary, so that it could failover and move workloads to the secondary and reconnect to the last copied volume state. See more on the DR section below.","title":"Disaster recover between locations"},{"location":"content/en/concepts/storage/#provisioning-volumes","text":"Volume provisioning in Kubernetes involves the CSI controllers and sidecar, as well as a custom storage driver. It reconciles PVCs by dynamically creating a PV for a PVC, and binding them together. This process depends on the CSI driver to be running on the SyncTarget compute resources, and would not be able to run on KCP workspaces. Instead, KCP will pick a designated SyncTarget for the workload placement, which will include the storage claims (PVCs), and the CSI driver on the SyncTarget will perform the storage provisioning. In order to support changing workload placement overtime, even if the provisioning SyncTarget is offline, KCP will have to retrieve the volume information from that SyncTarget , and keep it in the KCP workspace for future coordination. The volume information inside the PV is expected to be transferable between SyncTargets that connect to the same storage system and drivers, although some transformations would be required. To retrieve the volume information and maintain it in KCP, a special sync state is required that will sync UP the PV from a SyncTarget to KCP. This state is referred to as Upsync - see Resource Upsyncing . The provisioning flow includes: (A) PVC synced to SyncTarget , (B) CSI provisioning on the pcluster, (C) Syncer detects PVC binding and initiates PV Upsync . Transformations would be applied in KCP virtual workspace to make sure that the PVC and PV would appear bound in KCP, similar to how it is in a single cluster. Once provisioning itself is complete, coordination logic will switch to a normal Sync state, to allow multiple SyncTargets to share the same volume, and for owned volumes to move ownership to another SyncTarget .","title":"Provisioning volumes"},{"location":"content/en/concepts/storage/#moving-shared-volumes","text":"Shared volume can easily move to any SyncTarget in the same Location by syncing the PVC and PV together, so they bind only to each other on the pcluster. Syncing will transform their mutual references so that the PVC.volumeName = PV.name and PV.claimRef = { PVC.name, PVC.namespace } are set appropriately for the SyncTarget , since the downstream PVC.namespace and PV.name will not be the same as upstream. Moving volumes will set the volume's reclaimPolicy to always Retain , to avoid unintended deletion by any one of the SyncTargets while others use it. Once deletion of the upstream PVC is initiated, the coordination controller will transform the reclaimPolicy to Delete for one of the SyncTargets . See more in the section on deleting volumes.","title":"Moving shared volumes"},{"location":"content/en/concepts/storage/#moving-owned-volumes","text":"TBD - this section is a work in progress...","title":"Moving owned volumes"},{"location":"content/en/concepts/storage/#detach-from-owner","text":"Owned volumes require that at most one pcluster can use them at any given time. As placement changes, the coordination controller is responsible to serialize the state changes of the volume to move the ownership of the volume safely. First, it will detach the volume from the current owner, and wait for it to acknowledge that it successfully removed it, and only then will sync the volume to a new target.","title":"Detach from owner"},{"location":"content/en/concepts/storage/#forcing-detach-with-fencing","text":"However, in case the owner is not able to acknowledge that it detached the volume, a forced-detach flow might be possible. The storage system has to support a CSI extension for network fencing, effectively blocking an entire pcluster from accessing the storage until fencing is removed. Once the failed pcluster recovers, and can acknowledge that it detached from the moved volumes, fencing will be removed from the storage and that pcluster can recover the rest of its workloads. kubernetes-csi-addons NetworkFence (currently implemented only by ceph-csi).","title":"Forcing detach with fencing"},{"location":"content/en/concepts/storage/#storage-classes","text":"TBD - this section is a work in progress... Storage classes can be thought of as templates to PVs, which allow pclusters to support multiple storage providers, or configure different policies for the same provider. Just like PVs are invisible to users, so do storage classes. However, users may choose a storage class by name when specifying their PVCs. When the storage class field is left unspecified (which is common), the pcluster will use its default storage class. However, the default storage class is a bit limited for multi-tenancy because it is one class per the entire pcluster. Matching storage classes between SyncTargets in the same Location would be a simple way to ensure that storage can be moved transparently. However KCP should be able to verify the storage classes match across the Location and warn when this is not the case, to prevent future issues.","title":"Storage classes"},{"location":"content/en/concepts/storage/#open-questions","text":"How to match classes and make sure the same storage system is used in the location? How to support multiple classes per pcluster (eg. RWO + RWX)? Maybe a separate SyncTarget per class? Can we have a separate default class per workspace?","title":"Open questions"},{"location":"content/en/concepts/storage/#deleting-volumes","text":"TBD - this section is a work in progress... Persistent-volumes reclaiming allows volumes to be configured how to behave when they are reclaimed. By default, storage classes will apply a reclaimPolicy: Delete to dynamically provisioned PVs unless explicitly specified to Retain . This means that volumes there were provisioned, will also get de-provisioned and their storage will be deleted. However, admins can modify the class to Retain volumes, and invoke cleanup on their own schedule. While moving volumes, either shared or owned, the volume's reclaimPolicy will be set to Retain to prevent any SyncTarget from releasing the volume storage on scheduling changes. Once the PVC is marked for deletion on KCP, the coordination controller will first pick one SyncTarget as owner (or use the current owner for owned volumes) and make sure to remove all sharers, and wait for their sync state to be cleared. Then it will set the owner's volume reclaimPolicy to Delete so that it will release the volume storage. Setting a PV to Retain on KCP itself should also be respected by the controllers and allow manual cleanup of the volume in KCP, instead of automatically with the PVC.","title":"Deleting volumes"},{"location":"content/en/concepts/storage/#copying-volumes","text":"TBD - this section is a work in progress... ramen volume-replication-operator volsync","title":"Copying volumes"},{"location":"content/en/concepts/storage/#disaster-recovery","text":"TBD - this section is a work in progress... Pairing locations as continuously replicating storage between each other. KCP would have to be able to map primary volumes to secondary volumes to failover workloads between locations.","title":"Disaster recovery"},{"location":"content/en/concepts/storage/#examples","text":"TBD - this section is a work in progress...","title":"Examples"},{"location":"content/en/concepts/storage/#shared-nfs-storage","text":"NFS server running in every location, external to the SyncTarget , but available over the network. Note that high-availability and data-protection of the storage itself is out of scope and would be handled by storage admin or provided by enterprise products. Workloads allow volumes with RWX access-mode. KCP picks one SyncTarget to be the provisioner and syncs up the volume information. After provisioning completes, sync down to any SyncTarget in the Location that the workload decides to be placed to allow moving transparently as needed when clusters become offline or drained. Once the PVC is deleted, the deletion of the volume itself is performed by one of the SyncTargets .","title":"Shared NFS storage"},{"location":"content/en/concepts/storage/#roadmap","text":"Moving owned volumes Fencing Copy-on-demand Copy-continuous DR-location-pairing and primary->secondary volume mapping Statefulsets COSI Bucket + BucketAccess","title":"Roadmap"},{"location":"content/en/concepts/syncer/","text":"In order to register a Kubernetes clusters with the kcp server, users have to install a special component named syncer . Requirements kcp server kcp kubectl plugin kubernetes cluster Instructions (Optional) Skip this step, if you already have a physical cluster. Create a kind cluster to back the sync target: ```sh $ kind create cluster Creating cluster \"kind\" ... Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind ``` !!! note This step sets current context to the new kind cluster. Make sure to use a KCP kubeconfig for the next steps unless told otherwise. Create an organisation and immediately enter it: sh $ kubectl kcp workspace create my-org --enter Workspace \"my-org\" (type root:organization) created. Waiting for it to be ready... Workspace \"my-org\" (type root:organization) is ready to use. Current workspace is \"root:my-org\" (type \"root:organization\"). Enable the syncer for a p-cluster: sh kubectl kcp workload sync <synctarget name> --syncer-image <image name> -o syncer.yaml Where <image name> one of the syncer images for your corresponding KCP release (e.g. ghcr.io/kcp-dev/kcp/syncer:v0.7.5 ). Apply the manifest to the p-cluster: sh $ KUBECONFIG=<pcluster-config> kubectl apply -f syncer.yaml namespace/kcp-syncer-kind-1owee1ci created serviceaccount/kcp-syncer-kind-1owee1ci created secret/kcp-syncer-kind-1owee1ci-token created clusterrole.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created clusterrolebinding.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created secret/kcp-syncer-kind-1owee1ci created deployment.apps/kcp-syncer-kind-1owee1ci created and it will create a kcp-syncer deployment: sh $ KUBECONFIG=<pcluster-config> kubectl -n kcp-syncer-kind-1owee1ci get deployments NAME READY UP-TO-DATE AVAILABLE AGE kcp-syncer 1/1 1 1 13m Wait for the kcp sync target to go ready: bash kubectl wait --for=condition=Ready synctarget/<mycluster> Select resources to sync. Syncer will by default use the kubernetes APIExport in root:compute workspace and sync deployments/services/ingresses to the physical cluster. The related API schemas of the physical cluster should be comptible with kubernetes 1.24. User can select to sync other resources in physical clusters or from other APIExports on kcp server. To sync resources that the KCP server does not have an APIExport to support yet, run kubectl kcp workload sync <mycluster> --syncer-image <image name> --resources foo.bar -o syncer.yaml And apply the generated manifests to the physical cluster. The syncer will then import the API schema of foo.bar to the workspace of the synctarget, following up with an auto generated kubernetes APIExport/APIBinding in the same workspace. You can then create foo.bar in this workspace, or create an APIBinding in another workspace to bind this APIExport. To sync resource from another existing APIExport in the KCP server, run kubectl kcp workload sync <mycluster> --syncer-image <image name> --apiexports another-workspace:another-apiexport -o syncer.yaml Syncer will start syncing the resources in this APIExport as long as the SyncTarget has compatible API schemas. To see if a certain resource is supported to be synced by the syncer, you can check the state of the syncedResources in SyncTarget status. Bind workspaces to the Location Workspace After the SyncTarget is ready, switch to any workspace containing some workloads that you want to sync to this SyncTarget , and run kubectl kcp bind compute <workspace of synctarget> This command will create a Placement in the workspace. By default, it will also create APIBinding s for global kubernetes APIExport and kubernetes APIExport in workspace of SyncTarget , if any of these APIExport s are supported by the SyncTarget . Alternatively, if you would like to bind other APIExport s which are supported by the SyncerTarget , run: kubectl kcp bind compute <workspace of synctarget> --apiexports <apiexport workspace>:<apiexport name> In addition, you can specify the certain location or namespace to create placement. e.g. kubectl kcp bind compute <workspace of synctarget> --location-selectors=env=test --namespace-selector=purpose=workload this command will create a Placement selecting a Location with label env=test and bind the selected Location to namespaces with label purpose=workload . See more details of placement and location here Running a workload Create a deployment: sh kubectl create deployment kuard --image gcr.io/kuar-demo/kuard-amd64:blue !!! note Replace \"gcr.io/kuar-demo/kuard-amd64:blue\" with \"gcr.io/kuar-demo/kuard-arm64:blue\" in case you're running an Apple M1 based virtual machine. Verify the deployment on the local workspace: sh $ kubectl rollout status deployment/kuard Waiting for deployment \"kuard\" rollout to finish: 0 of 1 updated replicas are available... deployment \"kuard\" successfully rolled out For syncer development Building components The syncer, kcp and kubectl plugins should come from the same build, so they are compatible with each other. To build, make the root kcp folder your current working directory and run: make build-all install build-kind-images If your go version is not 1.19, which is the expected version, you need to run IGNORE_GO_VERSION=1 make build-all install build-kind-images Make a note of the syncer image that is produced by this build, which is in the output near the end. It should be something like kind.local/syncer-c2e3073d5026a8f7f2c47a50c16bdbec:8287441974cf604dd93da5e6d010a78d38ae49733ea3a5031048a516101dd8a2 . This will be used by the kubectl kcp workload sync ... command, below. The kubectl kcp plugin binaries should be first in your path so kubectl picks them up. which kubectl-kcp should point to the kubectl-kcp you have just built in your $GOPATH/bin , and not any other installed version. Start kcp on another terminal kcp start Running in a kind cluster with a local registry You can run the syncer in a kind cluster for development. Create a kind cluster with a local registry to simplify syncer development by executing the following script: bash /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/kubernetes-sigs/kind/main/site/static/examples/kind-with-registry.sh)\" From the kcp root directory: bash export KUBECONFIG=.kcp/admin.kubeconfig Create a location workspace and immediately enter it: sh $ kubectl kcp workspace create my-locations --enter Workspace \"my-locations\" (type root:organization) created. Waiting for it to be ready... Workspace \"my-locations\" (type root:organization) is ready to use. Current workspace is \"root:my-locations\" (type \"root:organization\"). To create the synctarget and use the image pushed to the local registry, supply <image name> to the kcp workload sync plugin command, where <image name> was captured in the build steps, above. sh kubectl kcp workload sync kind --syncer-image <image name> -o syncer.yaml Create a second workspace for your workloads and immediately enter it: sh $ kubectl kcp workspace .. $ kubectl kcp workspace create my-workloads --enter Workspace \"my-workloads\" (type root:organization) created. Waiting for it to be ready... Workspace \"my-workloads\" (type root:organization) is ready to use. Current workspace is \"root:my-workloads\" (type \"root:organization\"). Bind it to the my-locations workspace with the synctarget: bash kubectl kcp bind compute \"root:my-locations\" Apply the manifest to the p-cluster: sh $ KUBECONFIG=<pcluster-config> kubectl apply -f syncer.yaml namespace/kcp-syncer-kind-1owee1ci created serviceaccount/kcp-syncer-kind-1owee1ci created secret/kcp-syncer-kind-1owee1ci-token created clusterrole.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created clusterrolebinding.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created secret/kcp-syncer-kind-1owee1ci created deployment.apps/kcp-syncer-kind-1owee1ci created and it will create a kcp-syncer deployment: sh $ KUBECONFIG=<pcluster-config> kubectl -n kcp-syncer-kind-1owee1ci get deployments NAME READY UP-TO-DATE AVAILABLE AGE kcp-syncer 1/1 1 1 13m Wait for the kcp sync target to go ready: bash kubectl wait --for=condition=Ready synctarget/<mycluster> Add a deployment to the my-workloads workspace and check the p-cluster to see if the workload has been created there: bash kubectl create deployment --image=gcr.io/kuar-demo/kuard-amd64:blue --port=8080 kuard Running locally TODO(m1kola): we need a less hacky way to run locally: needs to be more close to what we have when running inside the kind with own kubeconfig. This assumes that KCP is also being run locally. Create a kind cluster to back the sync target: ```sh $ kind create cluster Creating cluster \"kind\" ... Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind ``` Make sure to use kubeconfig for your local KCP: bash export KUBECONFIG=.kcp/admin.kubeconfig Create an organisation and immediately enter it: sh $ kubectl kcp workspace create my-org --enter Workspace \"my-org\" (type root:organization) created. Waiting for it to be ready... Workspace \"my-org\" (type root:organization) is ready to use. Current workspace is \"root:my-org\" (type \"root:organization\"). Enable the syncer for a p-cluster: sh kubectl kcp workload sync <mycluster> --syncer-image <image name> -o syncer.yaml <image name> can be anything here as it will only be used to generate syncer.yaml which we are not going to apply. Gather data required for the syncer: bash syncTargetName=<mycluster> syncTargetUID=$(kubectl get synctarget $syncTargetName -o jsonpath=\"{.metadata.uid}\") fromCluster=$(kubectl ws current --short) Run the following snippet: bash go run ./cmd/syncer \\ --from-kubeconfig=.kcp/admin.kubeconfig \\ --from-context=base \\ --to-kubeconfig=$HOME/.kube/config \\ --sync-target-name=$syncTargetName \\ --sync-target-uid=$syncTargetUID \\ --from-cluster=$fromCluster \\ --resources=configmaps \\ --resources=deployments.apps \\ --resources=secrets \\ --resources=serviceaccounts \\ --qps=30 \\ --burst=20 Wait for the kcp sync target to go ready: bash kubectl wait --for=condition=Ready synctarget/<mycluster>","title":"Registering Kubernetes Clusters using syncer"},{"location":"content/en/concepts/syncer/#requirements","text":"kcp server kcp kubectl plugin kubernetes cluster","title":"Requirements"},{"location":"content/en/concepts/syncer/#instructions","text":"(Optional) Skip this step, if you already have a physical cluster. Create a kind cluster to back the sync target: ```sh $ kind create cluster Creating cluster \"kind\" ... Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind ``` !!! note This step sets current context to the new kind cluster. Make sure to use a KCP kubeconfig for the next steps unless told otherwise. Create an organisation and immediately enter it: sh $ kubectl kcp workspace create my-org --enter Workspace \"my-org\" (type root:organization) created. Waiting for it to be ready... Workspace \"my-org\" (type root:organization) is ready to use. Current workspace is \"root:my-org\" (type \"root:organization\"). Enable the syncer for a p-cluster: sh kubectl kcp workload sync <synctarget name> --syncer-image <image name> -o syncer.yaml Where <image name> one of the syncer images for your corresponding KCP release (e.g. ghcr.io/kcp-dev/kcp/syncer:v0.7.5 ). Apply the manifest to the p-cluster: sh $ KUBECONFIG=<pcluster-config> kubectl apply -f syncer.yaml namespace/kcp-syncer-kind-1owee1ci created serviceaccount/kcp-syncer-kind-1owee1ci created secret/kcp-syncer-kind-1owee1ci-token created clusterrole.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created clusterrolebinding.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created secret/kcp-syncer-kind-1owee1ci created deployment.apps/kcp-syncer-kind-1owee1ci created and it will create a kcp-syncer deployment: sh $ KUBECONFIG=<pcluster-config> kubectl -n kcp-syncer-kind-1owee1ci get deployments NAME READY UP-TO-DATE AVAILABLE AGE kcp-syncer 1/1 1 1 13m Wait for the kcp sync target to go ready: bash kubectl wait --for=condition=Ready synctarget/<mycluster>","title":"Instructions"},{"location":"content/en/concepts/syncer/#select-resources-to-sync","text":"Syncer will by default use the kubernetes APIExport in root:compute workspace and sync deployments/services/ingresses to the physical cluster. The related API schemas of the physical cluster should be comptible with kubernetes 1.24. User can select to sync other resources in physical clusters or from other APIExports on kcp server. To sync resources that the KCP server does not have an APIExport to support yet, run kubectl kcp workload sync <mycluster> --syncer-image <image name> --resources foo.bar -o syncer.yaml And apply the generated manifests to the physical cluster. The syncer will then import the API schema of foo.bar to the workspace of the synctarget, following up with an auto generated kubernetes APIExport/APIBinding in the same workspace. You can then create foo.bar in this workspace, or create an APIBinding in another workspace to bind this APIExport. To sync resource from another existing APIExport in the KCP server, run kubectl kcp workload sync <mycluster> --syncer-image <image name> --apiexports another-workspace:another-apiexport -o syncer.yaml Syncer will start syncing the resources in this APIExport as long as the SyncTarget has compatible API schemas. To see if a certain resource is supported to be synced by the syncer, you can check the state of the syncedResources in SyncTarget status.","title":"Select resources to sync."},{"location":"content/en/concepts/syncer/#bind-workspaces-to-the-location-workspace","text":"After the SyncTarget is ready, switch to any workspace containing some workloads that you want to sync to this SyncTarget , and run kubectl kcp bind compute <workspace of synctarget> This command will create a Placement in the workspace. By default, it will also create APIBinding s for global kubernetes APIExport and kubernetes APIExport in workspace of SyncTarget , if any of these APIExport s are supported by the SyncTarget . Alternatively, if you would like to bind other APIExport s which are supported by the SyncerTarget , run: kubectl kcp bind compute <workspace of synctarget> --apiexports <apiexport workspace>:<apiexport name> In addition, you can specify the certain location or namespace to create placement. e.g. kubectl kcp bind compute <workspace of synctarget> --location-selectors=env=test --namespace-selector=purpose=workload this command will create a Placement selecting a Location with label env=test and bind the selected Location to namespaces with label purpose=workload . See more details of placement and location here","title":"Bind workspaces to the Location Workspace"},{"location":"content/en/concepts/syncer/#running-a-workload","text":"Create a deployment: sh kubectl create deployment kuard --image gcr.io/kuar-demo/kuard-amd64:blue !!! note Replace \"gcr.io/kuar-demo/kuard-amd64:blue\" with \"gcr.io/kuar-demo/kuard-arm64:blue\" in case you're running an Apple M1 based virtual machine. Verify the deployment on the local workspace: sh $ kubectl rollout status deployment/kuard Waiting for deployment \"kuard\" rollout to finish: 0 of 1 updated replicas are available... deployment \"kuard\" successfully rolled out","title":"Running a workload"},{"location":"content/en/concepts/syncer/#for-syncer-development","text":"","title":"For syncer development"},{"location":"content/en/concepts/syncer/#building-components","text":"The syncer, kcp and kubectl plugins should come from the same build, so they are compatible with each other. To build, make the root kcp folder your current working directory and run: make build-all install build-kind-images If your go version is not 1.19, which is the expected version, you need to run IGNORE_GO_VERSION=1 make build-all install build-kind-images Make a note of the syncer image that is produced by this build, which is in the output near the end. It should be something like kind.local/syncer-c2e3073d5026a8f7f2c47a50c16bdbec:8287441974cf604dd93da5e6d010a78d38ae49733ea3a5031048a516101dd8a2 . This will be used by the kubectl kcp workload sync ... command, below. The kubectl kcp plugin binaries should be first in your path so kubectl picks them up. which kubectl-kcp should point to the kubectl-kcp you have just built in your $GOPATH/bin , and not any other installed version.","title":"Building components"},{"location":"content/en/concepts/syncer/#start-kcp-on-another-terminal","text":"kcp start","title":"Start kcp on another terminal"},{"location":"content/en/concepts/syncer/#running-in-a-kind-cluster-with-a-local-registry","text":"You can run the syncer in a kind cluster for development. Create a kind cluster with a local registry to simplify syncer development by executing the following script: bash /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/kubernetes-sigs/kind/main/site/static/examples/kind-with-registry.sh)\" From the kcp root directory: bash export KUBECONFIG=.kcp/admin.kubeconfig Create a location workspace and immediately enter it: sh $ kubectl kcp workspace create my-locations --enter Workspace \"my-locations\" (type root:organization) created. Waiting for it to be ready... Workspace \"my-locations\" (type root:organization) is ready to use. Current workspace is \"root:my-locations\" (type \"root:organization\"). To create the synctarget and use the image pushed to the local registry, supply <image name> to the kcp workload sync plugin command, where <image name> was captured in the build steps, above. sh kubectl kcp workload sync kind --syncer-image <image name> -o syncer.yaml Create a second workspace for your workloads and immediately enter it: sh $ kubectl kcp workspace .. $ kubectl kcp workspace create my-workloads --enter Workspace \"my-workloads\" (type root:organization) created. Waiting for it to be ready... Workspace \"my-workloads\" (type root:organization) is ready to use. Current workspace is \"root:my-workloads\" (type \"root:organization\"). Bind it to the my-locations workspace with the synctarget: bash kubectl kcp bind compute \"root:my-locations\" Apply the manifest to the p-cluster: sh $ KUBECONFIG=<pcluster-config> kubectl apply -f syncer.yaml namespace/kcp-syncer-kind-1owee1ci created serviceaccount/kcp-syncer-kind-1owee1ci created secret/kcp-syncer-kind-1owee1ci-token created clusterrole.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created clusterrolebinding.rbac.authorization.k8s.io/kcp-syncer-kind-1owee1ci created secret/kcp-syncer-kind-1owee1ci created deployment.apps/kcp-syncer-kind-1owee1ci created and it will create a kcp-syncer deployment: sh $ KUBECONFIG=<pcluster-config> kubectl -n kcp-syncer-kind-1owee1ci get deployments NAME READY UP-TO-DATE AVAILABLE AGE kcp-syncer 1/1 1 1 13m Wait for the kcp sync target to go ready: bash kubectl wait --for=condition=Ready synctarget/<mycluster> Add a deployment to the my-workloads workspace and check the p-cluster to see if the workload has been created there: bash kubectl create deployment --image=gcr.io/kuar-demo/kuard-amd64:blue --port=8080 kuard","title":"Running in a kind cluster with a local registry"},{"location":"content/en/concepts/syncer/#running-locally","text":"TODO(m1kola): we need a less hacky way to run locally: needs to be more close to what we have when running inside the kind with own kubeconfig. This assumes that KCP is also being run locally. Create a kind cluster to back the sync target: ```sh $ kind create cluster Creating cluster \"kind\" ... Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind ``` Make sure to use kubeconfig for your local KCP: bash export KUBECONFIG=.kcp/admin.kubeconfig Create an organisation and immediately enter it: sh $ kubectl kcp workspace create my-org --enter Workspace \"my-org\" (type root:organization) created. Waiting for it to be ready... Workspace \"my-org\" (type root:organization) is ready to use. Current workspace is \"root:my-org\" (type \"root:organization\"). Enable the syncer for a p-cluster: sh kubectl kcp workload sync <mycluster> --syncer-image <image name> -o syncer.yaml <image name> can be anything here as it will only be used to generate syncer.yaml which we are not going to apply. Gather data required for the syncer: bash syncTargetName=<mycluster> syncTargetUID=$(kubectl get synctarget $syncTargetName -o jsonpath=\"{.metadata.uid}\") fromCluster=$(kubectl ws current --short) Run the following snippet: bash go run ./cmd/syncer \\ --from-kubeconfig=.kcp/admin.kubeconfig \\ --from-context=base \\ --to-kubeconfig=$HOME/.kube/config \\ --sync-target-name=$syncTargetName \\ --sync-target-uid=$syncTargetUID \\ --from-cluster=$fromCluster \\ --resources=configmaps \\ --resources=deployments.apps \\ --resources=secrets \\ --resources=serviceaccounts \\ --qps=30 \\ --burst=20 Wait for the kcp sync target to go ready: bash kubectl wait --for=condition=Ready synctarget/<mycluster>","title":"Running locally"},{"location":"content/en/concepts/virtual-workspaces/","text":"Virtual workspaces are proxy-like apiservers under a custom URL that provide some computed view of real workspaces. Examples when the user does kubectl get workspaces only workspaces are shown that the user has access to. controllers should not be able to directly access customer workspaces. They should only be able to access the objects that are connected to their provided APIs. In April 19's community call this virtual workspace was showcased , developed during v0.4 phase. if we keep the initializer model with WorkspaceType , there must be a virtual workspace for the \"workspace type owner\" that gives access to initializing workspaces. the syncer will get a virtual workspace view of the workspaces it syncs to physical clusters. That view will have transformed objects potentially, especially deployment-splitter-like transformations will be implemented within a virtual workspace, transparently applied from the point of view of the syncer. FAQ Can we use go clients to watch resources on a virtual workspace? Absolutely. From the point of view of the controllers it is just a normal (client) URL. So one can use client-go informers (or controller-runtime) to watch the objects in a virtual workspace. !!! note A normal service account lives in just ONE workspace and can only access its own workspace. So in order to use a service account for accessing cross-workspace data (and that's what is necessary in example 2 and 3 at least), we need a virtual workspace to add the necessary authz. Are virtual workspaces read-only? No, they are not necessarily. Some are, some are not. The controller view virtual workspace will be writable, as well as the syncer virtual workspace. Do service teams have to write their own virtual workspace? Not for the standard cases as described above. There might be cases in the future where service teams provide their own virtual workspace for some very special purpose access patterns. But we are not there yet. Where does the developer get the URL from of the virtual workspace? The URLs will be \"published\" in some object status. E.g. APIExport.status will have a list of URLs that controllers have to connect to (example 2). Similarly, SyncTarget.status will have URLs for the syncer virtual workspaces, etc. We might do the same in WorkspaceType.status (example 3). Will there be multiple virtual workspace URLs my controller has to watch? Yes, as soon as we add sharding, it will become a list. So it might be that 1000 tenants are accessible under one URL, the next 1000 under another one, and so on. The controllers have to watch the mentioned URL lists in status of objects and start new instances (either with their own controller sharding eventually, or just in process with another go routine). Show me the code. The stock kcp virtual workspaces are in pkg/virtual . Who runs the virtual workspaces? The stock kcp virtual workspaces will be run through kcp start in-process. The personal workspace one (example 1) can also be run as its own process and the kcp apiserver will forward traffic to the external address. There might be reasons in the future like scalability that the later model is preferred. For the clients of virtual workspaces that has no impact. They are supposed to \"blindly\" use the URLs published in the API objects' status. Those URLs might point to in-process instances or external addresses depending on deployment topology.","title":"Virtual Workspaces"},{"location":"content/en/concepts/virtual-workspaces/#examples","text":"when the user does kubectl get workspaces only workspaces are shown that the user has access to. controllers should not be able to directly access customer workspaces. They should only be able to access the objects that are connected to their provided APIs. In April 19's community call this virtual workspace was showcased , developed during v0.4 phase. if we keep the initializer model with WorkspaceType , there must be a virtual workspace for the \"workspace type owner\" that gives access to initializing workspaces. the syncer will get a virtual workspace view of the workspaces it syncs to physical clusters. That view will have transformed objects potentially, especially deployment-splitter-like transformations will be implemented within a virtual workspace, transparently applied from the point of view of the syncer.","title":"Examples"},{"location":"content/en/concepts/virtual-workspaces/#faq","text":"Can we use go clients to watch resources on a virtual workspace? Absolutely. From the point of view of the controllers it is just a normal (client) URL. So one can use client-go informers (or controller-runtime) to watch the objects in a virtual workspace. !!! note A normal service account lives in just ONE workspace and can only access its own workspace. So in order to use a service account for accessing cross-workspace data (and that's what is necessary in example 2 and 3 at least), we need a virtual workspace to add the necessary authz. Are virtual workspaces read-only? No, they are not necessarily. Some are, some are not. The controller view virtual workspace will be writable, as well as the syncer virtual workspace. Do service teams have to write their own virtual workspace? Not for the standard cases as described above. There might be cases in the future where service teams provide their own virtual workspace for some very special purpose access patterns. But we are not there yet. Where does the developer get the URL from of the virtual workspace? The URLs will be \"published\" in some object status. E.g. APIExport.status will have a list of URLs that controllers have to connect to (example 2). Similarly, SyncTarget.status will have URLs for the syncer virtual workspaces, etc. We might do the same in WorkspaceType.status (example 3). Will there be multiple virtual workspace URLs my controller has to watch? Yes, as soon as we add sharding, it will become a list. So it might be that 1000 tenants are accessible under one URL, the next 1000 under another one, and so on. The controllers have to watch the mentioned URL lists in status of objects and start new instances (either with their own controller sharding eventually, or just in process with another go routine). Show me the code. The stock kcp virtual workspaces are in pkg/virtual . Who runs the virtual workspaces? The stock kcp virtual workspaces will be run through kcp start in-process. The personal workspace one (example 1) can also be run as its own process and the kcp apiserver will forward traffic to the external address. There might be reasons in the future like scalability that the later model is preferred. For the clients of virtual workspaces that has no impact. They are supposed to \"blindly\" use the URLs published in the API objects' status. Those URLs might point to in-process instances or external addresses depending on deployment topology.","title":"FAQ"},{"location":"content/en/concepts/workspaces/","text":"Multi-tenancy is implemented through workspaces. A workspace is a Kubernetes-cluster-like HTTPS endpoint, i.e. an endpoint usual Kubernetes client tooling (client-go, controller-runtime and others) and user interfaces (kubectl, helm, web console, ...) can talk to like to a Kubernetes cluster. Workspaces can be backed by a traditional REST store implementation through CRDs or native resources persisted in etcd. But there can be alternative implementations for special access patterns, e.g. a virtual workspace apiserver that transforms other APIs e.g. by projections (Workspace in kcp is a projection of ClusterWorkspace) or by applying visibility filters (e.g. showing all workspaces or all namespaces the current user has access to). Workspaces are represented to the user via the Workspace kind, e.g. kind: Workspace apiVersion: tenancy.kcp.io/v1alpha1 spec: type: Universal status: url: https://kcp.example.com/clusters/myapp There are different types of workspaces, and workspaces are arranged in a tree. Each type of workspace may restrict the types of its children and may restrict the types it may be a child of; a parent-child relationship is allowed if and only if the parent allows the child and the child allows the parent. The kcp binary has a built-in set of workspace types, and the admin may create objects that define additional types. Root Workspace is a singleton. It holds some data that applies to all workspaces, such as the set of defined workspace types (objects of type WorkspaceType ). HomeRoot Workspace is normally a singleton, holding the branch of workspaces that contains the user home workspaces as descendants. Can only be a child of the root workspace, and can only have HomeBucket children. HomeBucket Workspace are intermediate vertices in the hierarhcy between the HomeRoot and the user home workspaces. Can be a child of the root or another HomeBucket workspace. Allowed children are home and HomeBucket workspaces. Home Workspace is a user's home workspace. These hold user resources such as applications with services, secrets, configmaps, deployments, etc. Can only be a child of a HomeBucket workspace. Organization Workspace are workspaces holding organizational data, e.g. definitions of user workspaces, roles, policies, accounting data. Can only be a child of root. Team Workspace can only be a child of an Organization workspace. Universal Workspace is a basic type of workspace with no particular nature. Has no restrictions on parent or child workspace types. ClusterWorkspaces ClusterWorkspaces define traditional etcd-based, CRD enabled workspaces, available under /clusters/<parent-workspace-name>:<cluster-workspace-name> . E.g. organization workspaces are accessible at /clusters/root:<org-name> . A user workspace is accessible at /clusters/root:users:<bucket-d1>:..:<bucket-dN>:<user-workspace-name> . ClusterWorkspaces have a type. A type is defined by a WorkspaceType. A type defines initializers. They are set on new ClusterWorkspace objects and block the cluster workspace from leaving the initializing phase. Both system components and 3rd party components can use initializers to customize ClusterWorkspaces on creation, e.g. to bootstrap resources inside the workspace, or to set up permission in its parent. A cluster workspace of type Universal is a workspace without further initialization or special properties by default, and it can be used without a corresponding WorkspaceType object (though one can be added and its initializers will be applied). ClusterWorkSpaces of type Organization are described in the next section. !!! note In order to create cluster workspaces of a given type (including Universal ) you must have use permissions against the workspacetypes resources with the lower-case name of the cluster workspace type (e.g. universal ). All system:authenticated users inherit this permission automatically for type Universal . ClusterWorkspaces persisted in etcd on a shard have disjoint etcd prefix ranges, i.e. they have independent behaviour and no cluster workspace sees objects from other cluster workspaces. In contrast to namespace in Kubernetes, this includes non-namespaced objects, e.g. like CRDs where each workspace can have its own set of CRDs installed. User Home Workspaces User home workspaces are an optional feature of kcp. If enabled (through --enable-home-workspaces ), there is a special virtual Workspace called ~ in the root workspace. It is used by kubectl ws to derive the full path to the user home workspace, similar to how Unix cd ~ move the users to their home. The full path for a user's home workspace has a number of parts: <prefix>(:<bucket>)+:<user-name> . Buckets are used to ensure that at most ~1000 sub-buckets or users exist in any bucket, for scaling reasons. The bucket names are deterministically derived from the user name (via some hash). Example for user adam when using default configuration: root:users:a8:f1:adam . User home workspaces are created on-demand when they are first accessed, but this is not visible to the user, allowing the system to only incur the cost of these workspaces when they are needed. Only users of the configured home-creator-groups (default system:authenticated ) will have a home workspace. Bucket configuration options The kcp administrator can configure: <prefix> , which defaults to root:users bucket depth, which defaults to 2 bucket name length, in characters, which defaults to 2 The following outlines valid configuration options. With the default setup, ~5 users or ~700 sub-buckets will be in any bucket. !!! warning DO NOT set the bucket size to be longer than 2, as this will adversely impact performance. User-names have (26 * [(26 + 10 + 2) * 61] * 36 = 2169648) permutations, and buckets are made up of lowercase-alpha chars. Invalid configurations break the scale limit in sub-buckets or users. Valid configurations should target having not more than ~1000 sub-buckets per bucket and at least 5 users per bucket. Valid Configurations length depth sub-buckets users 1 3 26 * 1 = 26 2169648 / (26)^3 = 124 1 4 26 * 1 = 26 2169648 / (26)^4 = 5 2 2 26 * 26 = 676 2169648 / (26*26)^2 = 5 Invalid Configurations These are examples of invalid configurations and are for illustrative purposes only. In nearly all cases, the default values will be sufficient. length depth sub-buckets users 1 1 26 * 1 = 26 2169648 / (26) = 83448 1 2 26 * 1 = 26 2169648 / (26)^2 = 3209 2 1 26 * 26 = 676 2169648 / (26*26) = 3209 2 3 26 * 26 = 676 2169648 / (26*26)^3 = .007 3 1 26 26 26 = 17576 2169648 / (26 26 26) = 124 3 2 26 26 26 = 17576 2169648 / (26 26 26)^2 = .007 Organization Workspaces Organization workspaces are ClusterWorkspaces of type Organization , defined in the root workspace. Organization workspaces are accessible at /clusters/root:<org-name> . !!! note The organization WorkspaceType can only be created in the root workspace verified through admission. Organization workspaces have standard resources (on-top of Universal workspaces) which include the ClusterWorkspace API defined through an CRD deployed during organization workspace initialization. Root Workspace The root workspace is a singleton in the system accessible under /clusters/root . It is not represented by a ClusterWorkspace anywhere, but shares the same properties. Inside the root workspace at least the following resources are bootstrapped on kcp startup: ClusterWorkspace CRD WorkspaceShard CRD Cluster CRD. The root workspace is the only one that holds WorkspaceShard objects. WorkspaceShards are used to schedule a new ClusterWorkspace to, i.e. to select in which etcd the cluster workspace content is to be persisted. System Workspaces System workspaces are local to a shard and are named in the pattern system:<system-workspace-name> . They are only accessible to a shard-local admin user and there is neither a definition via a ClusterWorkspace nor any per-request check for workspace existence. System workspace are only accessible to a shard-local admin user, and there is neither a definition via a ClusterWorkspace, nor is there any validation of requests that the system workspace exists. The system:admin system workspace is special as it is also accessible through / of the shard, and at /cluster/system:admin at the same time.","title":"Workspaces"},{"location":"content/en/concepts/workspaces/#clusterworkspaces","text":"ClusterWorkspaces define traditional etcd-based, CRD enabled workspaces, available under /clusters/<parent-workspace-name>:<cluster-workspace-name> . E.g. organization workspaces are accessible at /clusters/root:<org-name> . A user workspace is accessible at /clusters/root:users:<bucket-d1>:..:<bucket-dN>:<user-workspace-name> . ClusterWorkspaces have a type. A type is defined by a WorkspaceType. A type defines initializers. They are set on new ClusterWorkspace objects and block the cluster workspace from leaving the initializing phase. Both system components and 3rd party components can use initializers to customize ClusterWorkspaces on creation, e.g. to bootstrap resources inside the workspace, or to set up permission in its parent. A cluster workspace of type Universal is a workspace without further initialization or special properties by default, and it can be used without a corresponding WorkspaceType object (though one can be added and its initializers will be applied). ClusterWorkSpaces of type Organization are described in the next section. !!! note In order to create cluster workspaces of a given type (including Universal ) you must have use permissions against the workspacetypes resources with the lower-case name of the cluster workspace type (e.g. universal ). All system:authenticated users inherit this permission automatically for type Universal . ClusterWorkspaces persisted in etcd on a shard have disjoint etcd prefix ranges, i.e. they have independent behaviour and no cluster workspace sees objects from other cluster workspaces. In contrast to namespace in Kubernetes, this includes non-namespaced objects, e.g. like CRDs where each workspace can have its own set of CRDs installed.","title":"ClusterWorkspaces"},{"location":"content/en/concepts/workspaces/#user-home-workspaces","text":"User home workspaces are an optional feature of kcp. If enabled (through --enable-home-workspaces ), there is a special virtual Workspace called ~ in the root workspace. It is used by kubectl ws to derive the full path to the user home workspace, similar to how Unix cd ~ move the users to their home. The full path for a user's home workspace has a number of parts: <prefix>(:<bucket>)+:<user-name> . Buckets are used to ensure that at most ~1000 sub-buckets or users exist in any bucket, for scaling reasons. The bucket names are deterministically derived from the user name (via some hash). Example for user adam when using default configuration: root:users:a8:f1:adam . User home workspaces are created on-demand when they are first accessed, but this is not visible to the user, allowing the system to only incur the cost of these workspaces when they are needed. Only users of the configured home-creator-groups (default system:authenticated ) will have a home workspace.","title":"User Home Workspaces"},{"location":"content/en/concepts/workspaces/#bucket-configuration-options","text":"The kcp administrator can configure: <prefix> , which defaults to root:users bucket depth, which defaults to 2 bucket name length, in characters, which defaults to 2 The following outlines valid configuration options. With the default setup, ~5 users or ~700 sub-buckets will be in any bucket. !!! warning DO NOT set the bucket size to be longer than 2, as this will adversely impact performance. User-names have (26 * [(26 + 10 + 2) * 61] * 36 = 2169648) permutations, and buckets are made up of lowercase-alpha chars. Invalid configurations break the scale limit in sub-buckets or users. Valid configurations should target having not more than ~1000 sub-buckets per bucket and at least 5 users per bucket.","title":"Bucket configuration options"},{"location":"content/en/concepts/workspaces/#valid-configurations","text":"length depth sub-buckets users 1 3 26 * 1 = 26 2169648 / (26)^3 = 124 1 4 26 * 1 = 26 2169648 / (26)^4 = 5 2 2 26 * 26 = 676 2169648 / (26*26)^2 = 5","title":"Valid Configurations"},{"location":"content/en/concepts/workspaces/#invalid-configurations","text":"These are examples of invalid configurations and are for illustrative purposes only. In nearly all cases, the default values will be sufficient. length depth sub-buckets users 1 1 26 * 1 = 26 2169648 / (26) = 83448 1 2 26 * 1 = 26 2169648 / (26)^2 = 3209 2 1 26 * 26 = 676 2169648 / (26*26) = 3209 2 3 26 * 26 = 676 2169648 / (26*26)^3 = .007 3 1 26 26 26 = 17576 2169648 / (26 26 26) = 124 3 2 26 26 26 = 17576 2169648 / (26 26 26)^2 = .007","title":"Invalid Configurations"},{"location":"content/en/concepts/workspaces/#organization-workspaces","text":"Organization workspaces are ClusterWorkspaces of type Organization , defined in the root workspace. Organization workspaces are accessible at /clusters/root:<org-name> . !!! note The organization WorkspaceType can only be created in the root workspace verified through admission. Organization workspaces have standard resources (on-top of Universal workspaces) which include the ClusterWorkspace API defined through an CRD deployed during organization workspace initialization.","title":"Organization Workspaces"},{"location":"content/en/concepts/workspaces/#root-workspace","text":"The root workspace is a singleton in the system accessible under /clusters/root . It is not represented by a ClusterWorkspace anywhere, but shares the same properties. Inside the root workspace at least the following resources are bootstrapped on kcp startup: ClusterWorkspace CRD WorkspaceShard CRD Cluster CRD. The root workspace is the only one that holds WorkspaceShard objects. WorkspaceShards are used to schedule a new ClusterWorkspace to, i.e. to select in which etcd the cluster workspace content is to be persisted.","title":"Root Workspace"},{"location":"content/en/concepts/workspaces/#system-workspaces","text":"System workspaces are local to a shard and are named in the pattern system:<system-workspace-name> . They are only accessible to a shard-local admin user and there is neither a definition via a ClusterWorkspace nor any per-request check for workspace existence. System workspace are only accessible to a shard-local admin user, and there is neither a definition via a ClusterWorkspace, nor is there any validation of requests that the system workspace exists. The system:admin system workspace is special as it is also accessible through / of the shard, and at /cluster/system:admin at the same time.","title":"System Workspaces"},{"location":"content/en/concepts/developers/","text":"","title":"Developers"},{"location":"content/en/concepts/developers/controllers/","text":"Keys for objects in listers/indexers When you need to get an object from a kcp-aware lister or an indexer, you can't just pass the object's name to the Get() function, like you do with a typical controller targeting Kubernetes. Projects using kcp's copy of client-go are using a modified key function. Here are what keys look like for an object foo for both cluster-scoped and namespace-scoped varieties: Organization Workspace Logical Cluster Namespace Key - - root - root - - root default default/root root my-org root:my-org - root:my-org root my-org root:my-org default default/root:my-org my-org my-workspace my-org:my-workspace - my-org:my-workspace my-org my-workspace my-org:my-workspace default default/my-org:my-workspace Encoding/decoding keys Use the github.com/kcp-dev/apimachinery/pkg/cache package to encode and decode keys.","title":"Writing kcp-aware controllers"},{"location":"content/en/concepts/developers/controllers/#keys-for-objects-in-listersindexers","text":"When you need to get an object from a kcp-aware lister or an indexer, you can't just pass the object's name to the Get() function, like you do with a typical controller targeting Kubernetes. Projects using kcp's copy of client-go are using a modified key function. Here are what keys look like for an object foo for both cluster-scoped and namespace-scoped varieties: Organization Workspace Logical Cluster Namespace Key - - root - root - - root default default/root root my-org root:my-org - root:my-org root my-org root:my-org default default/root:my-org my-org my-workspace my-org:my-workspace - my-org:my-workspace my-org my-workspace my-org:my-workspace default default/my-org:my-workspace","title":"Keys for objects in listers/indexers"},{"location":"content/en/concepts/developers/controllers/#encodingdecoding-keys","text":"Use the github.com/kcp-dev/apimachinery/pkg/cache package to encode and decode keys.","title":"Encoding/decoding keys"},{"location":"content/en/concepts/developers/library-usage/","text":"Instead of running the kcp as a binary using go run , you can include the kcp api-server in your own projects. To create and start the api-server with the default options (including an embedded etcd server): options, err := serveroptions.NewOptions().Complete() if err != nil { panic(err) } cfg, err := server.NewConfig(options) if err != nil { panic(err) } completedCfg, err := cfg.Complete() if err != nil { panic(err) } srv, err := server.NewServer(completedCfg) if err != nil { panic(err) } // Run() will block until the apiserver stops or an error occurs. if err := srv.Run(ctx); err != nil { panic(err) } You may also configure post-start hooks which are useful if you need to start a some process that depends on a connection to the newly created api-server such as a controller manager. // Create a new api-server with default options options, err := serveroptions.NewOptions().Complete() if err != nil { panic(err) } cfg, err := server.NewConfig(options) if err != nil { panic(err) } completedCfg, err := cfg.Complete() if err != nil { panic(err) } srv, err := server.NewServer(completedCfg) if err != nil { panic(err) } // Register a post-start hook that connects to the api-server srv.AddPostStartHook(\"connect-to-api\", func(ctx genericapiserver.PostStartHookContext) error { // Create a new client using the client config from our newly created api-server client := clientset.NewForConfigOrDie(ctx.LoopbackClientConfig) _, err := client.Discovery().ServerGroups() return err }) // Start the api-server if err := srv.Run(ctx); err != nil { panic(err) }","title":"Using kcp as a library"},{"location":"content/en/concepts/developers/releasing/","text":"!!! note You currently need write access to the kcp-dev/kcp repository to perform these tasks. You also need an available team member with approval permissions from <https://github.com/openshift/release/blob/master/ci-operator/config/kcp-dev/kcp/OWNERS>. Create git tags Prerequisite - make sure you have a GPG signing key https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key https://docs.github.com/en/authentication/managing-commit-signature-verification/adding-a-gpg-key-to-your-github-account https://docs.github.com/en/authentication/managing-commit-signature-verification/telling-git-about-your-signing-key Create the tags kcp has 2 go modules, and a unique tag is needed for each module every time we create a new release. git fetch from the main kcp repository (kcp-dev/kcp) to ensure you have the latest commits Tag the main module If your git remote for kcp-dev/kcp is named something other than upstream , change REF accordingly If you are creating a release from a release branch, change main in REF accordingly, or you can make REF a commit hash. shell REF=upstream/main TAG=v1.2.3 git tag --sign --message \"$TAG\" \"$TAG\" \"$REF\" Tag the pkg/apis module, following the same logic as above for REF and TAG shell REF=upstream/main TAG=v1.2.3 git tag --sign --message \"pkg/apis/$TAG\" \"pkg/apis/$TAG\" \"$REF\" Push the tags REMOTE=upstream TAG=v1.2.3 git push \"$REMOTE\" \"$TAG\" \"pkg/apis/$TAG\" If it's a new minor version If this is the first release of a new minor version (e.g. the last release was v0.7.x, and you are releasing the first 0.8.x version), follow the following steps. Otherwise, you can skip to Review/edit/publish the release in GitHub Create a release branch Set REMOTE , REF , and VERSION as appropriate. REMOTE=upstream REF=\"$REMOTE/main\" VERSION=1.2 git checkout -b \"release-$VERSION\" \"$REF\" git push \"$REMOTE\" \"release-$VERSION\" Configure prow for the new release branch Make sure you have openshift/release cloned Create a new branch Copy ci-operator/config/kcp-dev/kcp/kcp-dev-kcp-main.yaml to ci-operator/config/kcp-dev/kcp/kcp-dev-kcp-release-<version>.yaml Edit the new file Change main to the name of the release branch, such as release-0.8 yaml zz_generated_metadata: branch: main Change latest to the name of the release branch yaml promotion: namespace: kcp tag: latest tag_by_commit: true Edit core-services/prow/02_config/kcp-dev/kcp/_prowconfig.yaml Copy the main branch configuration to a new release-x.y entry Run make update Add the new/updated files and commit your changes Push your branch to your fork Open a pull request Wait for it to be reviewed and merged Update testgrid Make sure you have a clone of kubernetes/test-infra Edit config/testgrids/kcp/kcp.yaml In the test_groups section: Copy all the entries under # main to the bottom of the map Rename -main- to -release-<version>- In the dashboard_groups section: Add a new entry under dashboard_names for kcp-release-<version> In the dashboards section: Copy the kcp-main entry, including dashboard_tab and all its entries, to a new entry called kcp-release-<version> Rename main to release-<version> in the new entry Commit your changes Push your branch to your fork Open a pull request Wait for it to be reviewed and merged Review/edit/publish the release in GitHub The goreleaser workflow automatically creates a draft GitHub release for each tag. Navigate to the draft release for the tag you just pushed. You'll be able to find it under the releases page. If the release notes have been pre-populated, delete them. For the \"previous tag,\" select the most recent, appropriate tag as the starting point If this is a new minor release (e.g. v0.8.0), select the initial version of the previous minor release (e.g. v0.7.0) If this is a patch release (e.g. v0.8.7), select the previous patch release (e.g. v0.8.6) Click \"Generate release notes\" Publish the release Notify Create an email addressed to kcp-dev@googlegroups.com and kcp-users@googlegroups.com Subject: [release] <version> e.g. [release] v0.8.0 In the body, include noteworthy changes Provide a link to the release in GitHub for the full release notes Post a message in the #kcp-dev Slack channel","title":"Publishing a new kcp release"},{"location":"content/en/concepts/developers/releasing/#create-git-tags","text":"","title":"Create git tags"},{"location":"content/en/concepts/developers/releasing/#prerequisite-make-sure-you-have-a-gpg-signing-key","text":"https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key https://docs.github.com/en/authentication/managing-commit-signature-verification/adding-a-gpg-key-to-your-github-account https://docs.github.com/en/authentication/managing-commit-signature-verification/telling-git-about-your-signing-key","title":"Prerequisite - make sure you have a GPG signing key"},{"location":"content/en/concepts/developers/releasing/#create-the-tags","text":"kcp has 2 go modules, and a unique tag is needed for each module every time we create a new release. git fetch from the main kcp repository (kcp-dev/kcp) to ensure you have the latest commits Tag the main module If your git remote for kcp-dev/kcp is named something other than upstream , change REF accordingly If you are creating a release from a release branch, change main in REF accordingly, or you can make REF a commit hash. shell REF=upstream/main TAG=v1.2.3 git tag --sign --message \"$TAG\" \"$TAG\" \"$REF\" Tag the pkg/apis module, following the same logic as above for REF and TAG shell REF=upstream/main TAG=v1.2.3 git tag --sign --message \"pkg/apis/$TAG\" \"pkg/apis/$TAG\" \"$REF\"","title":"Create the tags"},{"location":"content/en/concepts/developers/releasing/#push-the-tags","text":"REMOTE=upstream TAG=v1.2.3 git push \"$REMOTE\" \"$TAG\" \"pkg/apis/$TAG\"","title":"Push the tags"},{"location":"content/en/concepts/developers/releasing/#if-its-a-new-minor-version","text":"If this is the first release of a new minor version (e.g. the last release was v0.7.x, and you are releasing the first 0.8.x version), follow the following steps. Otherwise, you can skip to Review/edit/publish the release in GitHub","title":"If it's a new minor version"},{"location":"content/en/concepts/developers/releasing/#create-a-release-branch","text":"Set REMOTE , REF , and VERSION as appropriate. REMOTE=upstream REF=\"$REMOTE/main\" VERSION=1.2 git checkout -b \"release-$VERSION\" \"$REF\" git push \"$REMOTE\" \"release-$VERSION\"","title":"Create a release branch"},{"location":"content/en/concepts/developers/releasing/#configure-prow-for-the-new-release-branch","text":"Make sure you have openshift/release cloned Create a new branch Copy ci-operator/config/kcp-dev/kcp/kcp-dev-kcp-main.yaml to ci-operator/config/kcp-dev/kcp/kcp-dev-kcp-release-<version>.yaml Edit the new file Change main to the name of the release branch, such as release-0.8 yaml zz_generated_metadata: branch: main Change latest to the name of the release branch yaml promotion: namespace: kcp tag: latest tag_by_commit: true Edit core-services/prow/02_config/kcp-dev/kcp/_prowconfig.yaml Copy the main branch configuration to a new release-x.y entry Run make update Add the new/updated files and commit your changes Push your branch to your fork Open a pull request Wait for it to be reviewed and merged","title":"Configure prow for the new release branch"},{"location":"content/en/concepts/developers/releasing/#update-testgrid","text":"Make sure you have a clone of kubernetes/test-infra Edit config/testgrids/kcp/kcp.yaml In the test_groups section: Copy all the entries under # main to the bottom of the map Rename -main- to -release-<version>- In the dashboard_groups section: Add a new entry under dashboard_names for kcp-release-<version> In the dashboards section: Copy the kcp-main entry, including dashboard_tab and all its entries, to a new entry called kcp-release-<version> Rename main to release-<version> in the new entry Commit your changes Push your branch to your fork Open a pull request Wait for it to be reviewed and merged","title":"Update testgrid"},{"location":"content/en/concepts/developers/releasing/#revieweditpublish-the-release-in-github","text":"The goreleaser workflow automatically creates a draft GitHub release for each tag. Navigate to the draft release for the tag you just pushed. You'll be able to find it under the releases page. If the release notes have been pre-populated, delete them. For the \"previous tag,\" select the most recent, appropriate tag as the starting point If this is a new minor release (e.g. v0.8.0), select the initial version of the previous minor release (e.g. v0.7.0) If this is a patch release (e.g. v0.8.7), select the previous patch release (e.g. v0.8.6) Click \"Generate release notes\" Publish the release","title":"Review/edit/publish the release in GitHub"},{"location":"content/en/concepts/developers/releasing/#notify","text":"Create an email addressed to kcp-dev@googlegroups.com and kcp-users@googlegroups.com Subject: [release] <version> e.g. [release] v0.8.0 In the body, include noteworthy changes Provide a link to the release in GitHub for the full release notes Post a message in the #kcp-dev Slack channel","title":"Notify"},{"location":"content/en/concepts/developers/replicate-new-resource/","text":"As of today adding a new resource for replication is a manual process that consists of the following steps: You need to register a new CRD in the cache server. Registration is required otherwise the cache server won\u2019t be able to serve the new resource. It boils down to adding a new entry into an array . If you don\u2019t have a CRD definition file for your type, you can use the crdpuller against any kube-apiserver to create the required manifest. Next, you need to register the new type in the replication controller. For that you need to add a new entry into the following array In general there are two types of resources. The ones that are replicated automatically (i.e. APIExports) and the ones that are subject to additional filtering, for example require some additional annotation (i.e. ClusterRoles). For optional resources we usually create a separate controller which simply annotates objects that need to be replicated. Then during registration we provide a filtering function that checks for existence of the annotation (i.e. filtering function for CR )","title":"Adding a new resource to the cache server and replication controller"},{"location":"content/en/concepts/investigations/logical-clusters/","text":"Kubernetes evolved from and was influenced by earlier systems that had weaker internal tenancy than a general-purpose compute platform requires. The namespace, quota, admission, and RBAC concepts were all envisioned quite early in the project, but evolved with Kubernetes and not all impacts to future evolution were completely anticipated. Tenancy within clusters is handled at the resource and namespace level, and within a namespace there are a limited number of boundaries. Most organizations use either namespace or cluster separation as their primary unit of self service, with variations leveraging a rich ecosystem of tools. The one concrete component that cannot be tenanted are API resources - from a Kubernetes perspective we have too much history and ecosystem to desire a change, and so intra-cluster tenancy will always be at its weakest when users desire to add diverse extensions and tools. In addition, controllers are practically limited to a cluster scope or a namespace scope by the design of our APIs and authorization models and so intra-cluster tenancy for extensions is particularly weak (you can't prevent an ingress controller from viewing \"all secrets\"). Our admission chain has historically been very powerful and allowed deep policy to be enforced for tenancy, but the lack of a dynamic plugin model in golang has limited us to what can be accomplished to external RPC webhooks which have a number of significant performance and reliability impacts (especially when coupled to the cluster the webhook is acting on). If we want to have larger numbers of tenants or stronger subdivision, we need to consider improving the scalability of our policy chain in a number of dimensions. Ideally, as a community we could improve both namespace and cluster tenancy at the same time in a way that provides enhanced tools for teams and organizations, addresses extensions holistically, and improves the reliability and performance of policy control from our control planes. Goal: Getting a new cluster that allows a team to add extensions efficiently should be effectively zero cost If a cluster is a desirable unit of tenancy, clusters should be amortized \"free\" and easy to operationalize as self-service. We have explored in the community a number of approaches that make new clusters cheaper (specifically virtual clusters in SIG-multi-tenancy, as well as the natural cloud vendor \"as-a-service\" options where they amortize the cost of many small clusters), but there are certain fundamental fixed costs that inflate the cost of those clusters. If we could make one more cluster the same cost as a namespace, we could dramatically improve isolation of teams as well as offering an advantage for more alignment on tenancy across the ecosystem. Constraint: A naive client should see no difference between a physical or logical cluster A logical cluster that behaves differently from a physical cluster is not valuable for existing tools. We would need to lean heavily on our existing abstractions to ensure clients see no difference, and to focus on implementation options that avoid reduplicating a large amount of the Kube API surface area. Constraint: The implementation must improve isolation within a single process As clusters grow larger or are used in more complex fashion, the failure modes of a single process API server have received significant attention within the last few years. To offer cheaper clusters, we'd have to also improve isolation between simultaneous clients and manage etcd usage, traffic, and both cpu and memory use at the control plane. These stronger controls would be beneficial to physical clusters and organizations running more complex clusters as well. Constraint: We should improve in-process options for policy Early in Kubernetes we discussed our options for extension of the core capability - admission control and controllers are the two primary levers, with aggregated APIs being an escape hatch for more complex API behavior (including the ability to wrap existing APIs or CRDs). We should consider options that could reduce the cost of complex policy such as making using the Kube API more library-like (to enable forking) as well as in-process options for policy that could deliver order of magnitude higher reliability and performance than webhooks. Areas of investigation Define high level user use cases Prototype modelling the simplest option to enable kcp demo functionality and team subdivision Explore client changes required to make multi-cluster controllers efficient Support surfacing into a logical cluster API resources from another logical cluster Layering RBAC so that changes in one logical cluster are additive to a source policy that is out of the logical cluster's control Explore quota of requests, cpu, memory, and persistent to complement P&F per logical cluster with hard and soft limits Explore making kcp usable as a library so that an extender could write Golang admission / hierarchal policy for logical clusters that reduces the need for external extension Work through how a set of etcd objects could be moved to another etcd for sharding operations and keep clients unaware (similar to the \"restore a cluster from backup\" problem) Explore providing a read only resource underlay from another logical cluster so that immutable default objects can be provided Investigate use cases that would benefit even a single cluster (justify having this be a feature in kube-apiserver on by default) Use cases The use cases of logical cluster can be seen to overlap heavily with transparent multi-cluster use cases, and are captured at the highest level in GOALS.md . The use cases below attempt to focus on logical clusters independent of the broader goals. As a developer of CRDs / controllers / extensions I can launch a local Kube control plane and test out multiple different versions of the same CRD in parallel quickly I can create a control plane for my organization's cloud resources (CRDs) that is centralized but doesn't require me to provision nodes. As an infrastructure admin I can have strong tenant separation between different application teams Allow tenant teams to run their own custom resources (CRDs) and controllers without impacting others Subdivide access to the underlying clusters, keep those clusters simpler and with fewer extensions, and reduce the impact of cluster failure As a user on an existing Kubernetes cluster I can get a temporary space to test an extension before installing it I can create clusters that have my own namespaces Progress Logical clusters represented as a prefix to etcd In the early prototype stage kcp has a series of patches that allow a header or API prefix path to alter the prefix used to retrieve resources from etcd. The set of available resources is stripped down to a minimal set of hardcoded APIs including namespaces, rbac, and crds by patching those out of kube-apiserver type registration. The header X-Kubernetes-Cluster supports either a named logical cluster or the value * , or the prefix /cluster/<name> may be used at the root. This alters the behavior of a number of components, primarily retrieval and storage of API objects in etcd by adding a new segment to the etcd key (instead of /<resource>/<namespace>/<name> , /<resource>/<cluster>/<namespace>/<name> ). Providing * is currently acting on watch to support watching resources across all clusters, which also has the side effect of populating the object metadata.clusterName field. If no logical cluster name is provided, the value admin is used (which behaves as a normal kube-apiserver would). This means new logical clusters start off empty (no RBAC or CRD resources), which the kcp prototype mitigates by calculating the set of API resources available by merging from the default admin CRDs + the hardcoded APIs. That demonstrates one avenue of efficiency - a new logical cluster has an amortized cost near zero for both RBAC (no duplication of several hundred RBAC roles into the logical cluster) and API OpenAPI documents (built on demand as the union of another logical cluster and any CRDs added to the new cluster). To LIST or WATCH these resources, the user specifies * as their cluster name which adjusts the key prefix to fetch all resources across all logical clusters. It's likely some intermediate step between client and server would be necessary to support \"watch subset\" efficiently, which would require work to better enable clients to recognize the need to relist as well as the need to make the prototype support some level of logical cluster subset retrieval besides just an etcd key prefix scan. Next steps Continuing to explore how clients might query multiple resources across multiple logical clusters What changes to resource version are necessary to allow Zero configuration on startup in local dev The kcp binary embeds etcd in a single node config and manages it for local iterative development. This is in keeping with optimize for local workflow, but can be replaced by connecting to an existing etcd instance (not currently implemented). Ideally, a kcp like process would have minimal external dependencies and be capable of running in a shardable configuration efficiently (each shard handling 100k objects), with other components handling logical cluster sharding. CRD virtualization, inheritance, and normalization A simple implementation of CRD virtualization (different logical clusters having different api resources), CRD inheritance (a logical cluster inheriting CRDs from a parent logical cluster), and CRD normalization (between multiple physical clusters) to find the lowest-common-denominator resource has been prototyped. The CRD structure in the kube-apiserver is currently \"up front\" (as soon as a CRD is created it shows up in apiresources), but with the goal of reducing the up front cost of a logical cluster we may wish to suggest refactors upstream that would make the model more amenable to \"on demand\" construction and merging at runtime. OpenAPI merging is a very expensive part of the kube-apiserver historically (rapid CRD changes can have a massive memory and CPU impact) and this may be a logical area to invest to allow scaling within regular clusters. Inheritance allows an admin to control which resources a client might use - this would be particularly useful in more opinionated platform flows for organizations that wish to offer only a subset of APIs. The simplest approach here is that all logical clusters inherit the admin virtual cluster (the default), but more complicated flows with policy and chaining should be possible. Normalization involves reading OpenAPI docs from one or more child clusters, converting those to CRDs, finding the lowest compatible version of those CRDs (the version that shares all fields), and materializing those objects as CRDs in a logical cluster. This allows the minimum viable hook for turning a generic control plane into a spot where real Kube objects can run, and would be a key part of transparent multi-cluster.","title":"Logical clusters"},{"location":"content/en/concepts/investigations/logical-clusters/#goal-getting-a-new-cluster-that-allows-a-team-to-add-extensions-efficiently-should-be-effectively-zero-cost","text":"If a cluster is a desirable unit of tenancy, clusters should be amortized \"free\" and easy to operationalize as self-service. We have explored in the community a number of approaches that make new clusters cheaper (specifically virtual clusters in SIG-multi-tenancy, as well as the natural cloud vendor \"as-a-service\" options where they amortize the cost of many small clusters), but there are certain fundamental fixed costs that inflate the cost of those clusters. If we could make one more cluster the same cost as a namespace, we could dramatically improve isolation of teams as well as offering an advantage for more alignment on tenancy across the ecosystem.","title":"Goal: Getting a new cluster that allows a team to add extensions efficiently should be effectively zero cost"},{"location":"content/en/concepts/investigations/logical-clusters/#constraint-a-naive-client-should-see-no-difference-between-a-physical-or-logical-cluster","text":"A logical cluster that behaves differently from a physical cluster is not valuable for existing tools. We would need to lean heavily on our existing abstractions to ensure clients see no difference, and to focus on implementation options that avoid reduplicating a large amount of the Kube API surface area.","title":"Constraint: A naive client should see no difference between a physical or logical cluster"},{"location":"content/en/concepts/investigations/logical-clusters/#constraint-the-implementation-must-improve-isolation-within-a-single-process","text":"As clusters grow larger or are used in more complex fashion, the failure modes of a single process API server have received significant attention within the last few years. To offer cheaper clusters, we'd have to also improve isolation between simultaneous clients and manage etcd usage, traffic, and both cpu and memory use at the control plane. These stronger controls would be beneficial to physical clusters and organizations running more complex clusters as well.","title":"Constraint: The implementation must improve isolation within a single process"},{"location":"content/en/concepts/investigations/logical-clusters/#constraint-we-should-improve-in-process-options-for-policy","text":"Early in Kubernetes we discussed our options for extension of the core capability - admission control and controllers are the two primary levers, with aggregated APIs being an escape hatch for more complex API behavior (including the ability to wrap existing APIs or CRDs). We should consider options that could reduce the cost of complex policy such as making using the Kube API more library-like (to enable forking) as well as in-process options for policy that could deliver order of magnitude higher reliability and performance than webhooks.","title":"Constraint: We should improve in-process options for policy"},{"location":"content/en/concepts/investigations/logical-clusters/#areas-of-investigation","text":"Define high level user use cases Prototype modelling the simplest option to enable kcp demo functionality and team subdivision Explore client changes required to make multi-cluster controllers efficient Support surfacing into a logical cluster API resources from another logical cluster Layering RBAC so that changes in one logical cluster are additive to a source policy that is out of the logical cluster's control Explore quota of requests, cpu, memory, and persistent to complement P&F per logical cluster with hard and soft limits Explore making kcp usable as a library so that an extender could write Golang admission / hierarchal policy for logical clusters that reduces the need for external extension Work through how a set of etcd objects could be moved to another etcd for sharding operations and keep clients unaware (similar to the \"restore a cluster from backup\" problem) Explore providing a read only resource underlay from another logical cluster so that immutable default objects can be provided Investigate use cases that would benefit even a single cluster (justify having this be a feature in kube-apiserver on by default)","title":"Areas of investigation"},{"location":"content/en/concepts/investigations/logical-clusters/#use-cases","text":"The use cases of logical cluster can be seen to overlap heavily with transparent multi-cluster use cases, and are captured at the highest level in GOALS.md . The use cases below attempt to focus on logical clusters independent of the broader goals.","title":"Use cases"},{"location":"content/en/concepts/investigations/logical-clusters/#as-a-developer-of-crds-controllers-extensions","text":"I can launch a local Kube control plane and test out multiple different versions of the same CRD in parallel quickly I can create a control plane for my organization's cloud resources (CRDs) that is centralized but doesn't require me to provision nodes.","title":"As a developer of CRDs / controllers / extensions"},{"location":"content/en/concepts/investigations/logical-clusters/#as-an-infrastructure-admin","text":"I can have strong tenant separation between different application teams Allow tenant teams to run their own custom resources (CRDs) and controllers without impacting others Subdivide access to the underlying clusters, keep those clusters simpler and with fewer extensions, and reduce the impact of cluster failure","title":"As an infrastructure admin"},{"location":"content/en/concepts/investigations/logical-clusters/#as-a-user-on-an-existing-kubernetes-cluster","text":"I can get a temporary space to test an extension before installing it I can create clusters that have my own namespaces","title":"As a user on an existing Kubernetes cluster"},{"location":"content/en/concepts/investigations/logical-clusters/#progress","text":"","title":"Progress"},{"location":"content/en/concepts/investigations/logical-clusters/#logical-clusters-represented-as-a-prefix-to-etcd","text":"In the early prototype stage kcp has a series of patches that allow a header or API prefix path to alter the prefix used to retrieve resources from etcd. The set of available resources is stripped down to a minimal set of hardcoded APIs including namespaces, rbac, and crds by patching those out of kube-apiserver type registration. The header X-Kubernetes-Cluster supports either a named logical cluster or the value * , or the prefix /cluster/<name> may be used at the root. This alters the behavior of a number of components, primarily retrieval and storage of API objects in etcd by adding a new segment to the etcd key (instead of /<resource>/<namespace>/<name> , /<resource>/<cluster>/<namespace>/<name> ). Providing * is currently acting on watch to support watching resources across all clusters, which also has the side effect of populating the object metadata.clusterName field. If no logical cluster name is provided, the value admin is used (which behaves as a normal kube-apiserver would). This means new logical clusters start off empty (no RBAC or CRD resources), which the kcp prototype mitigates by calculating the set of API resources available by merging from the default admin CRDs + the hardcoded APIs. That demonstrates one avenue of efficiency - a new logical cluster has an amortized cost near zero for both RBAC (no duplication of several hundred RBAC roles into the logical cluster) and API OpenAPI documents (built on demand as the union of another logical cluster and any CRDs added to the new cluster). To LIST or WATCH these resources, the user specifies * as their cluster name which adjusts the key prefix to fetch all resources across all logical clusters. It's likely some intermediate step between client and server would be necessary to support \"watch subset\" efficiently, which would require work to better enable clients to recognize the need to relist as well as the need to make the prototype support some level of logical cluster subset retrieval besides just an etcd key prefix scan.","title":"Logical clusters represented as a prefix to etcd"},{"location":"content/en/concepts/investigations/logical-clusters/#next-steps","text":"Continuing to explore how clients might query multiple resources across multiple logical clusters What changes to resource version are necessary to allow","title":"Next steps"},{"location":"content/en/concepts/investigations/logical-clusters/#zero-configuration-on-startup-in-local-dev","text":"The kcp binary embeds etcd in a single node config and manages it for local iterative development. This is in keeping with optimize for local workflow, but can be replaced by connecting to an existing etcd instance (not currently implemented). Ideally, a kcp like process would have minimal external dependencies and be capable of running in a shardable configuration efficiently (each shard handling 100k objects), with other components handling logical cluster sharding.","title":"Zero configuration on startup in local dev"},{"location":"content/en/concepts/investigations/logical-clusters/#crd-virtualization-inheritance-and-normalization","text":"A simple implementation of CRD virtualization (different logical clusters having different api resources), CRD inheritance (a logical cluster inheriting CRDs from a parent logical cluster), and CRD normalization (between multiple physical clusters) to find the lowest-common-denominator resource has been prototyped. The CRD structure in the kube-apiserver is currently \"up front\" (as soon as a CRD is created it shows up in apiresources), but with the goal of reducing the up front cost of a logical cluster we may wish to suggest refactors upstream that would make the model more amenable to \"on demand\" construction and merging at runtime. OpenAPI merging is a very expensive part of the kube-apiserver historically (rapid CRD changes can have a massive memory and CPU impact) and this may be a logical area to invest to allow scaling within regular clusters. Inheritance allows an admin to control which resources a client might use - this would be particularly useful in more opinionated platform flows for organizations that wish to offer only a subset of APIs. The simplest approach here is that all logical clusters inherit the admin virtual cluster (the default), but more complicated flows with policy and chaining should be possible. Normalization involves reading OpenAPI docs from one or more child clusters, converting those to CRDs, finding the lowest compatible version of those CRDs (the version that shares all fields), and materializing those objects as CRDs in a logical cluster. This allows the minimum viable hook for turning a generic control plane into a spot where real Kube objects can run, and would be a key part of transparent multi-cluster.","title":"CRD virtualization, inheritance, and normalization"},{"location":"content/en/concepts/investigations/minimal-api-server/","text":"The Kubernetes API machinery provides a pattern for declarative config-driven API with a number of conventions that simplify building configuration loops and consolidating sources of truth. There have been many efforts to make that tooling more reusable and less dependent on the rest of the Kube concepts but without a strong use case driving separation and a design the tooling is still fairly coupled to Kube. Goal Building and sustaining an API server that: reuses much of the Kubernetes API server codebase to support Kube-like CRUD operations adds, removes, or excludes some / any / all the built-in Kubernetes types excludes some default assumptions of Kubernetes specific to the \"Kube as a cluster\" like \"create the kubernetes default svc\" replaces / modifies some implementations like custom resources, backend storage (etcd vs others), RBAC, admission control, and other primitives As a secondary goal, identifying where exceptions or undocumented assumptions exist in the libraries that would make clients behave differently generically (where an abstraction is not complete) should help ensure future clients can more concretely work across different API servers consistently. Constraint: The abstraction for a minimal API server should not hinder Kubernetes development The primary consumer of the Kube API is Kubernetes - any abstraction that makes a standalone API server possible must not regress Kubernetes performance or overly complicate Kubernetes evolution. The abstraction should be an opportunity to improve interfaces within Kubernetes to decouple components and improve comprehension. Constraint: Reusing the existing code base While it is certainly possible to rebuild all of Kube from scratch, a large amount of client tooling and benefit exists within patterns like declarative apply. This investigation is scoped to working within the context of improving the existing code and making the minimal changes within the bounds of API compatibility to broaden utility. It should be possible to add an arbitrary set of the existing Kube resources to the minimal API server, up to and including what an existing kube-apiserver exposes. Several use cases desire RBAC, Namespaces, Secrets, or other parts of the workload, while ensuring a \"pick and choose\" mindset keeps the interface supporting the needs of the full Kubernetes server. In the short term, it would not be a goal of this investigation to replace the underlying storage implementation etcd , but it should be possible to more easily inject the appropriate initialization code so that someone can easily start an API server that uses a different storage mechanism. Areas of investigation Define high level user use cases Document existing efforts inside and outside of the SIG process Identify near-term SIG API-Machinery work that would benefit from additional decoupling (also wg-code-organization) Find consensus points on near term changes and draft a KEP Use cases As a developer of CRDs / controllers / extensions I can launch a local Kube API and test out multiple different versions of the same CRD in parallel quickly (shared with logical-clusters ) I can create a control plane for my organization's cloud resources (CRDs) that is centralized but doesn't require me to provision nodes (shared with logical-clusters ) ... benefits for unit testing CRDs in controller projects? As a Kubernetes core developer The core API server libraries are better separated and more strongly reviewed Additional contributors are incentivized to maintain the core libraries of Kube because of a broader set of use cases Kube client tools have fewer edge cases because they are tested against multiple sets of resources ... As an aggregated API server developer It is easy to reuse the k8s.io/apiserver code base to provide: A virtual read-only resource that proxies to another type e.g. metrics-server An end user facing resource backed by a CRD (editable only by admins) that has additional validation and transformation e.g. service catalog A subresource implementation for a core type (pod/logs) that is not embedded in the Kube apiserver code As a devops team I want to be able to create declarative APIs using the controller pattern ... So that I can have declarative infrastructure without a full Kube cluster ( https://github.com/thetirefire/badidea and https://docs.google.com/presentation/d/1TfCrsBEgvyOQ1MGC7jBKTvyaelAYCZzl3udRjPlVmWg/edit#slide=id.g401c104a3c_0_0 ) So that I can have controllers that list/watch/sync/react to user focused changes So that I can have a kubectl apply loop for my intent (spec) and see the current state (status) So that I can move cloud infrastructure integrations like AWS Controllers for k8s out of individual clusters into a centrally secured spot I want to be offer a \"cluster-like\" user experience to a Kube application author without exposing the cluster directly ( transparent multi-cluster ) So that I can keep app authors from directly knowing about where the app runs for security / infrastructure abstraction So that I can control where applications run across multiple clusters centrally So that I can offer self-service provisioning at a higher level than namespace or cluster I want to consolidate all of my infrastructure and use gitops to talk to them the same way I do for clusters ... More detailed requests With some moderate boilerplate (50-100 lines of code) I can start a Kube compliant API server with (some / any of): Only custom built-in types (code -> generic registry -> etcd) CRDs (CustomResourceDefinition) Aggregated API support (APIService) Quotas and rate control and priority and fairness A custom admission chain that does not depend on webhooks but is inline code A different backend for storage other than etcd (projects like kine ) Add / wrap some HTTP handlers with middleware Progress Initial work in k/k fork involved stripping out elements of kube-apiserver start that required \"the full stack\" or internal controllers such as the kubernetes.default.svc maintainer (roughly pkg/master ). It also looked at how to pull a subset of Kube resources (namespaces, rbac, but not pods) from the core resource group. The kcp binary uses fairly normal k8s.io/apiserver methods to init the apiserver process. Next steps include identifying example use cases and the interfaces they wish to customize in the control plane (see above) and then looking at how those could be composed in an approachable way. That also involves exploring what refactors and organization makes sense within the k/k project in concert with 1-2 sig-apimachinery members.","title":"Minimal API Server"},{"location":"content/en/concepts/investigations/minimal-api-server/#goal","text":"Building and sustaining an API server that: reuses much of the Kubernetes API server codebase to support Kube-like CRUD operations adds, removes, or excludes some / any / all the built-in Kubernetes types excludes some default assumptions of Kubernetes specific to the \"Kube as a cluster\" like \"create the kubernetes default svc\" replaces / modifies some implementations like custom resources, backend storage (etcd vs others), RBAC, admission control, and other primitives As a secondary goal, identifying where exceptions or undocumented assumptions exist in the libraries that would make clients behave differently generically (where an abstraction is not complete) should help ensure future clients can more concretely work across different API servers consistently.","title":"Goal"},{"location":"content/en/concepts/investigations/minimal-api-server/#constraint-the-abstraction-for-a-minimal-api-server-should-not-hinder-kubernetes-development","text":"The primary consumer of the Kube API is Kubernetes - any abstraction that makes a standalone API server possible must not regress Kubernetes performance or overly complicate Kubernetes evolution. The abstraction should be an opportunity to improve interfaces within Kubernetes to decouple components and improve comprehension.","title":"Constraint: The abstraction for a minimal API server should not hinder Kubernetes development"},{"location":"content/en/concepts/investigations/minimal-api-server/#constraint-reusing-the-existing-code-base","text":"While it is certainly possible to rebuild all of Kube from scratch, a large amount of client tooling and benefit exists within patterns like declarative apply. This investigation is scoped to working within the context of improving the existing code and making the minimal changes within the bounds of API compatibility to broaden utility. It should be possible to add an arbitrary set of the existing Kube resources to the minimal API server, up to and including what an existing kube-apiserver exposes. Several use cases desire RBAC, Namespaces, Secrets, or other parts of the workload, while ensuring a \"pick and choose\" mindset keeps the interface supporting the needs of the full Kubernetes server. In the short term, it would not be a goal of this investigation to replace the underlying storage implementation etcd , but it should be possible to more easily inject the appropriate initialization code so that someone can easily start an API server that uses a different storage mechanism.","title":"Constraint: Reusing the existing code base"},{"location":"content/en/concepts/investigations/minimal-api-server/#areas-of-investigation","text":"Define high level user use cases Document existing efforts inside and outside of the SIG process Identify near-term SIG API-Machinery work that would benefit from additional decoupling (also wg-code-organization) Find consensus points on near term changes and draft a KEP","title":"Areas of investigation"},{"location":"content/en/concepts/investigations/minimal-api-server/#use-cases","text":"","title":"Use cases"},{"location":"content/en/concepts/investigations/minimal-api-server/#as-a-developer-of-crds-controllers-extensions","text":"I can launch a local Kube API and test out multiple different versions of the same CRD in parallel quickly (shared with logical-clusters ) I can create a control plane for my organization's cloud resources (CRDs) that is centralized but doesn't require me to provision nodes (shared with logical-clusters ) ... benefits for unit testing CRDs in controller projects?","title":"As a developer of CRDs / controllers / extensions"},{"location":"content/en/concepts/investigations/minimal-api-server/#as-a-kubernetes-core-developer","text":"The core API server libraries are better separated and more strongly reviewed Additional contributors are incentivized to maintain the core libraries of Kube because of a broader set of use cases Kube client tools have fewer edge cases because they are tested against multiple sets of resources ...","title":"As a Kubernetes core developer"},{"location":"content/en/concepts/investigations/minimal-api-server/#as-an-aggregated-api-server-developer","text":"It is easy to reuse the k8s.io/apiserver code base to provide: A virtual read-only resource that proxies to another type e.g. metrics-server An end user facing resource backed by a CRD (editable only by admins) that has additional validation and transformation e.g. service catalog A subresource implementation for a core type (pod/logs) that is not embedded in the Kube apiserver code","title":"As an aggregated API server developer"},{"location":"content/en/concepts/investigations/minimal-api-server/#as-a-devops-team","text":"I want to be able to create declarative APIs using the controller pattern ... So that I can have declarative infrastructure without a full Kube cluster ( https://github.com/thetirefire/badidea and https://docs.google.com/presentation/d/1TfCrsBEgvyOQ1MGC7jBKTvyaelAYCZzl3udRjPlVmWg/edit#slide=id.g401c104a3c_0_0 ) So that I can have controllers that list/watch/sync/react to user focused changes So that I can have a kubectl apply loop for my intent (spec) and see the current state (status) So that I can move cloud infrastructure integrations like AWS Controllers for k8s out of individual clusters into a centrally secured spot I want to be offer a \"cluster-like\" user experience to a Kube application author without exposing the cluster directly ( transparent multi-cluster ) So that I can keep app authors from directly knowing about where the app runs for security / infrastructure abstraction So that I can control where applications run across multiple clusters centrally So that I can offer self-service provisioning at a higher level than namespace or cluster I want to consolidate all of my infrastructure and use gitops to talk to them the same way I do for clusters ... More detailed requests With some moderate boilerplate (50-100 lines of code) I can start a Kube compliant API server with (some / any of): Only custom built-in types (code -> generic registry -> etcd) CRDs (CustomResourceDefinition) Aggregated API support (APIService) Quotas and rate control and priority and fairness A custom admission chain that does not depend on webhooks but is inline code A different backend for storage other than etcd (projects like kine ) Add / wrap some HTTP handlers with middleware","title":"As a devops team"},{"location":"content/en/concepts/investigations/minimal-api-server/#progress","text":"Initial work in k/k fork involved stripping out elements of kube-apiserver start that required \"the full stack\" or internal controllers such as the kubernetes.default.svc maintainer (roughly pkg/master ). It also looked at how to pull a subset of Kube resources (namespaces, rbac, but not pods) from the core resource group. The kcp binary uses fairly normal k8s.io/apiserver methods to init the apiserver process. Next steps include identifying example use cases and the interfaces they wish to customize in the control plane (see above) and then looking at how those could be composed in an approachable way. That also involves exploring what refactors and organization makes sense within the k/k project in concert with 1-2 sig-apimachinery members.","title":"Progress"},{"location":"content/en/concepts/investigations/self-service-policy/","text":"Goal Improve consistency and reusability of self-service and policy enforcement across multiple Kubernetes clusters. Just like Kubernetes standardized deploying containerized software onto a small set of machines, we want to standardize self-service of application focused integration across multiple teams with organizational control. Or possibly Kubernetes standardized deploying applications into chunks of capacity. We want to standardize isolating and integrating application teams across organizations, and to do that in a way that makes applications everywhere more secure. Problem A key component of large Kubernetes clusters is shared use, where the usage pattern might vary from externally controlled (via gitops / existing operational tools) to a permissive self-service model. The most common partitioning model in Kubernetes is namespace, and the second most common model is cluster. Self-service is currently limited by the set of resources that are namespace scoped for the former, and by the need to parameterize and configure multiple clusters consistently for the latter. Cluster partitioning can uniquely offer distinct sets of APIs to consumers. Namespace partitioning is cheap up until the scale limits of the cluster (~10k namespaces), while cluster partitioning usually has a fixed cost per cluster in operational and resource usage, as well as lower total utilization. Once a deployment reaches the scale limit of a single cluster, operators often need to redefine their policies and tools to work in a multi-cluster environment. Many large deployers create their own systems for managing self-service policy above their clusters and leverage individual subsystems within Kubernetes to accomplish those goals. Approach The logical cluster concept offers an opportunity to allow self-service at a cluster scope, with the effective cost of the namespace partitioning scheme. In addition, the separation of workload at control plane (kcp) and data plane (physical cluster) via transparent multi-cluster or similar schemes allows strong policy control of what configuration is allowed (reject early), restriction of the supported API surface area for workload APIs (limit / control certain fields like pod security), and limits the access of individual users to the underlying infra (much like clusters limit access to nodes). It should be possible to accomplish current self-service namespace and cluster partitioning via the logical cluster mechanism + policy enforcement, and to incentivize a wider range of \"external policy control\" users to adopt self-service via stronger control points and desirable use cases (multi-cluster resiliency for apps). We want to enable concrete points of injection of policy that are difficult today in Kubernetes tenancy: The acquisition of a new logical cluster with capabilities and constraints How the APIs in a logical cluster are transformed to an underlying cluster How to manage the evolution of APIs available to a logical cluster over time New hierarchal policy options are more practical since different logical clusters can have different APIs Areas of investigation Using logical clusters as a mechanism for tenancy, but having a backing implementation that can change I.e. materialize logical clusters as an API resource in a separate logical cluster Or implementing logical clusters outside the system and having the kcp server implementation be a shim Formal \"policy module\" implementations that can be plugged into a minimal API server while using logical cluster impl Catalog the set of tenancy constructs in use in Kube Draw heavily on sig-multitenancy explorations - work done by cluster api nested , virtual clusters , namespace tenancy, and hierarchal namespace designs Look at reference materials created by large organizational adopters of Kube Consider making \"cost\" a first class control concept alongside quota and RBAC (i.e. a service load balancer \"costs\" $1, whereas a regular service costs $0.001) Could this more effectively limit user action Explore hierarchy of policy - if logical clusters are selectable by label, could you have composability of policy using controllers Explore using implicit resources i.e. within a logical cluster have all resources of type RoleBinding be fetched from two sources - within the cluster, and in a separate logical cluster - and merged, so that you could change the global source and watches would still fire Impliict resources have risk though - no way to \"lock\" them so the consequences of an implicit change can be expensive Progress Simple example of a policy implementation Building out an example flow that goes from creating a logical cluster resource that results in a logical cluster being accessible to client, with potential hook points for deeper integration. Describe a complicated policy implementation An example hosted multi-tenant service with billing, organizational policy, and tenancy isolation.","title":"Self-service policy"},{"location":"content/en/concepts/investigations/self-service-policy/#goal","text":"Improve consistency and reusability of self-service and policy enforcement across multiple Kubernetes clusters. Just like Kubernetes standardized deploying containerized software onto a small set of machines, we want to standardize self-service of application focused integration across multiple teams with organizational control. Or possibly Kubernetes standardized deploying applications into chunks of capacity. We want to standardize isolating and integrating application teams across organizations, and to do that in a way that makes applications everywhere more secure.","title":"Goal"},{"location":"content/en/concepts/investigations/self-service-policy/#problem","text":"A key component of large Kubernetes clusters is shared use, where the usage pattern might vary from externally controlled (via gitops / existing operational tools) to a permissive self-service model. The most common partitioning model in Kubernetes is namespace, and the second most common model is cluster. Self-service is currently limited by the set of resources that are namespace scoped for the former, and by the need to parameterize and configure multiple clusters consistently for the latter. Cluster partitioning can uniquely offer distinct sets of APIs to consumers. Namespace partitioning is cheap up until the scale limits of the cluster (~10k namespaces), while cluster partitioning usually has a fixed cost per cluster in operational and resource usage, as well as lower total utilization. Once a deployment reaches the scale limit of a single cluster, operators often need to redefine their policies and tools to work in a multi-cluster environment. Many large deployers create their own systems for managing self-service policy above their clusters and leverage individual subsystems within Kubernetes to accomplish those goals.","title":"Problem"},{"location":"content/en/concepts/investigations/self-service-policy/#approach","text":"The logical cluster concept offers an opportunity to allow self-service at a cluster scope, with the effective cost of the namespace partitioning scheme. In addition, the separation of workload at control plane (kcp) and data plane (physical cluster) via transparent multi-cluster or similar schemes allows strong policy control of what configuration is allowed (reject early), restriction of the supported API surface area for workload APIs (limit / control certain fields like pod security), and limits the access of individual users to the underlying infra (much like clusters limit access to nodes). It should be possible to accomplish current self-service namespace and cluster partitioning via the logical cluster mechanism + policy enforcement, and to incentivize a wider range of \"external policy control\" users to adopt self-service via stronger control points and desirable use cases (multi-cluster resiliency for apps). We want to enable concrete points of injection of policy that are difficult today in Kubernetes tenancy: The acquisition of a new logical cluster with capabilities and constraints How the APIs in a logical cluster are transformed to an underlying cluster How to manage the evolution of APIs available to a logical cluster over time New hierarchal policy options are more practical since different logical clusters can have different APIs","title":"Approach"},{"location":"content/en/concepts/investigations/self-service-policy/#areas-of-investigation","text":"Using logical clusters as a mechanism for tenancy, but having a backing implementation that can change I.e. materialize logical clusters as an API resource in a separate logical cluster Or implementing logical clusters outside the system and having the kcp server implementation be a shim Formal \"policy module\" implementations that can be plugged into a minimal API server while using logical cluster impl Catalog the set of tenancy constructs in use in Kube Draw heavily on sig-multitenancy explorations - work done by cluster api nested , virtual clusters , namespace tenancy, and hierarchal namespace designs Look at reference materials created by large organizational adopters of Kube Consider making \"cost\" a first class control concept alongside quota and RBAC (i.e. a service load balancer \"costs\" $1, whereas a regular service costs $0.001) Could this more effectively limit user action Explore hierarchy of policy - if logical clusters are selectable by label, could you have composability of policy using controllers Explore using implicit resources i.e. within a logical cluster have all resources of type RoleBinding be fetched from two sources - within the cluster, and in a separate logical cluster - and merged, so that you could change the global source and watches would still fire Impliict resources have risk though - no way to \"lock\" them so the consequences of an implicit change can be expensive","title":"Areas of investigation"},{"location":"content/en/concepts/investigations/self-service-policy/#progress","text":"","title":"Progress"},{"location":"content/en/concepts/investigations/self-service-policy/#simple-example-of-a-policy-implementation","text":"Building out an example flow that goes from creating a logical cluster resource that results in a logical cluster being accessible to client, with potential hook points for deeper integration.","title":"Simple example of a policy implementation"},{"location":"content/en/concepts/investigations/self-service-policy/#describe-a-complicated-policy-implementation","text":"An example hosted multi-tenant service with billing, organizational policy, and tenancy isolation.","title":"Describe a complicated policy implementation"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/","text":"A key tenet of Kubernetes is that workload placement is node-agnostic until the user needs it to be - Kube offers a homogeneous compute surface that admins or app devs can \"break-glass\" and set constraints all the way down to writing software that deeply integrates with nodes. But for the majority of workloads a cluster is no more important than a node - it's a detail determined by some human or automated process. A key area of investigation for kcp is exploring transparency of workloads to clusters. Aspirationally we want Kube workloads to be resilient to the operational characteristics of the underlying infrastructure and clusters orthogonally to the workload, by isolating the user from knowing of the details of the infrastructure. If workload APIs are more consistently \"node-less\" and \"cluster-agnostic\" that opens up ways to drive workload consistency across a large swathe of the compute landscape. Goal: The majority of applications and teams should have workflows where cluster is a detail A number of projects have explored this since the beginning of Kubernetes - this prototype should explore in detail how we can make a normal Kubernetes flow for most users be cluster-independent but still \"break-glass\" and describe placement in detail. Since this is a broad topic and we want to benefit the majority of users, we need to also add constraints that maximize the chance of these approaches being adopted. Constraint: The workflows and practices teams use today should be minimally disrupted Users typically only change their workflows when an improvement offers a significant multiplier. To be effective we must reduce friction (which reduces multipliers) and offer significant advantages to that workflow. Tools, practices, user experiences, and automation should \"just work\" when applied to cluster-agnostic or cluster-aware workloads. This includes gitops, rich web interfaces, kubectl , etc. That implies that a \"cluster\" and a \"Kube API\" is our key target, and that we must preserve a majority of semantic meaning of existing APIs. Constraint: 95% of workloads should \"just work\" when kubectl apply d to kcp It continues to be possible to build different abstractions on top of Kube, but existing workloads are what really benefit users. They have chosen the Kube abstractions deliberately because they are general purpose - rather than describe a completely new system we believe it is more effective to uplevel these existing apps. That means that existing primitives like Service, Deployment, PersistentVolumeClaim, StatefulSet must all require no changes to move from single-cluster to multi-cluster By choosing this constraint, we also accept that we will have to be opinionated on making the underlying clusters consistent, and we will have to limit / constrain certain behaviors. Ideally, we focus on preserving the user's view of the changes on a logical cluster, while making the workloads on a physical cluster look more consistent for infrastructure admins. This implies we need to explore both what these workloads might look like (a review of applications) and describe the points of control / abstraction between levels. Constraint: 90% of application infrastructure controllers should be useful against kcp A controller that performs app infra related functions should be useful without change against kcp . For instance, the etcd operator takes an Etcd cluster CRD and creates pods. It should be possible for that controller to target a kcp logical cluster with the CRD and create pods on the logical cluster that are transparently placed onto a cluster. Areas of investigation Define high level user use cases Study approaches from the ecosystem that do not require workloads to change significantly to spread Explore characteristics of most common Kube workload objects that could allow them to be transparently placed Identify the control points and data flow between workload and physical cluster that would be generally useful across a wide range of approaches - such as: How placement is assigned, altered, and removed (\"scheduling\" or \"placement\") How workloads are transformed from high level to low level and then summarized back Categorize approaches in the ecosystem and gaps where collaboration could improve velocity 4. Identify key infrastructure characteristics for multi-cluster Networking between components and transparency of location to movement Data movement, placement, and replication Abstraction/interception of off-cluster dependencies (external to the system) Consistency of infrastructure (where does Kube not sufficiently drive operational consistency) Seek consensus in user communities on whether the abstractions are practical Invest in key technologies in the appropriate projects Formalize parts of the prototype into project(s) drawing on the elements above if successful! Use cases Representing feedback from a number of multi-cluster users with a diverse set of technologies in play: As a user I can kubectl apply a workload that is agnostic to node placement to kcp and see the workload assigned to real resources and start running and the status summarized back to me. I can move an application (defined in 1) between two physical clusters by changing a single high level attribute As a user when I move an application (as defined in 2) no disruption of internal or external traffic is visible to my consumers As a user I can debug my application in a familiar manner regardless of cluster As a user with a stateful application by persistent volumes can move / replicate / be shared across clusters in a manner consistent with my storage type (read-write-one / read-write-many). As an infrastructure admin I can decommision an physical cluster and see workloads moved without disruption I can set capacity bounds that control admission to a particular cluster and react to workload growth organically Progress In the early prototype stage kcp uses the syncer and the deployment-splitter as stand-ins for more complex scheduling and transformation. This section should see more updates in the near term as we move beyond areas 1-2 (use cases and ecosystem research) Possible design simplifications Focus on every object having an annotation saying which clusters it is targeted at We can control the annotation via admission eventually, works for all objects Tracking declarative and atomic state change (NONE -> A, A->(A,B), A->NONE) on objects RBAC stays at the higher level and applies to the logical clusters, is not synced Implication is that controllers won't be syncable today, BUT that's ok because it's likely giving workloads control over the underlying cluster is a non-goal to start and would have to be explicit opt-in by admin Controllers already need to separate input from output - most controllers assume they're the same (but things like service load balancer) Think of scheduling as a policy at global, per logical cluster, and optionally the namespace (policy of object type -> 0..N clusters) Simplification over doing a bunch of per object work, since we want to be transparent (per object is a future optimization with some limits)","title":"Transparent multi-cluster"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#goal-the-majority-of-applications-and-teams-should-have-workflows-where-cluster-is-a-detail","text":"A number of projects have explored this since the beginning of Kubernetes - this prototype should explore in detail how we can make a normal Kubernetes flow for most users be cluster-independent but still \"break-glass\" and describe placement in detail. Since this is a broad topic and we want to benefit the majority of users, we need to also add constraints that maximize the chance of these approaches being adopted.","title":"Goal: The majority of applications and teams should have workflows where cluster is a detail"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#constraint-the-workflows-and-practices-teams-use-today-should-be-minimally-disrupted","text":"Users typically only change their workflows when an improvement offers a significant multiplier. To be effective we must reduce friction (which reduces multipliers) and offer significant advantages to that workflow. Tools, practices, user experiences, and automation should \"just work\" when applied to cluster-agnostic or cluster-aware workloads. This includes gitops, rich web interfaces, kubectl , etc. That implies that a \"cluster\" and a \"Kube API\" is our key target, and that we must preserve a majority of semantic meaning of existing APIs.","title":"Constraint: The workflows and practices teams use today should be minimally disrupted"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#constraint-95-of-workloads-should-just-work-when-kubectl-applyd-to-kcp","text":"It continues to be possible to build different abstractions on top of Kube, but existing workloads are what really benefit users. They have chosen the Kube abstractions deliberately because they are general purpose - rather than describe a completely new system we believe it is more effective to uplevel these existing apps. That means that existing primitives like Service, Deployment, PersistentVolumeClaim, StatefulSet must all require no changes to move from single-cluster to multi-cluster By choosing this constraint, we also accept that we will have to be opinionated on making the underlying clusters consistent, and we will have to limit / constrain certain behaviors. Ideally, we focus on preserving the user's view of the changes on a logical cluster, while making the workloads on a physical cluster look more consistent for infrastructure admins. This implies we need to explore both what these workloads might look like (a review of applications) and describe the points of control / abstraction between levels.","title":"Constraint: 95% of workloads should \"just work\" when kubectl applyd to kcp"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#constraint-90-of-application-infrastructure-controllers-should-be-useful-against-kcp","text":"A controller that performs app infra related functions should be useful without change against kcp . For instance, the etcd operator takes an Etcd cluster CRD and creates pods. It should be possible for that controller to target a kcp logical cluster with the CRD and create pods on the logical cluster that are transparently placed onto a cluster.","title":"Constraint: 90% of application infrastructure controllers should be useful against kcp"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#areas-of-investigation","text":"Define high level user use cases Study approaches from the ecosystem that do not require workloads to change significantly to spread Explore characteristics of most common Kube workload objects that could allow them to be transparently placed Identify the control points and data flow between workload and physical cluster that would be generally useful across a wide range of approaches - such as: How placement is assigned, altered, and removed (\"scheduling\" or \"placement\") How workloads are transformed from high level to low level and then summarized back Categorize approaches in the ecosystem and gaps where collaboration could improve velocity 4. Identify key infrastructure characteristics for multi-cluster Networking between components and transparency of location to movement Data movement, placement, and replication Abstraction/interception of off-cluster dependencies (external to the system) Consistency of infrastructure (where does Kube not sufficiently drive operational consistency) Seek consensus in user communities on whether the abstractions are practical Invest in key technologies in the appropriate projects Formalize parts of the prototype into project(s) drawing on the elements above if successful!","title":"Areas of investigation"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#use-cases","text":"Representing feedback from a number of multi-cluster users with a diverse set of technologies in play:","title":"Use cases"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#as-a-user","text":"I can kubectl apply a workload that is agnostic to node placement to kcp and see the workload assigned to real resources and start running and the status summarized back to me. I can move an application (defined in 1) between two physical clusters by changing a single high level attribute As a user when I move an application (as defined in 2) no disruption of internal or external traffic is visible to my consumers As a user I can debug my application in a familiar manner regardless of cluster As a user with a stateful application by persistent volumes can move / replicate / be shared across clusters in a manner consistent with my storage type (read-write-one / read-write-many).","title":"As a user"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#as-an-infrastructure-admin","text":"I can decommision an physical cluster and see workloads moved without disruption I can set capacity bounds that control admission to a particular cluster and react to workload growth organically","title":"As an infrastructure admin"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#progress","text":"In the early prototype stage kcp uses the syncer and the deployment-splitter as stand-ins for more complex scheduling and transformation. This section should see more updates in the near term as we move beyond areas 1-2 (use cases and ecosystem research)","title":"Progress"},{"location":"content/en/concepts/investigations/transparent-multi-cluster/#possible-design-simplifications","text":"Focus on every object having an annotation saying which clusters it is targeted at We can control the annotation via admission eventually, works for all objects Tracking declarative and atomic state change (NONE -> A, A->(A,B), A->NONE) on objects RBAC stays at the higher level and applies to the logical clusters, is not synced Implication is that controllers won't be syncable today, BUT that's ok because it's likely giving workloads control over the underlying cluster is a non-goal to start and would have to be explicit opt-in by admin Controllers already need to separate input from output - most controllers assume they're the same (but things like service load balancer) Think of scheduling as a policy at global, per logical cluster, and optionally the namespace (policy of object type -> 0..N clusters) Simplification over doing a bunch of per object work, since we want to be transparent (per object is a future optimization with some limits)","title":"Possible design simplifications"},{"location":"content/en/reference/cli/kcp/","text":"Synopsis KCP is the easiest way to manage Kubernetes applications against one or more clusters, by giving you a personal control plane that schedules your workloads onto one or many clusters, and making it simple to pick up and move. Advanced use cases including spreading your apps across clusters for resiliency, scheduling batch workloads onto clusters with free capacity, and enabling collaboration for individual teams without having access to the underlying clusters. This command provides KCP specific sub-command for kubectl. Options --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) -h, --help help for kcp --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp bind - Bind different types into current workspace. kcp claims - Operations related to viewing or updating permission claims kcp crd - CRD related operations kcp workload - Manages KCP sync targets kcp workspace - Manages KCP workspaces","title":"kcp"},{"location":"content/en/reference/cli/kcp/#synopsis","text":"KCP is the easiest way to manage Kubernetes applications against one or more clusters, by giving you a personal control plane that schedules your workloads onto one or many clusters, and making it simple to pick up and move. Advanced use cases including spreading your apps across clusters for resiliency, scheduling batch workloads onto clusters with free capacity, and enabling collaboration for individual teams without having access to the underlying clusters. This command provides KCP specific sub-command for kubectl.","title":"Synopsis"},{"location":"content/en/reference/cli/kcp/#options","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) -h, --help help for kcp --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options"},{"location":"content/en/reference/cli/kcp/#see-also","text":"kcp bind - Bind different types into current workspace. kcp claims - Operations related to viewing or updating permission claims kcp crd - CRD related operations kcp workload - Manages KCP sync targets kcp workspace - Manages KCP workspaces","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_bind/","text":"kcp bind [flags] Options -h, --help help for bind Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp - kubectl plugin for KCP kcp bind apiexport - Bind to an APIExport kcp bind compute - Bind to a location workspace","title":"kcp bind"},{"location":"content/en/reference/cli/kcp_bind/#options","text":"-h, --help help for bind","title":"Options"},{"location":"content/en/reference/cli/kcp_bind/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_bind/#see-also","text":"kcp - kubectl plugin for KCP kcp bind apiexport - Bind to an APIExport kcp bind compute - Bind to a location workspace","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_bind_apiexport/","text":"kcp bind apiexport <workspace_path:apiexport-name> [flags] Examples # Create an APIBinding named \"my-binding\" that binds to the APIExport \"my-export\" in the \"root:my-service\" workspace. kubectl kcp bind apiexport root:my-service:my-export --name my-binding Options --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for apiexport --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file --name string Name of the APIBinding to create. -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --timeout duration Duration to wait for APIBinding to be created successfully. (default 30s) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp bind - Bind different types into current workspace.","title":"kcp bind apiexport"},{"location":"content/en/reference/cli/kcp_bind_apiexport/#examples","text":"# Create an APIBinding named \"my-binding\" that binds to the APIExport \"my-export\" in the \"root:my-service\" workspace. kubectl kcp bind apiexport root:my-service:my-export --name my-binding","title":"Examples"},{"location":"content/en/reference/cli/kcp_bind_apiexport/#options","text":"--as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for apiexport --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file --name string Name of the APIBinding to create. -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --timeout duration Duration to wait for APIBinding to be created successfully. (default 30s) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options"},{"location":"content/en/reference/cli/kcp_bind_apiexport/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_bind_apiexport/#see-also","text":"kcp bind - Bind different types into current workspace.","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_bind_compute/","text":"kcp bind compute <location workspace> [flags] Examples # Create a placement to deploy standard kubernetes workloads to synctargets in the \"root:mylocations\" location workspace. kubectl kcp bind compute root:mylocations # Create a placement to deploy custom workloads to synctargets in the \"root:mylocations\" location workspace. kubectl kcp bind compute root:mylocations --apiexports=root:myapis:customapiexport # Create a placement to deploy standard kubernetes workloads to synctargets in the \"root:mylocations\" location workspace, and select only locations in the us-east region. kubectl kcp bind compute root:mylocations --location-selectors=region=us-east1 Options --apiexports strings APIExport to bind to this workspace for workload, each APIExport should be in the format of <absolute_ref_to_workspace>:<apiexport> (default [root:compute:kubernetes]) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for compute --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file --location-selectors strings A list of label selectors to select locations in the location workspace to sync workload. --name string Name of the placement to be created. -n, --namespace string If present, the namespace scope for this CLI request --namespace-selector string Label select to select namespaces to create workload. --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --timeout duration Duration to wait for Placement to be created and bound successfully. (default 30s) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp bind - Bind different types into current workspace.","title":"kcp bind compute"},{"location":"content/en/reference/cli/kcp_bind_compute/#examples","text":"# Create a placement to deploy standard kubernetes workloads to synctargets in the \"root:mylocations\" location workspace. kubectl kcp bind compute root:mylocations # Create a placement to deploy custom workloads to synctargets in the \"root:mylocations\" location workspace. kubectl kcp bind compute root:mylocations --apiexports=root:myapis:customapiexport # Create a placement to deploy standard kubernetes workloads to synctargets in the \"root:mylocations\" location workspace, and select only locations in the us-east region. kubectl kcp bind compute root:mylocations --location-selectors=region=us-east1","title":"Examples"},{"location":"content/en/reference/cli/kcp_bind_compute/#options","text":"--apiexports strings APIExport to bind to this workspace for workload, each APIExport should be in the format of <absolute_ref_to_workspace>:<apiexport> (default [root:compute:kubernetes]) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for compute --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file --location-selectors strings A list of label selectors to select locations in the location workspace to sync workload. --name string Name of the placement to be created. -n, --namespace string If present, the namespace scope for this CLI request --namespace-selector string Label select to select namespaces to create workload. --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --timeout duration Duration to wait for Placement to be created and bound successfully. (default 30s) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options"},{"location":"content/en/reference/cli/kcp_bind_compute/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_bind_compute/#see-also","text":"kcp bind - Bind different types into current workspace.","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_claims/","text":"kcp claims [flags] Examples # Lists the permission claims and their respective status related to a specific APIBinding. kubectl claims get apibinding cert-manager # List permission claims and their respective status for all APIBindings in current workspace. kubectl claims get apibinding Options -h, --help help for claims Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp - kubectl plugin for KCP kcp claims get - Operations related to fetching APIs with respect to permission claims","title":"kcp claims"},{"location":"content/en/reference/cli/kcp_claims/#examples","text":"# Lists the permission claims and their respective status related to a specific APIBinding. kubectl claims get apibinding cert-manager # List permission claims and their respective status for all APIBindings in current workspace. kubectl claims get apibinding","title":"Examples"},{"location":"content/en/reference/cli/kcp_claims/#options","text":"-h, --help help for claims","title":"Options"},{"location":"content/en/reference/cli/kcp_claims/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_claims/#see-also","text":"kcp - kubectl plugin for KCP kcp claims get - Operations related to fetching APIs with respect to permission claims","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_claims_get/","text":"kcp claims get [flags] Options -h, --help help for get Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp claims - Operations related to viewing or updating permission claims kcp claims get apibinding - Get claims related to apibinding","title":"kcp claims get"},{"location":"content/en/reference/cli/kcp_claims_get/#options","text":"-h, --help help for get","title":"Options"},{"location":"content/en/reference/cli/kcp_claims_get/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_claims_get/#see-also","text":"kcp claims - Operations related to viewing or updating permission claims kcp claims get apibinding - Get claims related to apibinding","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_claims_get_apibinding/","text":"kcp claims get apibinding <apibinding_name> [flags] Options --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for apibinding --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp claims get - Operations related to fetching APIs with respect to permission claims","title":"kcp claims get apibinding"},{"location":"content/en/reference/cli/kcp_claims_get_apibinding/#options","text":"--as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for apibinding --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options"},{"location":"content/en/reference/cli/kcp_claims_get_apibinding/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_claims_get_apibinding/#see-also","text":"kcp claims get - Operations related to fetching APIs with respect to permission claims","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_crd/","text":"kcp crd [flags] Options -h, --help help for crd Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp - kubectl plugin for KCP kcp crd snapshot - Snapshot a CRD and convert it to an APIResourceSchema","title":"kcp crd"},{"location":"content/en/reference/cli/kcp_crd/#options","text":"-h, --help help for crd","title":"Options"},{"location":"content/en/reference/cli/kcp_crd/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_crd/#see-also","text":"kcp - kubectl plugin for KCP kcp crd snapshot - Snapshot a CRD and convert it to an APIResourceSchema","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_crd_snapshot/","text":"kcp crd snapshot -f FILE --prefix PREFIX [flags] Examples # Convert a CRD in a yaml file to an APIResourceSchema. For a CRD named widgets.example.io, and a prefix value of # 'today', the new APIResourceSchema's name will be today.widgets.example.io. kubectl kcp crd snapshot -f crd.yaml --prefix 2022-05-07 > api-resource-schema.yaml # Convert a CRD from STDIN kubectl get crd foo -o yaml | kubectl kcp crd snapshot -f - --prefix today > output.yaml Options -f, --filename string Path to a file containing the CRD to convert to an APIResourceSchema, or - for stdin -h, --help help for snapshot -o, --output string Output format. Valid values are 'json' and 'yaml' (default \"yaml\") --prefix string Prefix to use for the APIResourceSchema's name, before <resource>.<group> Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp crd - CRD related operations","title":"kcp crd snapshot"},{"location":"content/en/reference/cli/kcp_crd_snapshot/#examples","text":"# Convert a CRD in a yaml file to an APIResourceSchema. For a CRD named widgets.example.io, and a prefix value of # 'today', the new APIResourceSchema's name will be today.widgets.example.io. kubectl kcp crd snapshot -f crd.yaml --prefix 2022-05-07 > api-resource-schema.yaml # Convert a CRD from STDIN kubectl get crd foo -o yaml | kubectl kcp crd snapshot -f - --prefix today > output.yaml","title":"Examples"},{"location":"content/en/reference/cli/kcp_crd_snapshot/#options","text":"-f, --filename string Path to a file containing the CRD to convert to an APIResourceSchema, or - for stdin -h, --help help for snapshot -o, --output string Output format. Valid values are 'json' and 'yaml' (default \"yaml\") --prefix string Prefix to use for the APIResourceSchema's name, before <resource>.<group>","title":"Options"},{"location":"content/en/reference/cli/kcp_crd_snapshot/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_crd_snapshot/#see-also","text":"kcp crd - CRD related operations","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workload/","text":"kcp workload [flags] Options -h, --help help for workload Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp - kubectl plugin for KCP kcp workload cordon - Mark sync target as unschedulable kcp workload drain - Start draining sync target in preparation for maintenance kcp workload sync - Create a synctarget in kcp with service account and RBAC permissions. Output a manifest to deploy a syncer for the given sync target in a physical cluster. kcp workload uncordon - Mark sync target as schedulable","title":"kcp workload"},{"location":"content/en/reference/cli/kcp_workload/#options","text":"-h, --help help for workload","title":"Options"},{"location":"content/en/reference/cli/kcp_workload/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workload/#see-also","text":"kcp - kubectl plugin for KCP kcp workload cordon - Mark sync target as unschedulable kcp workload drain - Start draining sync target in preparation for maintenance kcp workload sync - Create a synctarget in kcp with service account and RBAC permissions. Output a manifest to deploy a syncer for the given sync target in a physical cluster. kcp workload uncordon - Mark sync target as schedulable","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workload_cordon/","text":"kcp workload cordon <sync-target-name> [flags] Examples # Mark a sync target as unschedulable. kubectl kcp workload cordon <sync-target-name> Options --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for cordon --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp workload - Manages KCP sync targets","title":"kcp workload cordon"},{"location":"content/en/reference/cli/kcp_workload_cordon/#examples","text":"# Mark a sync target as unschedulable. kubectl kcp workload cordon <sync-target-name>","title":"Examples"},{"location":"content/en/reference/cli/kcp_workload_cordon/#options","text":"--as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for cordon --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options"},{"location":"content/en/reference/cli/kcp_workload_cordon/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workload_cordon/#see-also","text":"kcp workload - Manages KCP sync targets","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workload_drain/","text":"kcp workload drain <sync-target-name> [flags] Examples # Start draining a sync target in preparation for maintenance. kubectl kcp workload drain <sync-target-name> Options --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for drain --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp workload - Manages KCP sync targets","title":"kcp workload drain"},{"location":"content/en/reference/cli/kcp_workload_drain/#examples","text":"# Start draining a sync target in preparation for maintenance. kubectl kcp workload drain <sync-target-name>","title":"Examples"},{"location":"content/en/reference/cli/kcp_workload_drain/#options","text":"--as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for drain --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options"},{"location":"content/en/reference/cli/kcp_workload_drain/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workload_drain/#see-also","text":"kcp workload - Manages KCP sync targets","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workload_sync/","text":"kcp workload sync <sync-target-name> --syncer-image <kcp-syncer-image> [--resources=<resource1>,<resource2>..] -o <output-file> [flags] Examples # Ensure a syncer is running on the specified sync target. kubectl kcp workload sync <sync-target-name> --syncer-image <kcp-syncer-image> -o syncer.yaml KUBECONFIG=<pcluster-config> kubectl apply -f syncer.yaml # Directly apply the manifest kubectl kcp workload sync <sync-target-name> --syncer-image <kcp-syncer-image> -o - | KUBECONFIG=<pcluster-config> kubectl apply -f - Options --api-import-poll-interval duration Polling interval for API import. (default 1m0s) --apiexports strings APIExport to be supported by the syncer, each APIExport should be in the format of <absolute_ref_to_workspace>:<apiexport>, e.g. root:compute:kubernetes is the kubernetes APIExport in root:compute workspace (default [root:compute:kubernetes]) --as-uid string UID to impersonate for the operation --burst int Burst to use when talking to API servers. (default 30) --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --downstream-namespace-clean-delay duration Time to wait before deleting a downstream namespaces. (default 30s) --feature-gates string A set of key=value pairs that describe feature gates for alpha/experimental features. Options are: APIResponseCompression APIListChunking DryRun ServerSideApply APIPriorityAndFairness CustomResourceValidationExpressions AdvancedAuditing KCPSyncerTunnel ContextualLogging KCPLocationAPI -h, --help help for sync --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kcp-namespace string The name of the kcp namespace to create a service account in. (default \"default\") --kubeconfig string path to the kubeconfig file -n, --namespace string The namespace to create the syncer in the physical cluster. By default this is \"kcp-syncer-<synctarget-name>-<uid>\". -o, --output-file string The manifest file to be created and applied to the physical cluster. Use - for stdout. --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --qps float32 QPS to use when talking to API servers. (default 20) --replicas int Number of replicas of the syncer deployment. (default 1) --resources strings Resources to synchronize with kcp. --server string The address and port of the Kubernetes API server --syncer-image string The syncer image to use in the syncer's deployment YAML. Images are published at https://github.com/kcp-dev/kcp/pkgs/container/kcp%2Fsyncer. --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp workload - Manages KCP sync targets","title":"kcp workload sync"},{"location":"content/en/reference/cli/kcp_workload_sync/#examples","text":"# Ensure a syncer is running on the specified sync target. kubectl kcp workload sync <sync-target-name> --syncer-image <kcp-syncer-image> -o syncer.yaml KUBECONFIG=<pcluster-config> kubectl apply -f syncer.yaml # Directly apply the manifest kubectl kcp workload sync <sync-target-name> --syncer-image <kcp-syncer-image> -o - | KUBECONFIG=<pcluster-config> kubectl apply -f -","title":"Examples"},{"location":"content/en/reference/cli/kcp_workload_sync/#options","text":"--api-import-poll-interval duration Polling interval for API import. (default 1m0s) --apiexports strings APIExport to be supported by the syncer, each APIExport should be in the format of <absolute_ref_to_workspace>:<apiexport>, e.g. root:compute:kubernetes is the kubernetes APIExport in root:compute workspace (default [root:compute:kubernetes]) --as-uid string UID to impersonate for the operation --burst int Burst to use when talking to API servers. (default 30) --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --downstream-namespace-clean-delay duration Time to wait before deleting a downstream namespaces. (default 30s) --feature-gates string A set of key=value pairs that describe feature gates for alpha/experimental features. Options are: APIResponseCompression APIListChunking DryRun ServerSideApply APIPriorityAndFairness CustomResourceValidationExpressions AdvancedAuditing KCPSyncerTunnel ContextualLogging KCPLocationAPI -h, --help help for sync --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kcp-namespace string The name of the kcp namespace to create a service account in. (default \"default\") --kubeconfig string path to the kubeconfig file -n, --namespace string The namespace to create the syncer in the physical cluster. By default this is \"kcp-syncer-<synctarget-name>-<uid>\". -o, --output-file string The manifest file to be created and applied to the physical cluster. Use - for stdout. --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --qps float32 QPS to use when talking to API servers. (default 20) --replicas int Number of replicas of the syncer deployment. (default 1) --resources strings Resources to synchronize with kcp. --server string The address and port of the Kubernetes API server --syncer-image string The syncer image to use in the syncer's deployment YAML. Images are published at https://github.com/kcp-dev/kcp/pkgs/container/kcp%2Fsyncer. --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options"},{"location":"content/en/reference/cli/kcp_workload_sync/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workload_sync/#see-also","text":"kcp workload - Manages KCP sync targets","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workload_uncordon/","text":"kcp workload uncordon <sync-target-name> [flags] Examples # Mark a sync target as schedulable. kubectl kcp workload uncordon <sync-target-name> Options --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for uncordon --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp workload - Manages KCP sync targets","title":"kcp workload uncordon"},{"location":"content/en/reference/cli/kcp_workload_uncordon/#examples","text":"# Mark a sync target as schedulable. kubectl kcp workload uncordon <sync-target-name>","title":"Examples"},{"location":"content/en/reference/cli/kcp_workload_uncordon/#options","text":"--as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for uncordon --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options"},{"location":"content/en/reference/cli/kcp_workload_uncordon/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workload_uncordon/#see-also","text":"kcp workload - Manages KCP sync targets","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workspace/","text":"kcp workspace [create|create-context|use|current|<workspace>|..|.|-|~|<root:absolute:workspace>] [flags] Examples # shows the workspace you are currently using kubectl workspace . # enter a given workspace (this will change the current-context of your current KUBECONFIG) kubectl workspace use my-workspace # short-hand for the use syntax kubectl workspace my-workspace # enter a given absolute workspace kubectl workspace root:default:my-workspace # enter the parent workspace kubectl workspace .. # enter the previous workspace kubectl workspace - # go to your home workspace kubectl workspace # create a workspace and immediately enter it kubectl workspace create my-workspace --enter # create a context with the current workspace, e.g. root:default:my-workspace kubectl workspace create-context # create a context with the current workspace, named context-name kubectl workspace create-context context-name Options --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for workspace --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --short Print only the name of the workspace, e.g. for integration into the shell prompt --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp - kubectl plugin for KCP kcp workspace create - Creates a new workspace kcp workspace create-context - Create a kubeconfig context for the current workspace kcp workspace current - Print the current workspace. Same as 'kubectl ws .'. kcp workspace tree - Print the current workspace tree. kcp workspace use - Uses the given workspace as the current workspace. Using - means previous workspace, .. means parent workspace, . mean current, ~ means home workspace","title":"kcp workspace"},{"location":"content/en/reference/cli/kcp_workspace/#examples","text":"# shows the workspace you are currently using kubectl workspace . # enter a given workspace (this will change the current-context of your current KUBECONFIG) kubectl workspace use my-workspace # short-hand for the use syntax kubectl workspace my-workspace # enter a given absolute workspace kubectl workspace root:default:my-workspace # enter the parent workspace kubectl workspace .. # enter the previous workspace kubectl workspace - # go to your home workspace kubectl workspace # create a workspace and immediately enter it kubectl workspace create my-workspace --enter # create a context with the current workspace, e.g. root:default:my-workspace kubectl workspace create-context # create a context with the current workspace, named context-name kubectl workspace create-context context-name","title":"Examples"},{"location":"content/en/reference/cli/kcp_workspace/#options","text":"--as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use -h, --help help for workspace --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --kubeconfig string path to the kubeconfig file -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --short Print only the name of the workspace, e.g. for integration into the shell prompt --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options"},{"location":"content/en/reference/cli/kcp_workspace/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workspace/#see-also","text":"kcp - kubectl plugin for KCP kcp workspace create - Creates a new workspace kcp workspace create-context - Create a kubeconfig context for the current workspace kcp workspace current - Print the current workspace. Same as 'kubectl ws .'. kcp workspace tree - Print the current workspace tree. kcp workspace use - Uses the given workspace as the current workspace. Using - means previous workspace, .. means parent workspace, . mean current, ~ means home workspace","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workspace_create-context/","text":"kcp workspace create-context [<context-name>] [--overwrite] [flags] Examples kcp workspace create-context Options -h, --help help for create-context --kubeconfig string path to the kubeconfig file --overwrite Overwrite the context if it already exists Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp workspace - Manages KCP workspaces","title":"kcp workspace create-context"},{"location":"content/en/reference/cli/kcp_workspace_create-context/#examples","text":"kcp workspace create-context","title":"Examples"},{"location":"content/en/reference/cli/kcp_workspace_create-context/#options","text":"-h, --help help for create-context --kubeconfig string path to the kubeconfig file --overwrite Overwrite the context if it already exists","title":"Options"},{"location":"content/en/reference/cli/kcp_workspace_create-context/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workspace_create-context/#see-also","text":"kcp workspace - Manages KCP workspaces","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workspace_create/","text":"kcp workspace create [flags] Examples kcp workspace create <workspace name> [--type=<type>] [--enter [--ignore-not-ready]] --ignore-existing Options --enter Immediately enter the created workspace -h, --help help for create --ignore-existing Ignore if the workspace already exists. Requires none or absolute type path. --kubeconfig string path to the kubeconfig file --location-selector string A label selector to select the scheduling location of the created workspace. --type string A workspace type. The default type depends on where this child workspace is created. Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp workspace - Manages KCP workspaces","title":"kcp workspace create"},{"location":"content/en/reference/cli/kcp_workspace_create/#examples","text":"kcp workspace create <workspace name> [--type=<type>] [--enter [--ignore-not-ready]] --ignore-existing","title":"Examples"},{"location":"content/en/reference/cli/kcp_workspace_create/#options","text":"--enter Immediately enter the created workspace -h, --help help for create --ignore-existing Ignore if the workspace already exists. Requires none or absolute type path. --kubeconfig string path to the kubeconfig file --location-selector string A label selector to select the scheduling location of the created workspace. --type string A workspace type. The default type depends on where this child workspace is created.","title":"Options"},{"location":"content/en/reference/cli/kcp_workspace_create/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workspace_create/#see-also","text":"kcp workspace - Manages KCP workspaces","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workspace_current/","text":"kcp workspace current [--short] [flags] Examples kcp workspace current Options -h, --help help for current --kubeconfig string path to the kubeconfig file --short Print only the name of the workspace, e.g. for integration into the shell prompt Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp workspace - Manages KCP workspaces","title":"kcp workspace current"},{"location":"content/en/reference/cli/kcp_workspace_current/#examples","text":"kcp workspace current","title":"Examples"},{"location":"content/en/reference/cli/kcp_workspace_current/#options","text":"-h, --help help for current --kubeconfig string path to the kubeconfig file --short Print only the name of the workspace, e.g. for integration into the shell prompt","title":"Options"},{"location":"content/en/reference/cli/kcp_workspace_current/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workspace_current/#see-also","text":"kcp workspace - Manages KCP workspaces","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workspace_tree/","text":"kcp workspace tree [flags] Examples kcp workspace tree Options -f, --full Show full workspace names -h, --help help for tree --kubeconfig string path to the kubeconfig file Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp workspace - Manages KCP workspaces","title":"kcp workspace tree"},{"location":"content/en/reference/cli/kcp_workspace_tree/#examples","text":"kcp workspace tree","title":"Examples"},{"location":"content/en/reference/cli/kcp_workspace_tree/#options","text":"-f, --full Show full workspace names -h, --help help for tree --kubeconfig string path to the kubeconfig file","title":"Options"},{"location":"content/en/reference/cli/kcp_workspace_tree/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workspace_tree/#see-also","text":"kcp workspace - Manages KCP workspaces","title":"SEE ALSO"},{"location":"content/en/reference/cli/kcp_workspace_use/","text":"kcp workspace use <workspace>|..|.|-|~|<root:absolute:workspace> [flags] Options -h, --help help for use --kubeconfig string path to the kubeconfig file --short Print only the name of the workspace, e.g. for integration into the shell prompt Options inherited from parent commands --add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging SEE ALSO kcp workspace - Manages KCP workspaces","title":"kcp workspace use"},{"location":"content/en/reference/cli/kcp_workspace_use/#options","text":"-h, --help help for use --kubeconfig string path to the kubeconfig file --short Print only the name of the workspace, e.g. for integration into the shell prompt","title":"Options"},{"location":"content/en/reference/cli/kcp_workspace_use/#options-inherited-from-parent-commands","text":"--add_dir_header If true, adds the file directory to the header of the log messages --alsologtostderr log to standard error as well as files (no effect when -logtostderr=true) --as-uid string UID to impersonate for the operation --certificate-authority string Path to a cert file for the certificate authority --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory (no effect when -logtostderr=true) --log_file string If non-empty, use this log file (no effect when -logtostderr=true) --log_file_max_size uint Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string If present, the namespace scope for this CLI request --one_output If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true) --password string Password for basic authentication to the API server --proxy-url string If provided, this URL will be used to connect via proxy --server string The address and port of the Kubernetes API server --skip_headers If true, avoid header prefixes in the log messages --skip_log_headers If true, avoid headers when opening log files (no effect when -logtostderr=true) --stderrthreshold severity logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2) --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging","title":"Options inherited from parent commands"},{"location":"content/en/reference/cli/kcp_workspace_use/#see-also","text":"kcp workspace - Manages KCP workspaces","title":"SEE ALSO"},{"location":"content/en/reference/crd/apibindings.apis.kcp.io/","text":"APIBinding CRD schema reference (group apis.kcp.io) {{ page.meta.description }} Full name: apibindings.apis.kcp.io Group: apis.kcp.io Singular name: apibinding Plural name: apibindings Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object Spec holds the desired state. .spec.permissionClaims array permissionClaims records decisions about permission claims requested by the API service provider. Individual claims can be accepted or rejected. If accepted, the API service provider gets the requested access to the specified resources in this workspace. Access is granted per GroupResource, identity, and other properties. .spec.permissionClaims[*] object AcceptablePermissionClaim is a PermissionClaim that records if the user accepts or rejects it. .spec.permissionClaims[*].all boolean all claims all resources for the given group/resource. This is mutually exclusive with resourceSelector. .spec.permissionClaims[*].group string group is the name of an API group. For core groups this is the empty string \u2018\u201c\u201d\u2019. .spec.permissionClaims[*].identityHash string This is the identity for a given APIExport that the APIResourceSchema belongs to. The hash can be found on APIExport and APIResourceSchema\u2019s status. It will be empty for core types. Note that one must look this up for a particular KCP instance. .spec.permissionClaims[*].resource string Required resource is the name of the resource. Note: it is worth noting that you can not ask for permissions for resource provided by a CRD not provided by an api export. .spec.permissionClaims[*].resourceSelector array resourceSelector is a list of claimed resource selectors. .spec.permissionClaims[*].resourceSelector[*] object .spec.permissionClaims[*].resourceSelector[*].name string name of an object within a claimed group/resource. It matches the metadata.name field of the underlying object. If namespace is unset, all objects matching that name will be claimed. .spec.permissionClaims[*].resourceSelector[*].namespace string namespace containing the named object. Matches metadata.namespace field. If \u201cname\u201d is unset, all objects from the namespace are being claimed. .spec.permissionClaims[*].state string Required .spec.reference object Required reference uniquely identifies an API to bind to. .spec.reference.export object export is a reference to an APIExport by cluster name and export name. The creator of the APIBinding needs to have access to the APIExport with the verb bind in order to bind to it. .spec.reference.export.name string Required name is the name of the APIExport that describes the API. .spec.reference.export.path string path is a logical cluster path where the APIExport is defined. If the path is unset, the logical cluster of the APIBinding is used. .status object Status communicates the observed state. .status.apiExportClusterName string APIExportClusterName records the name (not path) of the logical cluster that contains the APIExport. .status.appliedPermissionClaims array appliedPermissionClaims is a list of the permission claims the system has seen and applied, according to the requests of the API service provider in the APIExport and the acceptance state in spec.permissionClaims. .status.appliedPermissionClaims[*] object PermissionClaim identifies an object by GR and identity hash. Its purpose is to determine the added permissions that a service provider may request and that a consumer may accept and allow the service provider access to. .status.appliedPermissionClaims[*].all boolean all claims all resources for the given group/resource. This is mutually exclusive with resourceSelector. .status.appliedPermissionClaims[*].group string group is the name of an API group. For core groups this is the empty string \u2018\u201c\u201d\u2019. .status.appliedPermissionClaims[*].identityHash string This is the identity for a given APIExport that the APIResourceSchema belongs to. The hash can be found on APIExport and APIResourceSchema\u2019s status. It will be empty for core types. Note that one must look this up for a particular KCP instance. .status.appliedPermissionClaims[*].resource string Required resource is the name of the resource. Note: it is worth noting that you can not ask for permissions for resource provided by a CRD not provided by an api export. .status.appliedPermissionClaims[*].resourceSelector array resourceSelector is a list of claimed resource selectors. .status.appliedPermissionClaims[*].resourceSelector[*] object .status.appliedPermissionClaims[*].resourceSelector[*].name string name of an object within a claimed group/resource. It matches the metadata.name field of the underlying object. If namespace is unset, all objects matching that name will be claimed. .status.appliedPermissionClaims[*].resourceSelector[*].namespace string namespace containing the named object. Matches metadata.namespace field. If \u201cname\u201d is unset, all objects from the namespace are being claimed. .status.boundResources array boundResources records the state of bound APIs. .status.boundResources[*] object BoundAPIResource describes a bound GroupVersionResource through an APIResourceSchema of an APIExport.. .status.boundResources[*].group string Required group is the group of the bound API. Empty string for the core API group. .status.boundResources[*].resource string Required resource is the resource of the bound API. kubebuilder:validation:MinLength=1 .status.boundResources[*].schema object Required Schema references the APIResourceSchema that is bound to this API. .status.boundResources[*].schema.UID string Required UID is the UID of the APIResourceSchema that is bound to this API. .status.boundResources[*].schema.identityHash string Required identityHash is the hash of the API identity that this schema is bound to. The API identity determines the etcd prefix used to persist the object. Different identity means that the objects are effectively served and stored under a distinct resource. A CRD of the same GroupVersionResource uses a different identity and hence a separate etcd prefix. .status.boundResources[*].schema.name string Required name is the bound APIResourceSchema name. .status.boundResources[*].storageVersions array storageVersions lists all versions of a resource that were ever persisted. Tracking these versions allows a migration path for stored versions in etcd. The field is mutable so a migration controller can finish a migration to another version (ensuring no old objects are left in storage), and then remove the rest of the versions from this list. Versions may not be removed while they exist in this list. .status.boundResources[*].storageVersions[*] string .status.conditions array conditions is a list of conditions that apply to the APIBinding. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. .status.exportPermissionClaims array exportPermissionClaims records the permissions that the export provider is asking for the binding to grant. .status.exportPermissionClaims[*] object PermissionClaim identifies an object by GR and identity hash. Its purpose is to determine the added permissions that a service provider may request and that a consumer may accept and allow the service provider access to. .status.exportPermissionClaims[*].all boolean all claims all resources for the given group/resource. This is mutually exclusive with resourceSelector. .status.exportPermissionClaims[*].group string group is the name of an API group. For core groups this is the empty string \u2018\u201c\u201d\u2019. .status.exportPermissionClaims[*].identityHash string This is the identity for a given APIExport that the APIResourceSchema belongs to. The hash can be found on APIExport and APIResourceSchema\u2019s status. It will be empty for core types. Note that one must look this up for a particular KCP instance. .status.exportPermissionClaims[*].resource string Required resource is the name of the resource. Note: it is worth noting that you can not ask for permissions for resource provided by a CRD not provided by an api export. .status.exportPermissionClaims[*].resourceSelector array resourceSelector is a list of claimed resource selectors. .status.exportPermissionClaims[*].resourceSelector[*] object .status.exportPermissionClaims[*].resourceSelector[*].name string name of an object within a claimed group/resource. It matches the metadata.name field of the underlying object. If namespace is unset, all objects matching that name will be claimed. .status.exportPermissionClaims[*].resourceSelector[*].namespace string namespace containing the named object. Matches metadata.namespace field. If \u201cname\u201d is unset, all objects from the namespace are being claimed. .status.phase string phase is the current phase of the APIBinding: - \u201c\u201d: the APIBinding has just been created, waiting to be bound. - Binding: the APIBinding is being bound. - Bound: the APIBinding is bound and the referenced APIs are available in the workspace.","title":"APIBinding"},{"location":"content/en/reference/crd/apibindings.apis.kcp.io/#apibinding-crd-schema-reference-group-apiskcpio","text":"{{ page.meta.description }} Full name: apibindings.apis.kcp.io Group: apis.kcp.io Singular name: apibinding Plural name: apibindings Scope: Cluster Versions: v1alpha1","title":"APIBinding CRD schema reference (group apis.kcp.io)"},{"location":"content/en/reference/crd/apiexportendpointslices.apis.kcp.io/","text":"APIExportEndpointSlice CRD schema reference (group apis.kcp.io) {{ page.meta.description }} Full name: apiexportendpointslices.apis.kcp.io Group: apis.kcp.io Singular name: apiexportendpointslice Plural name: apiexportendpointslices Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object spec holds the desired state: - the targetted APIExport - an optional partition for filtering .spec.export object Required export points to the API export. .spec.export.name string Required name is the name of the APIExport that describes the API. .spec.export.path string path is a logical cluster path where the APIExport is defined. If the path is unset, the logical cluster of the APIBinding is used. .spec.partition string partition (optional) points to a partition that is used for filtering the endpoints of the APIExport part of the slice. .status object status communicates the observed state: the filtered list of endpoints for the APIExport service. .status.conditions array conditions is a list of conditions that apply to the APIExportEndpointSlice. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. .status.endpoints array endpoints contains all the URLs of the APIExport service. .status.endpoints[*] object APIExportEndpoint contains the endpoint information of an APIExport service for a specific shard. .status.endpoints[*].url string Required url is an APIExport virtual workspace URL.","title":"APIExportEndpointSlice"},{"location":"content/en/reference/crd/apiexportendpointslices.apis.kcp.io/#apiexportendpointslice-crd-schema-reference-group-apiskcpio","text":"{{ page.meta.description }} Full name: apiexportendpointslices.apis.kcp.io Group: apis.kcp.io Singular name: apiexportendpointslice Plural name: apiexportendpointslices Scope: Cluster Versions: v1alpha1","title":"APIExportEndpointSlice CRD schema reference (group apis.kcp.io)"},{"location":"content/en/reference/crd/apiexports.apis.kcp.io/","text":"APIExport CRD schema reference (group apis.kcp.io) {{ page.meta.description }} Full name: apiexports.apis.kcp.io Group: apis.kcp.io Singular name: apiexport Plural name: apiexports Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object Spec holds the desired state. .spec.identity object identity points to a secret that contains the API identity in the \u2018key\u2019 file. The API identity determines an unique etcd prefix for objects stored via this APIExport. Different APIExport in a workspace can share a common identity, or have different ones. The identity (the secret) can also be transferred to another workspace when the APIExport is moved. The identity is a secret of the API provider. The APIBindings referencing this APIExport will store a derived, non-sensitive value of this identity. The identity of an APIExport cannot be changed. A derived, non-sensitive value of the identity key is stored in the APIExport status and this value is immutable. The identity is defaulted. A secret with the name of the APIExport is automatically created. .spec.identity.secretRef object secretRef is a reference to a secret that contains the API identity in the \u2018key\u2019 file. .spec.identity.secretRef.name string name is unique within a namespace to reference a secret resource. .spec.identity.secretRef.namespace string namespace defines the space within which the secret name must be unique. .spec.latestResourceSchemas array latestResourceSchemas records the latest APIResourceSchemas that are exposed with this APIExport. The schemas can be changed in the life-cycle of the APIExport. These changes have no effect on existing APIBindings, but only on newly bound ones. For updating existing APIBindings, use an APIDeployment keeping bound workspaces up-to-date. .spec.latestResourceSchemas[*] string .spec.maximalPermissionPolicy object maximalPermissionPolicy will allow for a service provider to set an upper bound on what is allowed for a consumer of this API. If the policy is not set, no upper bound is applied, i.e the consuming users can do whatever the user workspace allows the user to do. The policy consists of RBAC (Cluster)Roles and (Cluster)Bindings. A request of a user in a workspace that binds to this APIExport via an APIBinding is additionally checked against these rules, with the user name and the groups prefixed with apis.kcp.io:binding: . For example: assume a user adam with groups system:authenticated and a-team binds to this APIExport in another workspace root:org:ws. Then a request in that workspace against a resource of this APIExport is authorized as every other request in that workspace, but in addition the RBAC policy here in the APIExport workspace has to grant access to the user apis.kcp.io:binding:adam with the groups apis.kcp.io:binding:system:authenticated and apis.kcp.io:binding:a-team . .spec.maximalPermissionPolicy.local object local is the policy that is defined in same workspace as the API Export. .spec.permissionClaims array permissionClaims make resources available in APIExport\u2019s virtual workspace that are not part of the actual APIExport resources. PermissionClaims are optional and should be the least access necessary to complete the functions that the service provider needs. Access is asked for on a GroupResource + identity basis. PermissionClaims must be accepted by the user\u2019s explicit acknowledgement. Hence, when claims change, the respecting objects are not visible immediately. PermissionClaims overlapping with the APIExport resources are ignored. .spec.permissionClaims[*] object PermissionClaim identifies an object by GR and identity hash. Its purpose is to determine the added permissions that a service provider may request and that a consumer may accept and allow the service provider access to. .spec.permissionClaims[*].all boolean all claims all resources for the given group/resource. This is mutually exclusive with resourceSelector. .spec.permissionClaims[*].group string group is the name of an API group. For core groups this is the empty string \u2018\u201c\u201d\u2019. .spec.permissionClaims[*].identityHash string This is the identity for a given APIExport that the APIResourceSchema belongs to. The hash can be found on APIExport and APIResourceSchema\u2019s status. It will be empty for core types. Note that one must look this up for a particular KCP instance. .spec.permissionClaims[*].resource string Required resource is the name of the resource. Note: it is worth noting that you can not ask for permissions for resource provided by a CRD not provided by an api export. .spec.permissionClaims[*].resourceSelector array resourceSelector is a list of claimed resource selectors. .spec.permissionClaims[*].resourceSelector[*] object .spec.permissionClaims[*].resourceSelector[*].name string name of an object within a claimed group/resource. It matches the metadata.name field of the underlying object. If namespace is unset, all objects matching that name will be claimed. .spec.permissionClaims[*].resourceSelector[*].namespace string namespace containing the named object. Matches metadata.namespace field. If \u201cname\u201d is unset, all objects from the namespace are being claimed. .status object Status communicates the observed state. .status.conditions array conditions is a list of conditions that apply to the APIExport. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. .status.identityHash string identityHash is the hash of the API identity key of this APIExport. This value is immutable as soon as it is set. .status.virtualWorkspaces array virtualWorkspaces contains all APIExport virtual workspace URLs. Deprecated: use APIExportEndpointSlice.status.endpoints instead .status.virtualWorkspaces[*] object .status.virtualWorkspaces[*].url string Required url is an APIExport virtual workspace URL.","title":"APIExport"},{"location":"content/en/reference/crd/apiexports.apis.kcp.io/#apiexport-crd-schema-reference-group-apiskcpio","text":"{{ page.meta.description }} Full name: apiexports.apis.kcp.io Group: apis.kcp.io Singular name: apiexport Plural name: apiexports Scope: Cluster Versions: v1alpha1","title":"APIExport CRD schema reference (group apis.kcp.io)"},{"location":"content/en/reference/crd/apiresourceimports.apiresource.kcp.io/","text":"APIResourceImport CRD schema reference (group apiresource.kcp.io) {{ page.meta.description }} Full name: apiresourceimports.apiresource.kcp.io Group: apiresource.kcp.io Singular name: apiresourceimport Plural name: apiresourceimports Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object APIResourceImportSpec holds the desired state of the APIResourceImport (from the client). .spec.categories array categories is a list of grouped resources this custom resource belongs to (e.g. \u2018all\u2019). This is published in API discovery documents, and used by clients to support invocations like kubectl get all . .spec.categories[*] string .spec.columnDefinitions array .spec.columnDefinitions[*] object .spec.columnDefinitions[*].description string Required description is a human readable description of this column. .spec.columnDefinitions[*].format string Required format is an optional OpenAPI type modifier for this column. A format modifies the type and imposes additional rules, like date or time formatting for a string. The \u2018name\u2019 format is applied to the primary identifier column which has type \u2018string\u2019 to assist in clients identifying column is the resource name. See https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#data-types for more. .spec.columnDefinitions[*].jsonPath string Required .spec.columnDefinitions[*].name string Required name is a human readable name for the column. .spec.columnDefinitions[*].priority integer Required priority is an integer defining the relative importance of this column compared to others. Lower numbers are considered higher priority. Columns that may be omitted in limited space scenarios should be given a higher priority. .spec.columnDefinitions[*].type string Required type is an OpenAPI type definition for this column, such as number, integer, string, or array. See https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#data-types for more. .spec.groupVersion object Required .spec.groupVersion.group string .spec.groupVersion.version string Required .spec.kind string Required kind is the serialized kind of the resource. It is normally CamelCase and singular. Custom resource instances will use this value as the kind attribute in API calls. .spec.listKind string listKind is the serialized kind of the list for this resource. Defaults to \u201c kind List\u201d. .spec.location string Required Locaton the API resource is imported from This field is required .spec.openAPIV3Schema object Required .spec.plural string Required plural is the plural name of the resource to serve. The custom resources are served under /apis/<group>/<version>/.../<plural> . Must match the name of the CustomResourceDefinition (in the form <names.plural>.<group> ). Must be all lowercase. .spec.schemaUpdateStrategy string SchemaUpdateStrategy defines the schema update strategy for this API Resource import. Default value is UpdateUnpublished .spec.scope string Required ResourceScope is an enum defining the different scopes available to a custom resource .spec.shortNames array shortNames are short names for the resource, exposed in API discovery documents, and used by clients to support invocations like kubectl get <shortname> . It must be all lowercase. .spec.shortNames[*] string .spec.singular string singular is the singular name of the resource. It must be all lowercase. Defaults to lowercased kind . .spec.subResources array .spec.subResources[*] object .spec.subResources[*].name string Required .status object APIResourceImportStatus communicates the observed state of the APIResourceImport (from the controller). .status.conditions array .status.conditions[*] object APIResourceImportCondition contains details for the current condition of this negotiated api resource. .status.conditions[*].lastTransitionTime string Last time the condition transitioned from one status to another. .status.conditions[*].message string Human-readable message indicating details about last transition. .status.conditions[*].reason string Unique, one-word, CamelCase reason for the condition\u2019s last transition. .status.conditions[*].status string Required Status is the status of the condition. Can be True, False, Unknown. .status.conditions[*].type string Required Type is the type of the condition. Types include Compatible.","title":"APIResourceImport"},{"location":"content/en/reference/crd/apiresourceimports.apiresource.kcp.io/#apiresourceimport-crd-schema-reference-group-apiresourcekcpio","text":"{{ page.meta.description }} Full name: apiresourceimports.apiresource.kcp.io Group: apiresource.kcp.io Singular name: apiresourceimport Plural name: apiresourceimports Scope: Cluster Versions: v1alpha1","title":"APIResourceImport CRD schema reference (group apiresource.kcp.io)"},{"location":"content/en/reference/crd/apiresourceschemas.apis.kcp.io/","text":"APIResourceSchema CRD schema reference (group apis.kcp.io) {{ page.meta.description }} Full name: apiresourceschemas.apis.kcp.io Group: apis.kcp.io Singular name: apiresourceschema Plural name: apiresourceschemas Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object Spec holds the desired state. .spec.group string Required group is the API group of the defined custom resource. Empty string means the core API group. The resources are served under /apis/<group>/... or /api for the core group. .spec.names object Required names specify the resource and kind names for the custom resource. .spec.names.categories array categories is a list of grouped resources this custom resource belongs to (e.g. \u2018all\u2019). This is published in API discovery documents, and used by clients to support invocations like kubectl get all . .spec.names.categories[*] string .spec.names.kind string Required kind is the serialized kind of the resource. It is normally CamelCase and singular. Custom resource instances will use this value as the kind attribute in API calls. .spec.names.listKind string listKind is the serialized kind of the list for this resource. Defaults to \u201c kind List\u201d. .spec.names.plural string Required plural is the plural name of the resource to serve. The custom resources are served under /apis/<group>/<version>/.../<plural> . Must match the name of the CustomResourceDefinition (in the form <names.plural>.<group> ). Must be all lowercase. .spec.names.shortNames array shortNames are short names for the resource, exposed in API discovery documents, and used by clients to support invocations like kubectl get <shortname> . It must be all lowercase. .spec.names.shortNames[*] string .spec.names.singular string singular is the singular name of the resource. It must be all lowercase. Defaults to lowercased kind . .spec.scope string Required scope indicates whether the defined custom resource is cluster- or namespace-scoped. Allowed values are Cluster and Namespaced . .spec.versions array Required versions is the API version of the defined custom resource. Note: the OpenAPI v3 schemas must be equal for all versions until CEL version migration is supported. .spec.versions[*] object APIResourceVersion describes one API version of a resource. .spec.versions[*].additionalPrinterColumns array additionalPrinterColumns specifies additional columns returned in Table output. See https://kubernetes.io/docs/reference/using-api/api-concepts/#receiving-resources-as-tables for details. If no columns are specified, a single column displaying the age of the custom resource is used. .spec.versions[*].additionalPrinterColumns[*] object CustomResourceColumnDefinition specifies a column for server side printing. .spec.versions[*].additionalPrinterColumns[*].description string description is a human readable description of this column. .spec.versions[*].additionalPrinterColumns[*].format string format is an optional OpenAPI type definition for this column. The \u2018name\u2019 format is applied to the primary identifier column to assist in clients identifying column is the resource name. See https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#data-types for details. .spec.versions[*].additionalPrinterColumns[*].jsonPath string Required jsonPath is a simple JSON path (i.e. with array notation) which is evaluated against each custom resource to produce the value for this column. .spec.versions[*].additionalPrinterColumns[*].name string Required name is a human readable name for the column. .spec.versions[*].additionalPrinterColumns[*].priority integer priority is an integer defining the relative importance of this column compared to others. Lower numbers are considered higher priority. Columns that may be omitted in limited space scenarios should be given a priority greater than 0. .spec.versions[*].additionalPrinterColumns[*].type string Required type is an OpenAPI type definition for this column. See https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#data-types for details. .spec.versions[*].deprecated boolean deprecated indicates this version of the custom resource API is deprecated. When set to true, API requests to this version receive a warning header in the server response. Defaults to false. .spec.versions[*].deprecationWarning string deprecationWarning overrides the default warning returned to API clients. May only be set when deprecated is true. The default warning indicates this version is deprecated and recommends use of the newest served version of equal or greater stability, if one exists. .spec.versions[*].name string Required name is the version name, e.g. \u201cv1\u201d, \u201cv2beta1\u201d, etc. The custom resources are served under this version at /apis/<group>/<version>/... if served is true. .spec.versions[*].schema object Required schema describes the structural schema used for validation, pruning, and defaulting of this version of the custom resource. .spec.versions[*].served boolean Required served is a flag enabling/disabling this version from being served via REST APIs .spec.versions[*].storage boolean Required storage indicates this version should be used when persisting custom resources to storage. There must be exactly one version with storage=true. .spec.versions[*].subresources object subresources specify what subresources this version of the defined custom resource have. .spec.versions[*].subresources.scale object scale indicates the custom resource should serve a /scale subresource that returns an autoscaling/v1 Scale object. .spec.versions[*].subresources.scale.labelSelectorPath string labelSelectorPath defines the JSON path inside of a custom resource that corresponds to Scale status.selector . Only JSON paths without the array notation are allowed. Must be a JSON Path under .status or .spec . Must be set to work with HorizontalPodAutoscaler. The field pointed by this JSON path must be a string field (not a complex selector struct) which contains a serialized label selector in string form. More info: https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions#scale-subresource If there is no value under the given path in the custom resource, the status.selector value in the /scale subresource will default to the empty string. .spec.versions[*].subresources.scale.specReplicasPath string Required specReplicasPath defines the JSON path inside of a custom resource that corresponds to Scale spec.replicas . Only JSON paths without the array notation are allowed. Must be a JSON Path under .spec . If there is no value under the given path in the custom resource, the /scale subresource will return an error on GET. .spec.versions[*].subresources.scale.statusReplicasPath string Required statusReplicasPath defines the JSON path inside of a custom resource that corresponds to Scale status.replicas . Only JSON paths without the array notation are allowed. Must be a JSON Path under .status . If there is no value under the given path in the custom resource, the status.replicas value in the /scale subresource will default to 0. .spec.versions[*].subresources.status object status indicates the custom resource should serve a /status subresource. When enabled: 1. requests to the custom resource primary endpoint ignore changes to the status stanza of the object. 2. requests to the custom resource /status subresource ignore changes to anything other than the status stanza of the object.","title":"APIResourceSchema"},{"location":"content/en/reference/crd/apiresourceschemas.apis.kcp.io/#apiresourceschema-crd-schema-reference-group-apiskcpio","text":"{{ page.meta.description }} Full name: apiresourceschemas.apis.kcp.io Group: apis.kcp.io Singular name: apiresourceschema Plural name: apiresourceschemas Scope: Cluster Versions: v1alpha1","title":"APIResourceSchema CRD schema reference (group apis.kcp.io)"},{"location":"content/en/reference/crd/locations.scheduling.kcp.io/","text":"Location CRD schema reference (group scheduling.kcp.io) {{ page.meta.description }} Full name: locations.scheduling.kcp.io Group: scheduling.kcp.io Singular name: location Plural name: locations Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object LocationSpec holds the desired state of the Location. .spec.availableSelectorLabels array availableSelectorLabels is a list of labels that can be used to select an instance at this location in a placement object. .spec.availableSelectorLabels[*] object AvailableSelectorLabel specifies a label with key name and possible values. .spec.availableSelectorLabels[*].description string description is a human readable description of the label. .spec.availableSelectorLabels[*].key string Required key is the name of the label. .spec.availableSelectorLabels[*].values array Required values are the possible values for this labels. .spec.availableSelectorLabels[*].values[*] string LabelValue specifies a value of a label. .spec.description string description is a human-readable description of the location. .spec.instanceSelector object instanceSelector chooses the instances that will be part of this location. Note that these labels are not what is shown in the Location objects to the user. Depending on context, both will match or won\u2019t match. .spec.instanceSelector.matchExpressions array matchExpressions is a list of label selector requirements. The requirements are ANDed. .spec.instanceSelector.matchExpressions[*] object A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. .spec.instanceSelector.matchExpressions[*].key string Required key is the label key that the selector applies to. .spec.instanceSelector.matchExpressions[*].operator string Required operator represents a key\u2019s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. .spec.instanceSelector.matchExpressions[*].values array values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. .spec.instanceSelector.matchExpressions[*].values[*] string .spec.instanceSelector.matchLabels object matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \u201ckey\u201d, the operator is \u201cIn\u201d, and the values array contains only \u201cvalue\u201d. The requirements are ANDed. .spec.resource object Required resource is the group-version-resource of the instances that are subject to this location. .spec.resource.group string group is the name of an API group. .spec.resource.resource string Required resource is the name of the resource. .spec.resource.version string Required version is the version of the API. .status object LocationStatus defines the observed state of Location. .status.availableInstances integer available is the number of actual instances that are available at this location. .status.instances integer instances is the number of actual instances at this location.","title":"Location"},{"location":"content/en/reference/crd/locations.scheduling.kcp.io/#location-crd-schema-reference-group-schedulingkcpio","text":"{{ page.meta.description }} Full name: locations.scheduling.kcp.io Group: scheduling.kcp.io Singular name: location Plural name: locations Scope: Cluster Versions: v1alpha1","title":"Location CRD schema reference (group scheduling.kcp.io)"},{"location":"content/en/reference/crd/logicalclusters.core.kcp.io/","text":"LogicalCluster CRD schema reference (group core.kcp.io) {{ page.meta.description }} Full name: logicalclusters.core.kcp.io Group: core.kcp.io Singular name: logicalcluster Plural name: logicalclusters Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .metadata.name string .spec object LogicalClusterSpec is the specification of the LogicalCluster resource. .spec.directlyDeletable boolean DirectlyDeletable indicates that this logical cluster can be directly deleted by the user from within by deleting the LogicalCluster object. .spec.initializers array initializers are set on creation by the system and copied to status when initialization starts. .spec.initializers[*] string LogicalClusterInitializer is a unique string corresponding to a logical cluster initialization controller. .spec.owner object owner is a reference to a resource controlling the life-cycle of this logical cluster. On deletion of the LogicalCluster, the finalizer core.kcp.io/logicalcluster is removed from the owner. When this object is deleted, but the owner is not deleted, the owner is deleted too. .spec.owner.apiVersion string Required apiVersion is the group and API version of the owner. .spec.owner.cluster string Required cluster is the logical cluster in which the owner is located. .spec.owner.name string Required name is the name of the owner. .spec.owner.namespace string namespace is the optional namespace of the owner. .spec.owner.resource string Required resource is API resource to access the owner. .spec.owner.uid string Required UID is the UID of the owner. .status object LogicalClusterStatus communicates the observed state of the Workspace. .status.URL string url is the address under which the Kubernetes-cluster-like endpoint can be found. This URL can be used to access the logical cluster with standard Kubernetes client libraries and command line tools. .status.conditions array Current processing state of the LogicalCluster. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. .status.initializers array initializers are set on creation by the system and must be cleared by a controller before the logical cluster can be used. The LogicalCluster object will stay in the phase \u201cInitializing\u201d state until all initializers are cleared. .status.initializers[*] string LogicalClusterInitializer is a unique string corresponding to a logical cluster initialization controller. .status.phase string Phase of the logical cluster (Initializing, Ready).","title":"LogicalCluster"},{"location":"content/en/reference/crd/logicalclusters.core.kcp.io/#logicalcluster-crd-schema-reference-group-corekcpio","text":"{{ page.meta.description }} Full name: logicalclusters.core.kcp.io Group: core.kcp.io Singular name: logicalcluster Plural name: logicalclusters Scope: Cluster Versions: v1alpha1","title":"LogicalCluster CRD schema reference (group core.kcp.io)"},{"location":"content/en/reference/crd/negotiatedapiresources.apiresource.kcp.io/","text":"NegotiatedAPIResource CRD schema reference (group apiresource.kcp.io) {{ page.meta.description }} Full name: negotiatedapiresources.apiresource.kcp.io Group: apiresource.kcp.io Singular name: negotiatedapiresource Plural name: negotiatedapiresources Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object NegotiatedAPIResourceSpec holds the desired state of the NegotiatedAPIResource (from the client). .spec.categories array categories is a list of grouped resources this custom resource belongs to (e.g. \u2018all\u2019). This is published in API discovery documents, and used by clients to support invocations like kubectl get all . .spec.categories[*] string .spec.columnDefinitions array .spec.columnDefinitions[*] object .spec.columnDefinitions[*].description string Required description is a human readable description of this column. .spec.columnDefinitions[*].format string Required format is an optional OpenAPI type modifier for this column. A format modifies the type and imposes additional rules, like date or time formatting for a string. The \u2018name\u2019 format is applied to the primary identifier column which has type \u2018string\u2019 to assist in clients identifying column is the resource name. See https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#data-types for more. .spec.columnDefinitions[*].jsonPath string Required .spec.columnDefinitions[*].name string Required name is a human readable name for the column. .spec.columnDefinitions[*].priority integer Required priority is an integer defining the relative importance of this column compared to others. Lower numbers are considered higher priority. Columns that may be omitted in limited space scenarios should be given a higher priority. .spec.columnDefinitions[*].type string Required type is an OpenAPI type definition for this column, such as number, integer, string, or array. See https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#data-types for more. .spec.groupVersion object Required .spec.groupVersion.group string .spec.groupVersion.version string Required .spec.kind string Required kind is the serialized kind of the resource. It is normally CamelCase and singular. Custom resource instances will use this value as the kind attribute in API calls. .spec.listKind string listKind is the serialized kind of the list for this resource. Defaults to \u201c kind List\u201d. .spec.openAPIV3Schema object Required .spec.plural string Required plural is the plural name of the resource to serve. The custom resources are served under /apis/<group>/<version>/.../<plural> . Must match the name of the CustomResourceDefinition (in the form <names.plural>.<group> ). Must be all lowercase. .spec.publish boolean .spec.scope string Required ResourceScope is an enum defining the different scopes available to a custom resource .spec.shortNames array shortNames are short names for the resource, exposed in API discovery documents, and used by clients to support invocations like kubectl get <shortname> . It must be all lowercase. .spec.shortNames[*] string .spec.singular string singular is the singular name of the resource. It must be all lowercase. Defaults to lowercased kind . .spec.subResources array .spec.subResources[*] object .spec.subResources[*].name string Required .status object NegotiatedAPIResourceStatus communicates the observed state of the NegotiatedAPIResource (from the controller). .status.conditions array .status.conditions[*] object NegotiatedAPIResourceCondition contains details for the current condition of this negotiated api resource. .status.conditions[*].lastTransitionTime string Last time the condition transitioned from one status to another. .status.conditions[*].message string Human-readable message indicating details about last transition. .status.conditions[*].reason string Unique, one-word, CamelCase reason for the condition\u2019s last transition. .status.conditions[*].status string Required Status is the status of the condition. Can be True, False, Unknown. .status.conditions[*].type string Required Type is the type of the condition. Types include Submitted, Published, Refused and Enforced.","title":"NegotiatedAPIResource"},{"location":"content/en/reference/crd/negotiatedapiresources.apiresource.kcp.io/#negotiatedapiresource-crd-schema-reference-group-apiresourcekcpio","text":"{{ page.meta.description }} Full name: negotiatedapiresources.apiresource.kcp.io Group: apiresource.kcp.io Singular name: negotiatedapiresource Plural name: negotiatedapiresources Scope: Cluster Versions: v1alpha1","title":"NegotiatedAPIResource CRD schema reference (group apiresource.kcp.io)"},{"location":"content/en/reference/crd/partitions.topology.kcp.io/","text":"Partition CRD schema reference (group topology.kcp.io) {{ page.meta.description }} Full name: partitions.topology.kcp.io Group: topology.kcp.io Singular name: partition Plural name: partitions Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object spec holds the desired state. .spec.selector object selector (optional) is a label selector that filters shard targets. .spec.selector.matchExpressions array matchExpressions is a list of label selector requirements. The requirements are ANDed. .spec.selector.matchExpressions[*] object A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. .spec.selector.matchExpressions[*].key string Required key is the label key that the selector applies to. .spec.selector.matchExpressions[*].operator string Required operator represents a key\u2019s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. .spec.selector.matchExpressions[*].values array values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. .spec.selector.matchExpressions[*].values[*] string .spec.selector.matchLabels object matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \u201ckey\u201d, the operator is \u201cIn\u201d, and the values array contains only \u201cvalue\u201d. The requirements are ANDed.","title":"Partition"},{"location":"content/en/reference/crd/partitions.topology.kcp.io/#partition-crd-schema-reference-group-topologykcpio","text":"{{ page.meta.description }} Full name: partitions.topology.kcp.io Group: topology.kcp.io Singular name: partition Plural name: partitions Scope: Cluster Versions: v1alpha1","title":"Partition CRD schema reference (group topology.kcp.io)"},{"location":"content/en/reference/crd/partitionsets.topology.kcp.io/","text":"PartitionSet CRD schema reference (group topology.kcp.io) {{ page.meta.description }} Full name: partitionsets.topology.kcp.io Group: topology.kcp.io Singular name: partitionset Plural name: partitionsets Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object spec holds the desired state. .spec.dimensions array dimensions (optional) are used to group shards into partitions .spec.dimensions[*] string .spec.shardSelector object shardSelector (optional) specifies filtering for shard targets. .spec.shardSelector.matchExpressions array matchExpressions is a list of label selector requirements. The requirements are ANDed. .spec.shardSelector.matchExpressions[*] object A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. .spec.shardSelector.matchExpressions[*].key string Required key is the label key that the selector applies to. .spec.shardSelector.matchExpressions[*].operator string Required operator represents a key\u2019s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. .spec.shardSelector.matchExpressions[*].values array values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. .spec.shardSelector.matchExpressions[*].values[*] string .spec.shardSelector.matchLabels object matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \u201ckey\u201d, the operator is \u201cIn\u201d, and the values array contains only \u201cvalue\u201d. The requirements are ANDed. .status object status holds information about the current status .status.conditions array conditions is a list of conditions that apply to the APIExportEndpointSlice. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. .status.count integer count is the total number of partitions.","title":"PartitionSet"},{"location":"content/en/reference/crd/partitionsets.topology.kcp.io/#partitionset-crd-schema-reference-group-topologykcpio","text":"{{ page.meta.description }} Full name: partitionsets.topology.kcp.io Group: topology.kcp.io Singular name: partitionset Plural name: partitionsets Scope: Cluster Versions: v1alpha1","title":"PartitionSet CRD schema reference (group topology.kcp.io)"},{"location":"content/en/reference/crd/placements.scheduling.kcp.io/","text":"Placement CRD schema reference (group scheduling.kcp.io) {{ page.meta.description }} Full name: placements.scheduling.kcp.io Group: scheduling.kcp.io Singular name: placement Plural name: placements Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object .spec.locationResource object Required locationResource is the group-version-resource of the instances that are subject to the locations to select. .spec.locationResource.group string group is the name of an API group. .spec.locationResource.resource string Required resource is the name of the resource. .spec.locationResource.version string Required version is the version of the API. .spec.locationSelectors array locationSelectors represents a slice of label selector to select a location, these label selectors are logically ORed. .spec.locationSelectors[*] object A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects. .spec.locationSelectors[*].matchExpressions array matchExpressions is a list of label selector requirements. The requirements are ANDed. .spec.locationSelectors[*].matchExpressions[*] object A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. .spec.locationSelectors[*].matchExpressions[*].key string Required key is the label key that the selector applies to. .spec.locationSelectors[*].matchExpressions[*].operator string Required operator represents a key\u2019s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. .spec.locationSelectors[*].matchExpressions[*].values array values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. .spec.locationSelectors[*].matchExpressions[*].values[*] string .spec.locationSelectors[*].matchLabels object matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \u201ckey\u201d, the operator is \u201cIn\u201d, and the values array contains only \u201cvalue\u201d. The requirements are ANDed. .spec.locationWorkspace string locationWorkspace is an absolute reference to a workspace for the location. If it is not set, the workspace of APIBinding will be used. .spec.namespaceSelector object namespaceSelector is a label selector to select ns. It match all ns by default, but can be specified to a certain set of ns. .spec.namespaceSelector.matchExpressions array matchExpressions is a list of label selector requirements. The requirements are ANDed. .spec.namespaceSelector.matchExpressions[*] object A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. .spec.namespaceSelector.matchExpressions[*].key string Required key is the label key that the selector applies to. .spec.namespaceSelector.matchExpressions[*].operator string Required operator represents a key\u2019s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. .spec.namespaceSelector.matchExpressions[*].values array values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. .spec.namespaceSelector.matchExpressions[*].values[*] string .spec.namespaceSelector.matchLabels object matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \u201ckey\u201d, the operator is \u201cIn\u201d, and the values array contains only \u201cvalue\u201d. The requirements are ANDed. .status object .status.conditions array Current processing state of the Placement. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. .status.phase string phase is the current phase of the placement .status.selectedLocation object selectedLocation is the location that a picked by this placement. .status.selectedLocation.locationName string Required Name of the Location. .status.selectedLocation.path string Required path is an absolute reference to a workspace, e.g. root:org:ws. The workspace must be some ancestor or a child of some ancestor.","title":"Placement"},{"location":"content/en/reference/crd/placements.scheduling.kcp.io/#placement-crd-schema-reference-group-schedulingkcpio","text":"{{ page.meta.description }} Full name: placements.scheduling.kcp.io Group: scheduling.kcp.io Singular name: placement Plural name: placements Scope: Cluster Versions: v1alpha1","title":"Placement CRD schema reference (group scheduling.kcp.io)"},{"location":"content/en/reference/crd/shards.core.kcp.io/","text":"Shard CRD schema reference (group core.kcp.io) {{ page.meta.description }} Full name: shards.core.kcp.io Group: core.kcp.io Singular name: shard Plural name: shards Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .spec object ShardSpec holds the desired state of the Shard. .spec.baseURL string Required baseURL is the address of the KCP shard for direct connections, e.g. by some front-proxy doing the fan-out to the shards. .spec.externalURL string externalURL is the externally visible address presented to users in Workspace URLs. Changing this will break all existing logical clusters on that shard, i.e. existing kubeconfigs of clients will be invalid. Hence, when changing this value, the old URL used by clients must keep working. The external address will not be unique if a front-proxy does a fan-out to shards, but all logical cluster clients will talk to the front-proxy. In that case, put the address of the front-proxy here. Note that movement of shards is only possible (in the future) between shards that share a common external URL. This will be defaulted to the value of the baseURL. .spec.virtualWorkspaceURL string virtualWorkspaceURL is the address of the virtual workspace apiserver associated with this shard. It can be a direct address, an address of a front-proxy or even an address of an LB. As of today this address is assigned to APIExports. This will be defaulted to the value of the baseURL. .status object ShardStatus communicates the observed state of the Shard. .status.capacity object Set of integer resources that logical clusters can be scheduled into .status.conditions array Current processing state of the Shard. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important.","title":"Shard"},{"location":"content/en/reference/crd/shards.core.kcp.io/#shard-crd-schema-reference-group-corekcpio","text":"{{ page.meta.description }} Full name: shards.core.kcp.io Group: core.kcp.io Singular name: shard Plural name: shards Scope: Cluster Versions: v1alpha1","title":"Shard CRD schema reference (group core.kcp.io)"},{"location":"content/en/reference/crd/synctargets.workload.kcp.io/","text":"SyncTarget CRD schema reference (group workload.kcp.io) {{ page.meta.description }} Full name: synctargets.workload.kcp.io Group: workload.kcp.io Singular name: synctarget Plural name: synctargets Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .metadata.name string .spec object Spec holds the desired state. .spec.cells object Cells is a set of labels to identify the cells the SyncTarget belongs to. SyncTargets with the same cells run as they are in the same physical cluster. Each key/value pair in the cells should be added and updated by service providers (i.e. a network provider updates one key/value, while the storage provider updates another.) .spec.evictAfter string EvictAfter controls cluster schedulability of new and existing workloads. After the EvictAfter time, any workload scheduled to the cluster will be unassigned from the cluster. By default, workloads scheduled to the cluster are not evicted. .spec.supportedAPIExports array SupportedAPIExports defines a set of APIExports supposed to be supported by this SyncTarget. The SyncTarget will be selected to deploy the workload only when the resource schema on the SyncTarget is compatible with the resource schema included in the exports. If it is not set, the kubernetes export in the same workspace will be used by default. .spec.supportedAPIExports[*] object APIExportReference provides the fields necessary to resolve an APIExport. .spec.supportedAPIExports[*].export string Required export is the name of the APIExport. .spec.supportedAPIExports[*].path string path is the fully-qualified path to the workspace containing the APIExport. If it is empty, the current workspace is assumed. .spec.unschedulable boolean Unschedulable controls cluster schedulability of new workloads. By default, cluster is schedulable. .status object Status communicates the observed state. .status.allocatable object Allocatable represents the resources that are available for scheduling. .status.capacity object Capacity represents the total resources of the cluster. .status.conditions array Current processing state of the SyncTarget. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. .status.lastSyncerHeartbeatTime string A timestamp indicating when the syncer last reported status. .status.syncedResources array SyncedResources represents the resources that the syncer of the SyncTarget can sync. It MUST be updated by kcp server. .status.syncedResources[*] object .status.syncedResources[*].group string group is the name of an API group. For core groups this is the empty string \u2018\u201c\u201d\u2019. .status.syncedResources[*].identityHash string identityHash is the identity for a given APIExport that the APIResourceSchema belongs to. The hash can be found on APIExport and APIResourceSchema\u2019s status. It will be empty for core types. .status.syncedResources[*].resource string Required resource is the name of the resource. Note: it is worth noting that you can not ask for permissions for resource provided by a CRD not provided by an api export. .status.syncedResources[*].state string state indicate whether the resources schema is compatible to the SyncTarget. It must be updated by syncer after checking the API compatibility on SyncTarget. .status.syncedResources[*].versions array Required versions are the resource versions the syncer can choose to sync depending on availability on the downstream cluster. Conversion to the storage version, if necessary, will be done on the kcp side. The versions are ordered by precedence and the first version compatible is preferred by syncer. .status.syncedResources[*].versions[*] string .status.virtualWorkspaces array VirtualWorkspaces contains all virtual workspace URLs. .status.virtualWorkspaces[*] object .status.virtualWorkspaces[*].syncerURL string Required SyncerURL is the URL of the syncer virtual workspace. .status.virtualWorkspaces[*].upsyncerURL string Required UpsyncerURL is the URL of the upsyncer virtual workspace.","title":"SyncTarget"},{"location":"content/en/reference/crd/synctargets.workload.kcp.io/#synctarget-crd-schema-reference-group-workloadkcpio","text":"{{ page.meta.description }} Full name: synctargets.workload.kcp.io Group: workload.kcp.io Singular name: synctarget Plural name: synctargets Scope: Cluster Versions: v1alpha1","title":"SyncTarget CRD schema reference (group workload.kcp.io)"},{"location":"content/en/reference/crd/workspaces.tenancy.kcp.io/","text":"Workspace CRD schema reference (group tenancy.kcp.io) {{ page.meta.description }} Full name: workspaces.tenancy.kcp.io Group: tenancy.kcp.io Singular name: workspace Plural name: workspaces Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .metadata.name string .spec object Required WorkspaceSpec holds the desired state of the Workspace. .spec.URL string URL is the address under which the Kubernetes-cluster-like endpoint can be found. This URL can be used to access the workspace with standard Kubernetes client libraries and command line tools. Set by the system. .spec.cluster string cluster is the name of the logical cluster this workspace is stored under. Set by the system. .spec.location object location constraints where this workspace can be scheduled to. If the no location is specified, an arbitrary location is chosen. .spec.location.selector object selector is a label selector that filters workspace scheduling targets. .spec.location.selector.matchExpressions array matchExpressions is a list of label selector requirements. The requirements are ANDed. .spec.location.selector.matchExpressions[*] object A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. .spec.location.selector.matchExpressions[*].key string Required key is the label key that the selector applies to. .spec.location.selector.matchExpressions[*].operator string Required operator represents a key\u2019s relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. .spec.location.selector.matchExpressions[*].values array values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. .spec.location.selector.matchExpressions[*].values[*] string .spec.location.selector.matchLabels object matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \u201ckey\u201d, the operator is \u201cIn\u201d, and the values array contains only \u201cvalue\u201d. The requirements are ANDed. .spec.type object type defines properties of the workspace both on creation (e.g. initial resources and initially installed APIs) and during runtime (e.g. permissions). If no type is provided, the default type for the workspace in which this workspace is nesting will be used. The type is a reference to a WorkspaceType in the listed workspace, but lower-cased. The WorkspaceType existence is validated at admission during creation. The type is immutable after creation. The use of a type is gated via the RBAC workspacetypes/use resource permission. .spec.type.name string Required name is the name of the WorkspaceType .spec.type.path string path is an absolute reference to the workspace that owns this type, e.g. root:org:ws. .status object WorkspaceStatus communicates the observed state of the Workspace. .status.conditions array Current processing state of the Workspace. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. .status.initializers array initializers must be cleared by a controller before the workspace is ready and can be used. .status.initializers[*] string LogicalClusterInitializer is a unique string corresponding to a logical cluster initialization controller. .status.phase string Phase of the workspace (Scheduling, Initializing, Ready).","title":"Workspace"},{"location":"content/en/reference/crd/workspaces.tenancy.kcp.io/#workspace-crd-schema-reference-group-tenancykcpio","text":"{{ page.meta.description }} Full name: workspaces.tenancy.kcp.io Group: tenancy.kcp.io Singular name: workspace Plural name: workspaces Scope: Cluster Versions: v1alpha1","title":"Workspace CRD schema reference (group tenancy.kcp.io)"},{"location":"content/en/reference/crd/workspacetypes.tenancy.kcp.io/","text":"WorkspaceType CRD schema reference (group tenancy.kcp.io) {{ page.meta.description }} Full name: workspacetypes.tenancy.kcp.io Group: tenancy.kcp.io Singular name: workspacetype Plural name: workspacetypes Scope: Cluster Versions: v1alpha1 Version v1alpha1 Properties .apiVersion string APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources .kind string Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds .metadata object .metadata.name string .spec object .spec.additionalWorkspaceLabels object additionalWorkspaceLabels are a set of labels that will be added to a ClusterWorkspace on creation. .spec.defaultAPIBindings array defaultAPIBindings are the APIs to bind during initialization of workspaces created from this type. The APIBinding names will be generated dynamically. .spec.defaultAPIBindings[*] object APIExportReference provides the fields necessary to resolve an APIExport. .spec.defaultAPIBindings[*].export string Required export is the name of the APIExport. .spec.defaultAPIBindings[*].path string path is the fully-qualified path to the workspace containing the APIExport. If it is empty, the current workspace is assumed. .spec.defaultChildWorkspaceType object defaultChildWorkspaceType is the WorkspaceType that will be used by default if another, nested ClusterWorkspace is created in a workspace of this type. When this field is unset, the user must specify a type when creating nested workspaces. Extending another WorkspaceType does not inherit its defaultChildWorkspaceType. .spec.defaultChildWorkspaceType.name string Required name is the name of the WorkspaceType .spec.defaultChildWorkspaceType.path string path is an absolute reference to the workspace that owns this type, e.g. root:org:ws. .spec.extend object extend is a list of other WorkspaceTypes whose initializers and limitAllowedChildren and limitAllowedParents this WorkspaceType is inheriting. By (transitively) extending another WorkspaceType, this WorkspaceType will be considered as that other type in evaluation of limitAllowedChildren and limitAllowedParents constraints. A dependency cycle stop this WorkspaceType from being admitted as the type of a ClusterWorkspace. A non-existing dependency stop this WorkspaceType from being admitted as the type of a ClusterWorkspace. .spec.extend.with array with are WorkspaceTypes whose initializers are added to the list for the owning type, and for whom the owning type becomes an alias, as long as all of their required types are not mentioned in without. .spec.extend.with[*] object WorkspaceTypeReference is a globally unique, fully qualified reference to a workspace type. .spec.extend.with[*].name string Required name is the name of the WorkspaceType .spec.extend.with[*].path string path is an absolute reference to the workspace that owns this type, e.g. root:org:ws. .spec.initializer boolean initializer determines if this WorkspaceType has an associated initializing controller. These controllers are used to add functionality to a ClusterWorkspace; all controllers must finish their work before the ClusterWorkspace becomes ready for use. One initializing controller is supported per WorkspaceType; the identifier for this initializer will be a colon-delimited string using the workspace in which the WorkspaceType is defined, and the type\u2019s name. For example, if a WorkspaceType example is created in the root:org workspace, the implicit initializer name is root:org:Example . .spec.limitAllowedChildren object limitAllowedChildren specifies constraints for sub-workspaces created in workspaces of this type. These are in addition to child constraints of types this one extends. .spec.limitAllowedChildren.none boolean none means that no type matches. .spec.limitAllowedChildren.types array types is a list of WorkspaceTypes that match. A workspace type extending another workspace type automatically is considered as that extended type as well (even transitively). An empty list matches all types. .spec.limitAllowedChildren.types[*] object WorkspaceTypeReference is a globally unique, fully qualified reference to a workspace type. .spec.limitAllowedChildren.types[*].name string Required name is the name of the WorkspaceType .spec.limitAllowedChildren.types[*].path string path is an absolute reference to the workspace that owns this type, e.g. root:org:ws. .spec.limitAllowedParents object limitAllowedParents specifies constraints for the parent workspace that workspaces of this type are created in. These are in addition to parent constraints of types this one extends. .spec.limitAllowedParents.none boolean none means that no type matches. .spec.limitAllowedParents.types array types is a list of WorkspaceTypes that match. A workspace type extending another workspace type automatically is considered as that extended type as well (even transitively). An empty list matches all types. .spec.limitAllowedParents.types[*] object WorkspaceTypeReference is a globally unique, fully qualified reference to a workspace type. .spec.limitAllowedParents.types[*].name string Required name is the name of the WorkspaceType .spec.limitAllowedParents.types[*].path string path is an absolute reference to the workspace that owns this type, e.g. root:org:ws. .status object WorkspaceTypeStatus defines the observed state of WorkspaceType. .status.conditions array conditions is a list of conditions that apply to the APIExport. .status.conditions[*] object Condition defines an observation of a object operational state. .status.conditions[*].lastTransitionTime string Required Last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. .status.conditions[*].message string A human readable message indicating details about the transition. This field may be empty. .status.conditions[*].reason string The reason for the condition\u2019s last transition in CamelCase. The specific API may choose whether or not this field is considered a guaranteed API. This field may not be empty. .status.conditions[*].severity string Severity provides an explicit classification of Reason code, so the users or machines can immediately understand the current situation and act accordingly. The Severity field MUST be set only when Status=False. .status.conditions[*].status string Required Status of the condition, one of True, False, Unknown. .status.conditions[*].type string Required Type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. .status.virtualWorkspaces array virtualWorkspaces contains all APIExport virtual workspace URLs. .status.virtualWorkspaces[*] object .status.virtualWorkspaces[*].url string Required url is a WorkspaceType initialization virtual workspace URL.","title":"WorkspaceType"},{"location":"content/en/reference/crd/workspacetypes.tenancy.kcp.io/#workspacetype-crd-schema-reference-group-tenancykcpio","text":"{{ page.meta.description }} Full name: workspacetypes.tenancy.kcp.io Group: tenancy.kcp.io Singular name: workspacetype Plural name: workspacetypes Scope: Cluster Versions: v1alpha1","title":"WorkspaceType CRD schema reference (group tenancy.kcp.io)"},{"location":"enhancements/0001_project_enhancements/","text":"0001: Project process refinement for 2023 Summary As our project grows we need to scale our collaboration processes to meet the needs of a growing community. The approchability of a project is important to foster continued growth and ensure the mission and, perhaps just as important, non-goals of the project are clearly understood. This enhancement proposes some next steps to take in the refinement of kcp's existing processes. Motivation Over the past year there has been a drive to show the \"realness\" of the kcp project. That investment has been made by contributing enough code to each of the core features of kcp that an early adopter could, with enough drive, integrate with the APIs to understand the value proposition. Optimizing for that goal has allowed quick progress but at the expense of things like documentation. Within kcp there are also a few logical projects that can be separated in order to provide more clarity on the direction of each project and facilitate growing contributors in those areas. In order to keep growing it is essential that the kcp project provides an easily approachable method for consumers to quickly understand the goals of the project, see the current state of features, and run the latest build. It is also important that potential contributors be able to quickly setup their development environment, understand the engagement model for development, and be able to discover the right forums for their area of interest. This document proposes that now is the right time to take these steps in order to facilitate objectives in 2023. Goals Agree on the need for an official enhancement process Agree on the need to split the workloads and control plane areas of interest Agree on the need to create a formal definition of done that includes documentation and testing requirements Agree on the need to identify graduation criteria of existing APIs Assign ownership of the above for definition and implementation Non-Goals Actually define the specifics of each of the goals - this should be up to the community Proposal Implement an enhancement proposal process An enhancement process has many benefits for a project, including: * creating an approchable method to driving change in a project * offering a forum for asynchronous feedback * providing a discoverable history of project change and the motivations behind the changes * setting expectations on the bar for contribution in areas like quality and graduation criteria This proposal suggests that kcp adopt an enhancement process that follows practices familiar to many in the community via the KEP process by tailoring aspects of that process to fit the kcp project's needs. Suggested action items Definition of an enhancement proposal template Creation of an enhancement proposal repository and removal of this proposal from the main repo to that repository. Establishing the approval criteria for said process and the expectations for review and feedback in a way that can be adopted by sub-projects identified later in this proposal. This definition for enhancements should help establish the definition of done for an enhancement that includes any documentation and testing requirements, helping establish clear expectations for authors as well as helping the community (as consumers of kcp) approach new features uniformly. Suggested timeline Completed in: Q1 2023 Of the items in this proposal this process is likely to have the highest impact on our ability to collaborate on new items going forward. Create enhancements for existing APIs to document graduation criteria As part of this proposal it is suggested that the project would benefit from a bit of \"back porting\" of documentation to cover existing APIs. As a minimum bar it is proposed that we: Suggested Action Items Create an enhancement for each existing API in the main repository Identify the current level of the API and the known development path Identify the graduation criteria of the API Link any existing work to the enhancement for the API As an optional follow on, update the kcp.io site to include links to proposals as part of the documentation similar to thanos Present enhancements during community meetings Suggested timeline Completed by: end of Q3 2023 Existing APIs can adopt change via the enhancement proposal as they go. Our understanding of how we may move through alpha, beta, and v1 becomes more clear based on the rate of feedback. If nothing else, by that time documentation should exist that minimizes the need to further document historical decisions. Split the workloads project from the control plane Within kcp there exists the following components that can be thought of as individual investment areas. kcp - the generic control plane with workspaces and API Import and API Export components kcp workloads - the transparent multi-cluster components kcp edge workloads - the transparent multi-cluster components focused on edge use cases kcp controllers dev - the components focused on tooling and development of kcp aware controllers kcp catalog - discover component for published APIs Right now the components that stand out as unnecessarily coupled are the generic control plane and workloads pieces. This proposal suggests that each of these have a clear split. This scoping of responsibility enables a more clear engagement model of where some of these components need to collaborate with other upstream projects who are also interested in the same spaces. Out of scope Separation of the generic control plane from tenancy out of scope for now. It is possible that it is revisited in the future. Known dependencies kcp workloads will depend on kcp (same repo currently) kcp edge workloads may depend on kcp worklaods kcp catalog will depend on kcp Suggested action items new repo be created for kcp workloads source code be moved as approporate optionally, special interest groups may be established to own the processes and discussion around the areas independently, but following the overall project's process model Suggested timeline Level of effort analysis completed in: Q1 2023 Target completion by: end of Q2 2023 Transparent multi-cluster features are under heavy development. An initial level of effort evaluation should be conducted in order to know the scope of this change in order to plan it in a way that is not overly disruptive to continuing on the critical path of any MVP development. Drawbacks May be considered too much, too soon. Slows down the contribution velocity of those already involved Alternatives As an alternative, the project could do nothing and hope that a shared understanding of how contribution happens establishes itself based on a lead-by-example method or by inheriting the established patterns of other communities. This proposal suggests that it is more beneficial to codify these items rather than rely on hope as a strategy.","title":"0001: Project process refinement for 2023"},{"location":"enhancements/0001_project_enhancements/#0001-project-process-refinement-for-2023","text":"","title":"0001: Project process refinement for 2023"},{"location":"enhancements/0001_project_enhancements/#summary","text":"As our project grows we need to scale our collaboration processes to meet the needs of a growing community. The approchability of a project is important to foster continued growth and ensure the mission and, perhaps just as important, non-goals of the project are clearly understood. This enhancement proposes some next steps to take in the refinement of kcp's existing processes.","title":"Summary"},{"location":"enhancements/0001_project_enhancements/#motivation","text":"Over the past year there has been a drive to show the \"realness\" of the kcp project. That investment has been made by contributing enough code to each of the core features of kcp that an early adopter could, with enough drive, integrate with the APIs to understand the value proposition. Optimizing for that goal has allowed quick progress but at the expense of things like documentation. Within kcp there are also a few logical projects that can be separated in order to provide more clarity on the direction of each project and facilitate growing contributors in those areas. In order to keep growing it is essential that the kcp project provides an easily approachable method for consumers to quickly understand the goals of the project, see the current state of features, and run the latest build. It is also important that potential contributors be able to quickly setup their development environment, understand the engagement model for development, and be able to discover the right forums for their area of interest. This document proposes that now is the right time to take these steps in order to facilitate objectives in 2023.","title":"Motivation"},{"location":"enhancements/0001_project_enhancements/#goals","text":"Agree on the need for an official enhancement process Agree on the need to split the workloads and control plane areas of interest Agree on the need to create a formal definition of done that includes documentation and testing requirements Agree on the need to identify graduation criteria of existing APIs Assign ownership of the above for definition and implementation","title":"Goals"},{"location":"enhancements/0001_project_enhancements/#non-goals","text":"Actually define the specifics of each of the goals - this should be up to the community","title":"Non-Goals"},{"location":"enhancements/0001_project_enhancements/#proposal","text":"","title":"Proposal"},{"location":"enhancements/0001_project_enhancements/#implement-an-enhancement-proposal-process","text":"An enhancement process has many benefits for a project, including: * creating an approchable method to driving change in a project * offering a forum for asynchronous feedback * providing a discoverable history of project change and the motivations behind the changes * setting expectations on the bar for contribution in areas like quality and graduation criteria This proposal suggests that kcp adopt an enhancement process that follows practices familiar to many in the community via the KEP process by tailoring aspects of that process to fit the kcp project's needs.","title":"Implement an enhancement proposal process"},{"location":"enhancements/0001_project_enhancements/#suggested-action-items","text":"Definition of an enhancement proposal template Creation of an enhancement proposal repository and removal of this proposal from the main repo to that repository. Establishing the approval criteria for said process and the expectations for review and feedback in a way that can be adopted by sub-projects identified later in this proposal. This definition for enhancements should help establish the definition of done for an enhancement that includes any documentation and testing requirements, helping establish clear expectations for authors as well as helping the community (as consumers of kcp) approach new features uniformly.","title":"Suggested action items"},{"location":"enhancements/0001_project_enhancements/#suggested-timeline","text":"Completed in: Q1 2023 Of the items in this proposal this process is likely to have the highest impact on our ability to collaborate on new items going forward.","title":"Suggested timeline"},{"location":"enhancements/0001_project_enhancements/#create-enhancements-for-existing-apis-to-document-graduation-criteria","text":"As part of this proposal it is suggested that the project would benefit from a bit of \"back porting\" of documentation to cover existing APIs. As a minimum bar it is proposed that we:","title":"Create enhancements for existing APIs to document graduation criteria"},{"location":"enhancements/0001_project_enhancements/#suggested-action-items_1","text":"Create an enhancement for each existing API in the main repository Identify the current level of the API and the known development path Identify the graduation criteria of the API Link any existing work to the enhancement for the API As an optional follow on, update the kcp.io site to include links to proposals as part of the documentation similar to thanos Present enhancements during community meetings","title":"Suggested Action Items"},{"location":"enhancements/0001_project_enhancements/#suggested-timeline_1","text":"Completed by: end of Q3 2023 Existing APIs can adopt change via the enhancement proposal as they go. Our understanding of how we may move through alpha, beta, and v1 becomes more clear based on the rate of feedback. If nothing else, by that time documentation should exist that minimizes the need to further document historical decisions.","title":"Suggested timeline"},{"location":"enhancements/0001_project_enhancements/#split-the-workloads-project-from-the-control-plane","text":"Within kcp there exists the following components that can be thought of as individual investment areas. kcp - the generic control plane with workspaces and API Import and API Export components kcp workloads - the transparent multi-cluster components kcp edge workloads - the transparent multi-cluster components focused on edge use cases kcp controllers dev - the components focused on tooling and development of kcp aware controllers kcp catalog - discover component for published APIs Right now the components that stand out as unnecessarily coupled are the generic control plane and workloads pieces. This proposal suggests that each of these have a clear split. This scoping of responsibility enables a more clear engagement model of where some of these components need to collaborate with other upstream projects who are also interested in the same spaces.","title":"Split the workloads project from the control plane"},{"location":"enhancements/0001_project_enhancements/#out-of-scope","text":"Separation of the generic control plane from tenancy out of scope for now. It is possible that it is revisited in the future.","title":"Out of scope"},{"location":"enhancements/0001_project_enhancements/#known-dependencies","text":"kcp workloads will depend on kcp (same repo currently) kcp edge workloads may depend on kcp worklaods kcp catalog will depend on kcp","title":"Known dependencies"},{"location":"enhancements/0001_project_enhancements/#suggested-action-items_2","text":"new repo be created for kcp workloads source code be moved as approporate optionally, special interest groups may be established to own the processes and discussion around the areas independently, but following the overall project's process model","title":"Suggested action items"},{"location":"enhancements/0001_project_enhancements/#suggested-timeline_2","text":"Level of effort analysis completed in: Q1 2023 Target completion by: end of Q2 2023 Transparent multi-cluster features are under heavy development. An initial level of effort evaluation should be conducted in order to know the scope of this change in order to plan it in a way that is not overly disruptive to continuing on the critical path of any MVP development.","title":"Suggested timeline"},{"location":"enhancements/0001_project_enhancements/#drawbacks","text":"May be considered too much, too soon. Slows down the contribution velocity of those already involved","title":"Drawbacks"},{"location":"enhancements/0001_project_enhancements/#alternatives","text":"As an alternative, the project could do nothing and hope that a shared understanding of how contribution happens establishes itself based on a lead-by-example method or by inheriting the established patterns of other communities. This proposal suggests that it is more beneficial to codify these items rather than rely on hope as a strategy.","title":"Alternatives"}]}